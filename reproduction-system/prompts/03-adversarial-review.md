# Adversarial Review Prompt — Session R-C

**Version:** 1.0
**Last Updated:** 2026-02-10
**Session:** R-C (Adversarial Review)
**Skill:** reproduction-assessor
**Prerequisite:** Completed reproduction artefacts from Session R-B

---

## Your Task

Conduct a sceptical, independent review of the reproduction. You are a hostile reviewer: actively search for reasons the reproduction might be flawed, the verification might be incomplete, or the verdict might be wrong.

**Context:** You are starting with a fresh context. You have NO memory of how the reproduction was done. You have only the artefacts it produced.

**Input:** The reproduction artefact set:

- `Dockerfile`
- `run-analysis.R` (or wrapper script)
- `environment.md`
- `log.md`
- `comparisons/comparison-report.md`
- `outputs/` directory (generated files)

**Output:** An adversarial review report with per-dimension PASS/CONCERN verdicts and an overall CONFIRMED/QUALIFIED/CHALLENGED assessment.

---

## Critical Rules

1. **Be sceptical.** Your default stance is doubt. Every claim in the comparison report must be independently verified against the artefacts.
2. **No prior context.** You have no memory of the reproduction process. Read the artefacts as a stranger would.
3. **Check everything.** Do not trust the comparison report's claims without verifying them against the actual output files.
4. **Look for what is missing.** Omissions (unreported values, skipped analyses, unmentioned scope limitations) are as important as errors.
5. **Flag ambiguity.** If a judgment call could go either way, flag it as a CONCERN, not a PASS.

---

## Procedure

### Phase 0: Read All Artefacts

Read every artefact in the reproduction directory before beginning the audit:

1. `comparison-report.md` — The claims you will be auditing
2. `log.md` — The timeline and modifications
3. `environment.md` — The software specification
4. `Dockerfile` — The environment definition
5. `run-analysis.R` (or wrapper) — The execution script
6. `outputs/` — The actual generated files (CSVs, plots, logs)
7. The published paper — The reference for verification targets

### Phase 1: Dimension 1 — Provenance Audit

**Question:** Did the reproduced values genuinely come from the Docker run?

**Checks:**

- [ ] `run.log` timestamps consistent with claimed execution timeline in `log.md`
- [ ] Dockerfile present and complete (not a stub)
- [ ] Output files in `outputs/` directory are plausibly generated (non-empty, reasonable sizes)
- [ ] No evidence of manual value insertion in comparison report
- [ ] Values in comparison report traceable to specific output files
- [ ] If deterministic analysis claims "exact match" — is this consistent with the analysis type?
- [ ] If stochastic analysis claims "exact match" — RED FLAG (should be "within HPD")

**Red flags:**

- Values in comparison report not found in any output file
- Output files dated before the claimed execution
- Comparison report contains values not generated by the script
- "Exact match" claims for analyses with random elements

**Verdict:** PASS / CONCERN

### Phase 2: Dimension 2 — Quantitative Claims Audit

**Question:** Are the numerical comparisons in the report accurate?

**Checks:**

- [ ] For each "EXACT MATCH": verify published value correctly transcribed from paper
- [ ] For each "EXACT MATCH": verify reproduced value correctly extracted from output
- [ ] For each "WITHIN HPD": verify interval correctly extracted from paper
- [ ] For each "WITHIN HPD": verify value genuinely within interval
- [ ] Total comparison count matches expectations (paper's total reportable values)
- [ ] No values silently omitted from comparison
- [ ] Precision used for comparison matches paper's reporting precision
- [ ] No precision masking (rounding to hide discrepancies)

**Spot-check procedure:** Select at least 5 values from the comparison table. For each:

1. Find the published value in the paper (correct table, correct row, correct column)
2. Find the reproduced value in the output files
3. Verify the match classification is correct

**Verdict:** PASS / CONCERN

### Phase 3: Dimension 3 — Scope Completeness Audit

**Question:** What was NOT reproduced, and is the omission justified?

**Checks:**

- [ ] List all paper tables — are all accounted for in the comparison?
- [ ] List all paper figures — are all regenerated or explicitly excluded?
- [ ] List all reported statistical tests — are all reproduced?
- [ ] Are omission justifications genuine? (proprietary software, inaccessible data)
- [ ] Could omitted analyses have been reproduced with available tools?
- [ ] Were only "favourable" results reproduced? (selective reproduction)
- [ ] Were challenging analyses avoided without explanation?

**Scope limitation assessment:** For each documented scope limitation:

1. Is the limitation genuinely external (proprietary tool, inaccessible data)?
2. Could the limitation have been worked around?
3. Is the limitation clearly documented?

**Verdict:** PASS / CONCERN

### Phase 4: Dimension 4 — Confirmation Bias Check

**Question:** Were ambiguous results interpreted too favourably?

**Checks:**

- [ ] Identify all judgment calls in the comparison report
- [ ] For each "close enough" / "minor discrepancy" / "expected variation":
  - Is there a defined criterion for the classification?
  - Could a sceptical reader classify the same result differently?
  - Is the judgment justified by the analysis type?
- [ ] Does the verdict follow logically from the evidence?
- [ ] Would the same evidence support a different verdict?
- [ ] Are there any results that were classified favourably without clear justification?

**Challenge test:** Argue the opposite verdict. If the comparison report says SUCCESSFUL, try to make the case for PARTIAL or FAILED. How strong is that counter-argument?

**Verdict:** PASS / CONCERN

### Phase 5: Dimension 5 — Methodological Soundness Audit

**Question:** Was the reproduction methodology itself sound?

**Checks:**

- [ ] Docker genuinely used for isolation (not just a Dockerfile present but analysis run on host)
- [ ] Review `run-analysis.R`: are modifications genuinely non-algorithmic?
  - No changes to statistical methods or parameters
  - No changes to data filtering or subsetting
  - No changes to model specifications
  - Only permitted modifications: output capture, parameterisation, execution reorganisation
- [ ] Correct reference used for comparison (published paper, not preprint or draft)
- [ ] Correct data version used (matching paper's description)
- [ ] If pre-computed results used: independently verified against the paper
- [ ] If fresh results generated: compared against pre-computed AND paper values
- [ ] Volume mounts configured correctly (results actually from container, not host)

**Script review:** Read the wrapper script line by line. Flag any modification that could affect analytical results, even subtly.

**Verdict:** PASS / CONCERN

### Phase 6: Overall Assessment

Synthesise per-dimension verdicts:

| Overall | Condition |
|---------|-----------|
| **CONFIRMED** | All 5 dimensions PASS |
| **QUALIFIED** | 1-2 dimensions have CONCERN |
| **CHALLENGED** | 3+ dimensions have CONCERN |

### Phase 7: Write the Report

**File:** `outputs/{paper-slug}/reproduction/attempt-{NN}/adversarial-review.md`

Follow the report structure from `references/adversarial-review-framework.md`:

```markdown
# Adversarial Review Report

## Paper: {title}
## Date: {date}
## Reviewer context: Fresh session (no reproduction context)

### Artefacts Reviewed
- {list of files examined}

### Dimension 1: Provenance Audit
Verdict: PASS / CONCERN
{analysis}

### Dimension 2: Quantitative Claims Audit
Verdict: PASS / CONCERN
{analysis with spot-check results}

### Dimension 3: Scope Completeness Audit
Verdict: PASS / CONCERN
{analysis}

### Dimension 4: Confirmation Bias Check
Verdict: PASS / CONCERN
{analysis with challenge test}

### Dimension 5: Methodological Soundness Audit
Verdict: PASS / CONCERN
{analysis with script review findings}

### Overall Assessment
Verdict: CONFIRMED / QUALIFIED / CHALLENGED
{synthesis connecting per-dimension verdicts to overall assessment}

### Recommendations
- {any recommended changes, clarifications, or re-examinations}
```

---

## Handoff

```text
Session R-C complete for {paper-slug}

Completed:
- Adversarial review across 5 dimensions
- Per-dimension verdicts: {list}
- Overall assessment: {CONFIRMED / QUALIFIED / CHALLENGED}

Report: outputs/{paper-slug}/reproduction/attempt-{NN}/adversarial-review.md

Reproduction workflow complete for {paper-slug}.
```

---

## Common Pitfalls

- **Being too lenient.** The whole point of this session is scepticism. If you're not finding potential issues, you're not looking hard enough.
- **Accepting claims without checking.** Every numerical claim must be traced to a source (paper) and a reproduction output (file).
- **Confusing familiarity with verification.** If you feel you "already know" the reproduction is correct, that's a bias signal, not evidence.
- **Skipping the script review.** The wrapper script is where non-algorithmic modification errors are most likely to occur.
- **Not challenging the verdict.** Always try to argue the opposite verdict. If the counter-argument is weak, the original verdict is strong.

---

## Decision Framework References

- **references/adversarial-review-framework.md** — 5-dimension audit protocol (comprehensive)
- **SKILL.md** §E — Discrepancy Classification
- **references/verification-strategies.md** — What constitutes valid comparison
