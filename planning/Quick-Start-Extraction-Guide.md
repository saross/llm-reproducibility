# Quick-Start Extraction Prototype Guide
## From First Paper to CWTS Presentation in Two Weeks

**Principle**: Build the bicycle first. Learn empirically. Iterate rapidly.

**Timeline**:
- **Days 1-3**: First paper extraction working
- **Week 1 (Days 4-7)**: Refine based on learning, extract Papers 2-3
- **Week 2**: Consolidate findings, prepare CWTS presentation materials
- **Week 3**: Iterate based on CWTS feedback

---

## Phase 1: First Paper Extraction (Days 1-3)

### Day 1: Setup and Initial Extraction

**Paper Selection:**
Choose the methodological digital archaeology paper that is:
- Most representative of your work
- Has clearest methods/claims structure
- You know intimately (can validate extraction easily)

**Suggested: Start with the surface survey methodological paper** (combines methodological + empirical elements, likely clearer structure than pure interdisciplinary work)

**Hour 1-2: Exploratory Extraction (Claims)**

Let's start with the simplest possible extraction task: **identifying explicit claims**

**Prompt Template (to be refined):**
```
I need you to extract explicit claims from this research paper. 

PAPER CONTEXT:
[Title, authors, abstract]

FULL TEXT:
[Paper text]

TASK:
Extract all explicit claims made in this paper. For each claim:
1. Quote the exact text where the claim appears
2. Classify as: EMPIRICAL (observable fact), INTERPRETATION (inference from data), 
   METHODOLOGICAL (about research approach), or THEORETICAL (conceptual/general)
3. Indicate confidence (DEFINITE, PROBABLE, SPECULATIVE based on hedging language)
4. Note page/section location

OUTPUT FORMAT:
{
  "claim_id": "C001",
  "claim_text": "Verbatim text of claim",
  "claim_type": "EMPIRICAL|INTERPRETATION|METHODOLOGICAL|THEORETICAL",
  "confidence": "DEFINITE|PROBABLE|SPECULATIVE",
  "location": {"section": "Results", "page": 12, "paragraph": 3},
  "verbatim_quote": "Exact text from paper including surrounding context"
}

CRITICAL RULES:
- Only extract claims EXPLICITLY stated in the text
- Do NOT infer or interpret unstated claims
- If uncertain whether something is a claim, include it but note uncertainty
- Provide exact quotes to enable verification
```

**Expected Output**: 10-30 claims from a typical methods paper

**Validation Approach**:
- Review each extracted claim
- Check: Is this actually in the paper? (catches hallucinations)
- Check: Is this a claim or just description? (refines definition)
- Check: Is classification reasonable? (validates typology)
- Document any extraction that seems wrong or questionable

**Hour 3-4: Evidence Extraction**

Once claims look reasonable, add evidence extraction:

**Prompt Template:**
```
Now extract the EVIDENCE supporting the claims you identified.

CONTEXT:
- You previously extracted [N] claims
- Now identify what evidence supports each claim

TASK:
For each piece of evidence:
1. Describe the evidence type (DATA, OBSERVATION, LITERATURE, COMPARATIVE)
2. Link to specific claim(s) it supports (by claim_id)
3. Quote relevant text
4. Note if evidence is:
   - PRIMARY (generated by this study)
   - SECONDARY (from literature/other sources)
   - NEGATIVE (absence of expected pattern)

OUTPUT FORMAT:
{
  "evidence_id": "E001",
  "evidence_type": "DATA|OBSERVATION|LITERATURE|COMPARATIVE",
  "evidence_source": "PRIMARY|SECONDARY|NEGATIVE",
  "description": "Brief description of the evidence",
  "supports_claims": ["C001", "C003"],
  "contradicts_claims": [],
  "location": {...},
  "verbatim_quote": "..."
}
```

**Hour 5-6: Methods Extraction**

**Prompt Template:**
```
Extract the METHODS described in this paper.

Distinguish between:
1. STATED methods (explicitly described)
2. IMPLICIT methods (implied but not fully specified)

For now, extract ONLY explicitly stated methods.

OUTPUT FORMAT:
{
  "method_id": "M001",
  "method_type": "FIELD|LAB|ANALYTICAL|THEORETICAL",
  "method_status": "STATED",
  "description": "Description of method",
  "justification": "Why this method was chosen (if stated)",
  "limitations": "Acknowledged limitations (if any)",
  "location": {...},
  "verbatim_quote": "..."
}
```

### Day 2: Relationship Mapping and Validation

**Hour 1-3: Build CEM Graph Manually**

Take extraction outputs and create first CEM graph:
- Load into JSON structure
- Manually verify/correct claim-evidence links
- Add any missed relationships
- Document what the extraction got right and wrong

**Hour 4-6: Systematic Validation**

Create validation metrics:

**Precision**: % of extracted elements that are actually present/correct
**Recall**: % of actual elements that were extracted (requires hand-coding sample)
**Provenance**: % of extractions with accurate text citations

**Initial Targets**:
- Precision >80% (most extractions are real)
- Recall >60% (getting majority of content, can improve)
- Provenance >90% (citations are accurate)

**Document**:
- Systematic errors (types of claims/evidence frequently missed)
- Hallucinations (extracted elements not in text)
- Provenance failures (citations wrong or vague)
- Edge cases (things that are hard to classify)

### Day 3: Refinement and Second Extraction

**Hour 1-3: Improve Prompts**

Based on Day 2 validation:
- Refine definitions (what counts as a claim?)
- Add examples from the paper (few-shot learning)
- Strengthen anti-hallucination instructions
- Improve provenance requirements

**Hour 4-6: Re-extract First Paper**

Run improved prompts on same paper:
- Compare new vs old extractions
- Calculate improvement in metrics
- Document what changed and why

**End of Day 3 Deliverable**: 
Working extraction system producing reasonable CEM graphs for one paper, with documented accuracy and known limitations.

---

## Phase 2: Multi-Paper Validation (Days 4-7)

### Day 4: Extract Paper 2 (Different Type)

Apply refined prompts to second paper (e.g., if Paper 1 was surface survey, Paper 2 is interdisciplinary)

**Focus**: Does extraction transfer across paper types?

### Day 5: Extract Paper 3

Third paper type from your corpus.

**Focus**: What are common vs paper-specific extraction patterns?

### Day 6: Multi-Model Comparison

Re-run Paper 1 extraction with GPT-OSS:120b (or another local model)

**Compare**:
- Agreement rate (% of claims extracted by both models)
- Unique extractions (what each model found that the other missed)
- Hallucination patterns (do models hallucinate differently?)

**Output**: Understanding of model-specific biases

### Day 7: Consolidation and Pattern Analysis

**Cross-paper analysis**:
- Which types of claims extract reliably across all papers?
- Which are problematic (low precision/recall, high variance)?
- What methods extract well vs poorly?
- Are research design elements extractable?

**Document systematic findings**:
- Methodological claims extract at X% precision
- Negative evidence detected in Y% of cases where present
- Research design (inductive/deductive) explicitly stated in Z% of papers
- Limitations acknowledged in W% of papers

---

## Phase 3: CWTS Presentation Preparation (Week 2)

### Day 8-10: Design Document Development

Now that you have empirical extraction data, write the comprehensive design document:

**Section 1: Extraction Architecture** (based on what actually worked)
- Prompt designs that achieved best results
- Successful strategies for hallucination mitigation
- Provenance linking approach
- Multi-model consensus findings (if tested)

**Section 2: CEM Schema** (as actually implemented)
- JSON structure used
- Node/edge types that emerged as useful
- Attribute schemas refined through practice
- Examples from your three papers

**Section 3: Validation Framework** (metrics that proved meaningful)
- Precision/recall calculation methods used
- Error taxonomies discovered
- Comparison approach for hand-coding
- Inter-model agreement assessment (if applicable)

**Section 4: Credibility Signal Translation** (preliminary)
- How extracted CEM elements map to repliCATS signals
- Which signals seem tractable for automation
- Which require more work
- Proposed assessment architecture

**Section 5: Research Design Detection** (critical for HASS)
- Can LLMs detect exploratory vs confirmatory research?
- Can they identify inductive/deductive/abductive reasoning?
- Can they flag selective evidence use?
- Preliminary findings from your three papers

### Day 11-12: CWTS Presentation Materials

**Create**:
1. **Slide deck** (20 minutes):
   - Problem statement (credibility assessment in HASS)
   - Approach (CEM extraction + repliCATS signals)
   - Preliminary results (3 papers, accuracy metrics)
   - Key challenges discovered
   - Proposed next steps
   - Questions for CWTS experts

2. **Demo materials**:
   - Live extraction example (show prompt → output → validation)
   - CEM graph visualization for one paper
   - Comparison of manual vs automated extraction
   - Error analysis examples

3. **Discussion prompts**:
   - How do our extracted elements map to research quality indicators?
   - Which credibility signals seem most tractable?
   - How does this compare to repliCATS approach?
   - What are we missing from metascience perspective?

### Day 13-14: Buffer and CWTS Feedback

Reserve for:
- Unexpected issues
- Additional analysis requested by CWTS
- Incorporating early feedback
- Planning Phase 2 work based on discussions

---

## Incremental Rubric Development Strategy

Rather than comprehensive rubric upfront, build incrementally:

### Week 1 Rubric (Minimal - Just Getting Started)

**Claims Identification:**
- Is this sentence making an assertion about the world/methods/theory?
- If yes → extract as claim
- Tag type: empirical, interpretation, methodological, theoretical
- Note confidence: definite, probable, speculative

**Evidence Identification:**
- Does this reference data, observations, or prior work?
- Does it connect to a specific claim?
- If yes → extract as evidence
- Tag as primary, secondary, or negative

**Methods Identification:**
- Is this describing how research was conducted?
- Explicitly stated only (no inference yet)
- Tag as field, lab, analytical, or theoretical method

### Week 2 Rubric (Expanded - Based on Learning)

Add layers discovered as important:
- Claim hierarchy (which claims support which?)
- Evidence chains (observation → analysis → interpretation)
- Contradictory evidence (what paper acknowledges as problematic)
- Limitations (what paper says is missing/uncertain)

### Week 3+ Rubric (Comprehensive)

Add sophistication:
- Research design classification (exploratory/confirmatory)
- Reasoning type (inductive/deductive/abductive)
- Implicit methods (what can be reasonably inferred)
- Selective evidence patterns (what's systematically missing)

**Key Principle**: Each week's rubric guides next week's extraction. Learn by doing.

---

## Technical Setup (Today)

**Immediate actions**:

1. **Select first paper** (send me title/PDF - I'll need to see it to extract)

2. **Prepare paper text**:
   - Extract text from PDF (OCR if needed)
   - Check for section headers, figure captions
   - Note any supplementary materials

3. **Define success criteria for Day 3**:
   - What would "good enough" extraction look like?
   - How will you validate outputs quickly?
   - What's the minimum viable CEM graph?

4. **Set up parallel hand-coding**:
   - As I extract, you can hand-code sections
   - Compare in real-time to catch issues early
   - This accelerates iteration

---

## CWTS Presentation Preview (What to Show in 2 Weeks)

**Core narrative**:

"We're building automated credibility assessment for interpretive HASS research by:
1. Extracting knowledge structures (claims, evidence, methods) from papers
2. Mapping these to credibility signals adapted from repliCATS
3. Scaling quality assessment beyond citation counting

In two weeks, we've:
- Developed working extraction system for methodological archaeology papers
- Achieved X% precision on claim extraction, Y% on evidence linking
- Identified key challenges: [research design detection, implicit methods, negative evidence]
- Discovered patterns: [what extracts well, what doesn't]

We need your feedback on:
- Credibility signal translation for interpretive research
- Validation strategies for non-replicable phenomena
- Integration with broader research assessment frameworks"

**Evidence to present**:
- 3 papers fully extracted and analyzed
- Quantified accuracy metrics
- CEM graph visualizations
- Error analysis with examples
- Preliminary credibility assessment design

---

## Risk Mitigation for Rapid Prototyping

**Risk**: Moving too fast leads to sloppy work
**Mitigation**: Document everything. Every prompt, every error, every decision. Fast iteration requires good record-keeping.

**Risk**: First paper is atypical, extraction doesn't generalize
**Mitigation**: Three papers in Week 1 specifically to test generalization early

**Risk**: Accuracy is lower than expected
**Mitigation**: This is discovery research. Document limitations rigorously. "LLMs achieve X% precision" is a valid finding even if X is disappointing.

**Risk**: CWTS feedback fundamentally redirects approach
**Mitigation**: Lightweight design means less sunk cost. Easy to pivot based on expert input.

---

## Next Immediate Steps

1. **Send me your first paper** (title + text, or just tell me which methodological paper and I can search for it if it's openly accessible)

2. **Clarify Day 3 success criteria**: What would make you confident to move to Papers 2-3?

3. **Identify CWTS presentation opportunities**: When in the next 2 weeks would you present? To whom?

4. **Set up local model infrastructure** (if testing multi-model this week): Which models are readily available and working?

Let's start extracting TODAY. Send me the first paper and let's see what we find.
