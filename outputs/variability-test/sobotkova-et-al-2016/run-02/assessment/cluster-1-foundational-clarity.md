# Cluster 1: Foundational Clarity (Transparency Pillar)

**Paper:** Sobotkova et al. (2016) - Measure Twice, Cut Once
**Run ID:** run-02
**Assessment Date:** 2025-12-04

## Signal 1.1: Comprehensibility

### Score: 75/100

### Assessment

**Methodological Paper Anchors Applied**

The paper demonstrates strong comprehensibility for its target audience of archaeologists considering digital recording adoption. The co-development framework is clearly articulated with concrete examples.

### Strengths

1. **Clear problem framing:** The spectrum from general-purpose DBMS to bespoke software is explicitly mapped, positioning FAIMS in context
2. **Transparent authorship positioning:** Authors identify themselves as both software developers and archaeological users, enabling dual-perspective interpretation
3. **Concrete examples:** Each claim illustrated with specific deployment details (costs, timelines, outcomes)
4. **Structured presentation:** Thematic organisation (three themes) aids navigation of deployment lessons

### Weaknesses

1. **Technical jargon:** Some software development terminology (XML definition documents, offline-first architecture) unexplained for archaeological readers
2. **Assumed knowledge:** Single-context recording assumed familiar without definition
3. **Selective depth:** Some case studies more detailed than others (Boncuklu most detailed, PAZC briefest)

### Evidence Mapping

| Aspect | Evidence |
|--------|----------|
| Clear problem statement | C034, C035 (FAIMS positioning on software spectrum) |
| Method clarity | M001 (case study documentation), P001 (contribution protocol) |
| Workflow transparency | P005, P006 (module customisation process) |

---

## Signal 1.2: Transparency

### Score: 70/100

### Assessment

**Methodological Paper Anchors Applied**

Good transparency regarding software availability and general deployment approach, with some gaps in systematic data collection methods.

### Strengths

1. **Code availability:** Excellent - GitHub repository explicitly stated, GPLv3 licence clear, Google Play distribution documented
2. **Supplementary materials:** Communication logs preserved and made available
3. **Cost transparency:** Specific cost figures provided for multiple projects
4. **Process documentation:** Detailed description of customisation workflow

### Weaknesses

1. **Data availability gaps:** Supplementary materials lack DOIs or persistent identifiers
2. **Metrics methodology unclear:** How cost estimates and time savings were calculated not specified
3. **Selection criteria unstated:** Why these three projects were chosen not explained
4. **User feedback informality:** "Universal" agreement claims without systematic collection

### Evidence Mapping

| Aspect | Status |
|--------|--------|
| Code availability | ✅ GitHub + GPLv3 (reproducibility_infrastructure) |
| Data availability | ⚠️ Supplements available but no DOI |
| Method documentation | ✅ Explicit protocols (P001-P010) |
| Conflict disclosure | ❌ Not addressed (authors are FAIMS developers) |

### Infrastructure Assessment

| Component | Score |
|-----------|-------|
| Persistent Identifiers | 4/10 (no DOIs, ORCIDs, or data identifiers) |
| FAIR Compliance | 72.5% (good accessibility, weaker findability) |
| Preregistration | N/A (methodological paper) |

---

## Cluster 1 Summary

### Aggregate Score: 72/100

### Interpretation (Methodological Paper Framework)

The paper achieves good foundational clarity for a methodological/software evaluation paper. The co-development process is comprehensible, software is transparent and available, but some documentation practices (persistent identifiers, systematic metrics collection) fall short of current best practices.

### Key Findings

1. **Comprehensibility (75):** Clear presentation of complex software-archaeology interface; minor terminology gaps
2. **Transparency (70):** Strong code availability offset by informal metrics methodology and missing conflict disclosure

### Caveats Applied

- Authors' dual role as developers and evaluators not disclosed as potential conflict
- Cost/time metrics are self-reported estimates without independent verification methodology

---

*Cluster 1 assessment complete. Proceeding to Cluster 2: Evidential Strength.*
