# Credibility Assessment Report

**Paper:** Introducing Preregistration of Research Design to Archaeology
**Slug:** ross-ballsun-stanton-2022
**DOI:** 10.31235/osf.io/sbwcq
**Assessment Date:** 2025-11-30
**Assessor Version:** v1.0

---

## Executive Summary

**Verdict:** Good | **Confidence:** High

**Paper Type:** Methodological (theoretical_framework)
**Research Approach:** Abductive
**Publication Era:** Current era ‚Äî FAIR and open science expectations established

This methodological advocacy paper presents a well-structured argument for adopting preregistration in archaeological research. The paper demonstrates strong foundational clarity with explicit claims, well-defined concepts, and a traceable argumentative structure. Evidential strength is good overall, with particularly strong plausibility grounded in cross-disciplinary evidence, though robustness is moderate as expected for advocacy papers. The paper exemplifies strong open science practices appropriate for its type.

**Key Strengths:**
- Argument structure is clear and traceable with explicit definitions (predictive/postdictive framework)
- Strong grounding in cross-disciplinary evidence of reproducibility issues
- Exemplary FAIR compliance (87.5%) with complete author ORCIDs and paper DOI

**Key Concerns:**
- Alternative transparency mechanisms not systematically compared (typical for advocacy)
- Robustness score (58) reflects advocacy nature ‚Äî readers should evaluate alternatives independently

**Bottom Line:** A credible methodological argument that meets current transparency standards and provides a well-documented case for preregistration, though readers should independently consider alternative approaches.

---

## Classification Summary

| Dimension | Value |
|-----------|-------|
| **Paper Type** | Methodological (theoretical_framework) |
| **Research Approach** | Abductive |
| **Framework Applied** | abductive_emphasis |
| **Quality State** | HIGH |
| **Classification Confidence** | High |
| **Publication Year** | 2021 |
| **Era Context** | Current era ‚Äî FAIR and open science expectations established |

### Quality State Implications

Quality state is HIGH, enabling precise scoring (¬±5 point precision). Full credibility assessment proceeds with standard report format and approach-specific anchors.

### Paper Type Context

Methodological papers argue for specific practices or approaches. Assessment focuses on argument quality, logical coherence, and evidence support rather than empirical validity. Lower evidence counts and Moderate Robustness scores are expected ‚Äî the paper builds a case for preregistration rather than testing multiple alternatives against each other.

---

## Signal Scores Dashboard

| Signal | Score | Band | Cluster | Context Flag |
|--------|-------|------|---------|--------------|
| Comprehensibility | 78 | Good | 1 | ‚Äî |
| Transparency | 75 | Good | 1 | ‚Äî |
| Plausibility | 80 | Excellent | 2 | ‚Äî |
| Validity | 72 | Good | 2 | ‚Äî |
| Robustness | 58 | Moderate | 2 | ‚ö†Ô∏è Expected for advocacy |
| Generalisability | 76 | Good | 2 | ‚Äî |
| Reproducibility | 72 | Good | 3 | üîß Methodological variant |

**Aggregate Score:** 73/100 (Good) *[EXPERIMENTAL]*

> **Note:** The aggregate score is experimental. We are investigating what it means and whether to retain it. Equal weights are used across all 7 signals.

### Context Flag Key

| Flag | Meaning |
|------|---------|
| ‚Äî | Default expectations apply |
| ‚ö†Ô∏è | Score expected to differ from typical empirical paper |
| üîß | Methodological variant anchors applied |

---

## Detailed Findings

### Cluster 1: Foundational Clarity

**Rating:** Strong

This methodological advocacy paper demonstrates strong comprehensibility for an abductive argument. The central explanatory claim ‚Äî that preregistration is the best solution for addressing transparency and reproducibility challenges in archaeology ‚Äî is explicitly stated and logically developed throughout the paper. Key terms are well-defined, including the foundational distinction between "predictive" and "postdictive" inquiry.

**Strengths:**
- Central claims explicit and well-bounded (preregistration as solution)
- Key terms defined (C002: preregistration definition, M001: predictive/postdictive distinction)
- Argument structure highly traceable through logical progression
- Research designs explicitly documented (RD001-RD003)
- Strong FAIR compliance (87.5%) with DOIs and ORCIDs

**Weaknesses:**
- Alternative solutions not systematically compared
- Some foundational assumptions implicit (IA001-IA005)
- Cross-disciplinary inference assumptions could be more explicit

---

### Cluster 2: Evidential Strength

**Rating:** Good (Adequate-to-Strong)

The paper demonstrates strong plausibility (80) with the argument well-grounded in cross-disciplinary evidence and archaeological history. The proposed explanation ‚Äî that preregistration is the best solution for addressing reproducibility challenges ‚Äî coheres with established domain knowledge. Alternative explanations are properly evaluated, though not exhaustively tested.

**Strengths:**
- Argument grounded in extensive cross-disciplinary evidence
- Historical evidence shows continuity with archaeological traditions (1970s processual)
- Quantitative evidence of QRPs provides concrete grounding (51% HARKing)
- Scope appropriately hedged ("many, but by no means all")
- Multiple evidence types support triangulation

**Weaknesses:**
- Alternative transparency approaches not systematically compared
- Evidence from other disciplines may not directly transfer
- Framework sensitivity not explored (what if disciplinary transfer assumptions questioned?)
- Single best-explanation framework; alternatives not rigorously ruled out

---

### Cluster 3: Reproducibility

**Rating:** Good
**Pathway:** Methodological Transparency Variant

This is a methodological advocacy paper with no computational component ‚Äî there is no data analysis, statistical testing, or code to execute. Therefore, standard reproducibility assessment does not apply. Instead, Methodological Transparency is assessed: whether the argumentative approach is documented clearly enough for independent evaluation.

**Strengths:**
- Argumentative structure clearly documented (RD001-RD005)
- Conceptual framework explicitly defined (M001: predictive/postdictive distinction)
- Evidence sources well-cited with >90% DOI coverage
- Reasoning progression traceable from problem to solution
- Strong FAIR compliance (87.5%) for methodological paper

**Weaknesses:**
- Selection criteria for comparator disciplines implicit
- Decision to focus on preregistration vs alternatives not fully explicit
- Some evidence weighting involves undocumented judgment

---

## Infrastructure & FAIR Summary

### FAIR Compliance

| Dimension | Score | Rating |
|-----------|-------|--------|
| **Findable** | 4/4 | Excellent |
| **Accessible** | 4/4 | Excellent |
| **Interoperable** | 2/4 | Moderate |
| **Reusable** | 4/4 | Excellent |
| **Overall FAIR** | 14/16 (87.5%) | Highly FAIR |

### Availability Status

| Resource | Status | Details |
|----------|--------|---------|
| **Code** | Not Applicable | Methodological/opinion paper with no computational analysis |
| **Data** | Not Applicable | Methodological/theoretical paper with no empirical data to share |
| **Preregistration** | Not Applicable | Preregistration not expected for methodological papers |

### Persistent Identifiers

| Type | Status |
|------|--------|
| Paper DOI | ‚úÖ Present (10.31235/osf.io/sbwcq) |
| Author ORCIDs | ‚úÖ Complete (2/2 authors) |
| Software DOIs | N/A ‚Äî No software |
| Data DOIs | N/A ‚Äî No data |

### Infrastructure Gaps

No significant infrastructure gaps for this paper type. The paper exemplifies appropriate open science practices for a methodological publication. Minor improvements possible:
- Consider HTML/semantic version for improved machine-actionability
- Link to related OSF projects/templates via PIDs

---

## Contextual Interpretation

This section explains what scores **mean for this specific paper type**, helping readers understand why certain scores may differ from typical empirical research expectations.

This paper is a methodological advocacy piece using abductive reasoning (inference to best explanation). Assessment expectations differ from empirical research papers in several ways:

1. **Evidence structure:** Methodological papers build arguments from literature synthesis, cross-disciplinary comparison, and theoretical reasoning rather than primary data collection. Lower evidence counts are expected.

2. **Robustness expectations:** Advocacy papers argue for a position rather than testing multiple alternatives against each other. A Moderate Robustness score is typical, not a severe criticism.

3. **Reproducibility variant:** With no computational component, reproducibility is assessed as Methodological Transparency ‚Äî whether reasoning is traceable enough for independent evaluation.

### Signal-Specific Context

#### Robustness (58, Moderate) ‚ö†Ô∏è

**Why this score:** Methodological advocacy papers argue for a position rather than testing multiple alternatives. The paper builds a coherent case for preregistration based on cross-disciplinary evidence, but does not exhaustively compare it to alternative transparency mechanisms.

**What this means:** A Moderate Robustness score is typical ‚Äî and not a severe criticism ‚Äî for this paper type. The score reflects that readers cannot fully evaluate whether preregistration is "best" from the paper alone.

**What readers should consider:** Evaluate alternative approaches to research transparency (e.g., registered reports, data sharing mandates, statistical reforms) independently when assessing the argument.

#### Reproducibility (72, Good) üîß

**Why this score:** This paper has no computational component ‚Äî there is no analysis to re-run. Reproducibility is assessed using the Methodological Transparency variant, which evaluates reasoning traceability rather than computational reproduction.

**What this means:** The score reflects that the argument structure is well-documented and reasoning is traceable. "Reproduction" means following the same argumentative approach and verifying that cited evidence supports the claims made.

---

## Era Context

**Publication Year:** 2021
**Era:** Current era ‚Äî FAIR and open science expectations established

The paper was published in 2021, placing it firmly in the current era of open science expectations. By this time:
- FAIR principles were well-established in scholarly communication
- OSF preprints typically exemplify open science best practices
- Data and code sharing expectations had become standard for empirical research
- ORCID adoption was increasingly expected for author identification

The paper meets and exceeds current era expectations for a methodological publication. Authors practice what they preach regarding open science ‚Äî the paper itself is an OSF preprint with complete PID coverage.

---

## Structured Output

The following JSON block contains all assessment data in machine-readable format.

```json
{
  "credibility_report": {
    "version": "1.0",
    "paper": {
      "slug": "ross-ballsun-stanton-2022",
      "title": "Introducing Preregistration of Research Design to Archaeology",
      "doi": "10.31235/osf.io/sbwcq",
      "publication_year": 2021
    },
    "classification": {
      "paper_type": "methodological",
      "paper_subtype": "theoretical_framework",
      "approach": "abductive",
      "framework": "abductive_emphasis",
      "quality_state": "high",
      "classification_confidence": "high"
    },
    "verdict": {
      "band": "good",
      "confidence": "high",
      "aggregate_score": 73,
      "aggregate_experimental": true
    },
    "signals": {
      "comprehensibility": { "score": 78, "band": "good" },
      "transparency": { "score": 75, "band": "good" },
      "plausibility": { "score": 80, "band": "excellent" },
      "validity": { "score": 72, "band": "good" },
      "robustness": { "score": 58, "band": "moderate", "context_flag": "expected_for_advocacy" },
      "generalisability": { "score": 76, "band": "good" },
      "reproducibility": { "score": 72, "band": "good", "variant": "methodological_transparency" }
    },
    "aggregates": {
      "cluster_1_rating": "strong",
      "cluster_2_rating": "good",
      "cluster_3_rating": "good"
    },
    "infrastructure": {
      "fair_score": 14,
      "fair_maximum": 16,
      "fair_percentage": 87.5,
      "fair_rating": "highly_fair",
      "code_availability": "not_applicable",
      "data_availability": "not_applicable",
      "preregistration": "not_applicable"
    },
    "era_context": {
      "publication_year": 2021,
      "era": "current",
      "era_label": "Current era ‚Äî FAIR and open science expectations established",
      "expectations_note": "Paper meets and exceeds current era expectations for methodological publication"
    },
    "assessment_metadata": {
      "assessment_date": "2025-11-30",
      "assessor_version": "v1.0",
      "schema_version": "1.0"
    }
  }
}
```

---

## Assessment Metadata

| Field | Value |
|-------|-------|
| **Assessment Date** | 2025-11-30 |
| **Assessor Version** | v1.0 |
| **Schema Version** | 1.0 |
| **Report Template** | credibility-assessment-report-template v1.0 |

---

**End of Report**
