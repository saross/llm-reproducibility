{
  "classification_metadata": {
    "classified_after_pass": 7,
    "classification_date": "2025-11-30",
    "classifier_version": "v0.2-alpha"
  },

  "paper_type": "empirical",
  "paper_type_justification": "This paper uses field survey data to validate machine learning predictions about archaeological phenomena (burial mound detection). Claims describe and explain performance outcomes of the CNN model. Methods (transfer learning, CNN training, validation) are tools for investigating the substantive research question: can pre-trained CNNs effectively detect burial mounds? The primary contribution is empirical findings about model performance, not the method itself. The paper answers 'Does this approach work?' through systematic empirical testing.",

  "taxonomy_feedback": {
    "category_fit_quality": "excellent",
    "proposed_new_category": null,
    "rationale_for_proposal": null,
    "characteristics_of_proposed_category": null,
    "alternative_papers_that_might_fit": []
  },

  "expressed_approach": {
    "approach": "deductive",
    "evidence": [
      "RD001: 'External validation design comparing ML model predictions against comprehensive field survey data' - clear hypothesis-testing structure",
      "RD002: 'Comparative two-run design testing impact of training data curation on model performance' - explicit test of hypothesis that curated data improves performance",
      "Abstract: 'we used a dataset of 773 mounds... to validate the performance of a pre-trained CNN' - validation implies prediction to test workflow"
    ],
    "source_sections": ["abstract", "data", "methods"],
    "confidence": "high"
  },

  "revealed_approach": {
    "approach": "deductive",
    "evidence": {
      "claims_structure": "Claims are performance assessments testing predictions: C001 'Pre-trained CNNs have significant limitations...', C002 'The pre-trained CNN model failed to identify burial mounds', C003 'External validation with field data is an essential part of CNN workflows'. These are hypothesis-rejection claims following systematic testing. Claims describe whether predictions were confirmed (mostly rejected).",
      "methods_application": "Methods follow confirmatory testing workflow: M001 (transfer learning), M002 (CNN training), M005 (automated performance evaluation using held-out test set), M006 (field-based external validation comparing model predictions against surveyed ground truth). The 70:20:10 train/validation/test split and external field validation are classic deductive validation procedures.",
      "analytical_workflow": "Prediction to systematic test to comparison against ground truth to conclusion. Two model runs (2021, 2022) tested specific hypothesis that curated training data improves performance. Quantitative metrics (precision 12.8%, recall rates) used to assess hypothesis confirmation. Workflow is classic deductive: predict model will work, test against known data, reject hypothesis based on poor performance."
    },
    "confidence": "high"
  },

  "expressed_vs_revealed": {
    "alignment": "matched",
    "harking_flag": false,
    "mismatch_explanation": "Paper explicitly frames research as validation study (deductive) and actually conducts systematic hypothesis testing. The research question 'Can pre-trained CNNs detect burial mounds?' is tested through systematic comparison of predictions against field survey ground truth. No discrepancy between stated design and actual methodology. Notably, the paper reports negative results (model failed) which demonstrates honest deductive practice - hypothesis was tested and rejected rather than reframed."
  },

  "primary_classification": {
    "approach": "deductive",
    "confidence": "high",
    "justification": "This is clearly deductive research. The paper: (1) has explicit research designs framed as validation/testing (RD001, RD002), (2) uses quantitative metrics to assess predictions against ground truth, (3) follows hypothesis to test to confirmation/rejection workflow, (4) reports negative results when hypothesis not supported (model failed to detect mounds effectively), (5) uses standard ML validation practices (train/validation/test splits, held-out test sets). The two-run comparative design (2021 vs 2022 with curated training data) explicitly tests hypothesis that training data quality affects performance. Classification is unambiguous."
  },

  "mixed_method_characterisation": {
    "is_mixed": false
  },

  "transparency_assessment": {
    "expressed_methodology_present": true,
    "transparency_quality": "high",
    "transparency_notes": "Explicit research design statements present. Methods section clearly describes transfer learning approach, training procedures, and validation workflow. Research designs RD001-RD004 document the study structure comprehensively. Some expected information missing (hyperparameters, threshold selection rationale) but overall transparency is high for an empirical ML validation study."
  },

  "credibility_framework": {
    "framework_to_use": "deductive_emphasis",
    "signal_prioritisation": {
      "primary_signals": ["validity", "robustness", "reproducibility"],
      "secondary_signals": ["transparency", "comprehensibility", "plausibility"],
      "deemphasised_signals": ["generalisability"],
      "rationale": "Deductive ML validation study prioritises: (1) Validity - are the validation metrics and ground truth comparison appropriate for assessing CNN performance? (2) Robustness - sensitivity of conclusions to analytical choices (threshold selection, training data curation)? (3) Reproducibility - can others reproduce the CNN training and validation? Code and data availability critical. Generalisability deemphasised as authors explicitly note results are context-specific (Kazanlak Valley, this landscape type)."
    }
  },

  "classification_notes": "Exemplary deductive empirical paper. Notable features: (1) Honest reporting of negative results - model failed, not reframed as success. (2) Clear validation design with external ground truth. (3) Two-run comparative design tests specific hypothesis about training data quality. (4) RD003 explicitly designed to counterbalance publication bias by documenting failure. This is a well-designed validation study that happens to find the approach doesn't work in this context."
}
