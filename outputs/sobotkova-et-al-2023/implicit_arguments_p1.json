[
  {
    "implicit_argument_id": "IA001",
    "content": "The error rate reported (<6%) is acceptable/good for this type of digitisation work",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "The paper presents the <6% error rate as evidence of success and high quality, but never explicitly states what threshold would constitute acceptable accuracy for map digitisation. The claim that results are 'high-quality' depends on an unstated assumption about acceptable error rates in this domain.",
    "trigger_text": [
      "with an error rate under 6%",
      "The resulting dataset was consistent, well-documented, and ready for analysis",
      "overall accuracy was high, over 94%"
    ],
    "trigger_locations": [
      "Abstract, page 1",
      "Abstract, page 1",
      "3.5.2 Digitisation errors, page 7"
    ],
    "supports_claims": [
      "C004"
    ],
    "location": {
      "section": "Abstract and Results",
      "page": 1
    }
  },
  {
    "implicit_argument_id": "IA002",
    "content": "Person-hours invested is an appropriate metric for assessing digitisation efficiency",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "The entire comparative analysis of approaches uses person-hours as the primary efficiency metric, but never explicitly justifies why this is the most relevant metric versus other possibilities like cost, calendar time, cognitive load, or quality-adjusted productivity.",
    "trigger_text": [
      "This digitisation required 241 person-hours",
      "our crowdsourcing approach is most efficient for digitisation projects",
      "if staff time is the primary limiting resource"
    ],
    "trigger_locations": [
      "Abstract, page 1",
      "Abstract, page 1",
      "Discussion section 4.1, page 9"
    ],
    "supports_claims": [
      "C001",
      "C017"
    ],
    "location": {
      "section": "Abstract and throughout paper",
      "page": 1
    }
  },
  {
    "implicit_argument_id": "IA003",
    "content": "The TRAP project context (archaeological fieldwork in rural Bulgaria with undergraduate participants) is sufficiently representative for generalising findings",
    "argument_type": "bridging_claim",
    "inference_reasoning": "The paper makes generalisable claims about when crowdsourcing approaches are worthwhile ('10,000-60,000 features', 'may offer advantages for datasets as small as a few hundred records') but these are based on a single project context. The bridge from 'our experience' to 'general recommendations' requires assuming the TRAP context is representative.",
    "trigger_text": [
      "A conservative estimate based on our work suggests",
      "This approach is readily transferable to other mobile GIS systems and map corpora",
      "our experience provides only a single data point"
    ],
    "trigger_locations": [
      "Abstract, page 1",
      "Conclusion section 5, page 11",
      "Conclusion section 5, page 11"
    ],
    "supports_claims": [
      "C001",
      "C002",
      "C006"
    ],
    "location": {
      "section": "Abstract and Conclusion",
      "page": 1
    }
  },
  {
    "implicit_argument_id": "IA004",
    "content": "Volunteer satisfaction and retention are important factors in digitisation project success beyond raw productivity metrics",
    "argument_type": "logical_implication",
    "inference_reasoning": "The paper emphasises volunteer attrition problems with desktop GIS and improved satisfaction with the mobile approach. If raw productivity were the only consideration, the paper would not need to discuss satisfaction, morale, or friction. The inclusion of these factors implies they matter for project success, but this is never explicitly stated as a criterion.",
    "trigger_text": [
      "volunteer attrition combined with demands on staff time during the height of fieldwork rendered this approach unsuccessful",
      "Low volunteer attrition indicated satisfaction with the experience",
      "digitisation was one of the least popular activities, reducing morale and causing friction"
    ],
    "trigger_locations": [
      "2.2 Crowdsourcing digitisation, page 4",
      "3.3 Digitisation comparison with desktop GIS, page 7",
      "4.1.1 Desktop GIS approaches versus crowdsourcing, page 10"
    ],
    "supports_claims": [
      "C003",
      "C004",
      "C017"
    ],
    "location": {
      "section": "Methods and Discussion",
      "page": 4
    }
  },
  {
    "implicit_argument_id": "IA005",
    "content": "The technical infrastructure for FAIMS Mobile (servers, devices, network) was available and functional, with these costs not fully accounted in efficiency calculations",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "The paper calculates efficiency based on customisation and deployment time but mentions reusing existing project infrastructure. The comparison assumes infrastructure costs are either negligible or equivalent across approaches, but this is not explicitly stated or justified.",
    "trigger_text": [
      "we were already using FAIMS Mobile for in-field legacy data verification",
      "Reusing the platform for digitisation offered a consistent working environment for users, reduced administrative load on staff, leveraged our experience with the platform, and avoided any additional hardware or software costs",
      "reusing the same equipment and system as the previous year"
    ],
    "trigger_locations": [
      "2.3 Using a mobile application, page 4",
      "2.3 Using a mobile application, page 4",
      "3.1 Project staff time, page 7"
    ],
    "supports_claims": [
      "C001",
      "C017"
    ],
    "location": {
      "section": "Approach",
      "page": 4
    }
  },
  {
    "implicit_argument_id": "IA006",
    "content": "Soviet military topographic maps are sufficiently accurate and reliable sources for archaeological feature locations",
    "argument_type": "disciplinary_assumption",
    "inference_reasoning": "The entire digitisation effort treats the Soviet maps as authoritative sources for mound locations, but never explicitly discusses the accuracy or reliability of these maps themselves. Ground-truthing is mentioned as a follow-up activity, but the assumption that maps are worth digitising in the first place is unstated.",
    "trigger_text": [
      "digitising mounds from over 20,000 sq km of Soviet military 1:50,000 topographic maps",
      "followed by ground-truthing (which continued through 2022)",
      "The ability to record mounds that no longer exist, but are represented in historical maps, is especially important"
    ],
    "trigger_locations": [
      "1.1 TRAP section, page 2",
      "1.1 TRAP section, page 2",
      "1.2 Burial mounds section, page 2"
    ],
    "supports_claims": [
      "C020"
    ],
    "location": {
      "section": "Introduction",
      "page": 2
    }
  },
  {
    "implicit_argument_id": "IA007",
    "content": "The authors had low expectations for crowdsourcing success based on their 2010 desktop GIS experience",
    "argument_type": "bridging_claim",
    "inference_reasoning": "The claim that success was 'unexpected' implies prior expectations. The paper describes the 2010 failed desktop GIS attempt but never explicitly states this created pessimism about volunteer digitisation generally. The bridge from '2010 failure' to 'unexpected success in 2017-18' requires assuming the failed experience shaped expectations.",
    "trigger_text": [
      "Our crowdsourced digitisation effort involving novice volunteers using an adapted mobile application for data capture proved unexpectedly successful",
      "volunteer attrition combined with demands on staff time during the height of fieldwork rendered this approach unsuccessful",
      "In 2017, faced with a short field season and little time for student training, we focused on implementing tools that would empower volunteers to digitise maps independently"
    ],
    "trigger_locations": [
      "4. Discussion, page 9",
      "2.2 Crowdsourcing digitisation, page 4",
      "2.2 Crowdsourcing digitisation, page 4"
    ],
    "supports_claims": [
      "C041"
    ],
    "location": {
      "section": "Discussion",
      "page": 9
    }
  },
  {
    "implicit_argument_id": "IA008",
    "content": "The specific feature characteristics (high density, moderate obtrusiveness) and simple data requirements (point + 10 attributes) made this digitisation task relatively easy compared to other map digitisation scenarios",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "The paper makes generalisable recommendations about when crowdsourcing is suitable based on feature counts, but these recommendations are calibrated to their specific task characteristics. The assumption that other digitisation tasks have comparable difficulty per feature is never stated or justified.",
    "trigger_text": [
      "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000–60,000 records, assuming similar feature characteristics and data collection requirements",
      "Such symbols occurred at a high density, averaging about 200 per tile (0.5 per sq km)",
      "The mound symbols were moderately obtrusive",
      "The records we sought to create were relatively simple: a point for the feature, a record number, plus ten attributes"
    ],
    "trigger_locations": [
      "4.1.2 Machine learning versus crowdsourcing, page 10",
      "2.1 Archaeological features in Soviet topographic maps, page 4",
      "2.1 Archaeological features in Soviet topographic maps, page 4",
      "2.1 Archaeological features in Soviet topographic maps, page 4"
    ],
    "supports_claims": [
      "C001",
      "C050"
    ],
    "location": {
      "section": "Methods and Discussion",
      "page": 4
    }
  },
  {
    "implicit_argument_id": "IA009",
    "content": "The 2017-2018 digitisation rates and error rates are representative and would be replicated in future deployments",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "All efficiency calculations and threshold recommendations depend on the measured rates (54-92 s/feature, <6% errors) being stable and representative. The paper acknowledges 2018 was less productive than 2017 but doesn't discuss whether measured performance represents best-case, typical, or variable scenarios.",
    "trigger_text": [
      "The concentrated digitisation in 2017 was more productive than the intermittent work of 2018",
      "In 2017, it was used for a total of 125.8 person-hours concentrated across five rainy days",
      "In 2018, use was more sporadic",
      "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000–60,000 features"
    ],
    "trigger_locations": [
      "3.2 Student-volunteer digitisation velocity and volume, page 7",
      "3.2 Student-volunteer digitisation velocity and volume, page 7",
      "3.2 Student-volunteer digitisation velocity and volume, page 7",
      "Abstract, page 1"
    ],
    "supports_claims": [
      "C001",
      "C029",
      "C050"
    ],
    "location": {
      "section": "Results and Discussion",
      "page": 7
    }
  },
  {
    "implicit_argument_id": "IA010",
    "content": "The customisation effort (35 h programmer, 4 h staff) is representative of what would be required for similar projects deploying FAIMS Mobile or comparable platforms",
    "argument_type": "logical_implication",
    "inference_reasoning": "The efficiency calculations treat the 35+4 h customisation time as a fixed cost, but this was the second version of FAIMS Mobile customisation by programmers already familiar with the platform. If customisation time varies significantly based on platform knowledge, complexity, or requirements, the threshold calculations would change.",
    "trigger_text": [
      "For the first season of use (2017), creating the Map Digitisation customisation of FAIMS Mobile required 35 h from an undergraduate student programmer plus 4 h from staff",
      "For the second season, adding additional validation to ensure population of latitude and longitude from GPS (see 'Recoverable data omissions and incomplete records' below) took 1 h of development from the programmer",
      "customisation of systems like FAIMS Mobile can be outsourced more easily than other project activities"
    ],
    "trigger_locations": [
      "3.1 Project staff time, page 7",
      "3.1 Project staff time, page 7",
      "4.1.1 Desktop GIS approaches versus crowdsourcing, page 9"
    ],
    "supports_claims": [
      "C001",
      "C018",
      "C045"
    ],
    "location": {
      "section": "Results and Discussion",
      "page": 7
    }
  },
  {
    "implicit_argument_id": "IA011",
    "content": "Volunteer availability and willingness is not a limiting constraint for crowdsourcing approach (i.e., projects can recruit sufficient volunteers)",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "The efficiency comparisons focus on staff time as the limiting resource, but don't discuss volunteer recruitment, availability, or retention as constraints. The 2010 desktop GIS experience showed volunteer attrition was a problem, but the paper assumes the mobile approach solves this without providing retention data.",
    "trigger_text": [
      "This discussion, furthermore, focuses on our most limited resource: staff time",
      "volunteer attrition combined with demands on staff time during the height of fieldwork rendered this approach unsuccessful",
      "Low volunteer attrition indicated satisfaction with the experience",
      "if staff time is the primary limiting resource"
    ],
    "trigger_locations": [
      "4.1 Choosing an approach, page 9",
      "2.2 Crowdsourcing digitisation, page 4",
      "3.3 Digitisation comparison with desktop GIS, page 7",
      "4.1 Choosing an approach, page 9"
    ],
    "supports_claims": [
      "C001",
      "C017",
      "C018"
    ],
    "location": {
      "section": "Discussion",
      "page": 9
    }
  },
  {
    "implicit_argument_id": "IA012",
    "content": "The ML comparison is representative: training data requirements, setup time, and expertise needs for ML approaches to historical map digitisation are similar to the Urban Occupations Project example",
    "argument_type": "bridging_claim",
    "inference_reasoning": "The entire ML comparison rests on a single reference point (Can, Gerrits, and Kabadayi 2021), treating it as representative of ML approaches generally. The bridge from 'one ML project required 1,300 h' to 'ML becomes worthwhile above 60,000 features' requires assuming this example is typical.",
    "trigger_text": [
      "The ERC-funded Urban Occupations Project (Can, Gerrits, and Kabadayi 2021), however, provides one benchmark for judging when pursuing a ML approach might be worthwhile",
      "This example, which appears to have required a minimum of about 1,300 h of preparation time alone, suggests that ML approaches are worthwhile for large-scale projects",
      "Above 60,000 records, ML approaches should be contemplated"
    ],
    "trigger_locations": [
      "4.1.2 Machine learning versus crowdsourcing, page 10",
      "4.1.2 Machine learning versus crowdsourcing, page 10",
      "4.1.2 Machine learning versus crowdsourcing, page 10"
    ],
    "supports_claims": [
      "C001",
      "C050",
      "C054"
    ],
    "location": {
      "section": "4.1.2 Machine learning versus crowdsourcing",
      "page": 10
    }
  },
  {
    "implicit_argument_id": "IA013",
    "content": "The mobile interface is inherently more intuitive than desktop GIS interfaces for novice users, independent of customisation quality",
    "argument_type": "logical_implication",
    "inference_reasoning": "The paper attributes improved usability to both careful customisation design and the mobile/touch interface paradigm. The claim that 'student volunteers are accustomed to, and even prefer, slippy-map, touch-screen interfaces' implies mobile is inherently advantageous, but this is never tested against a well-designed desktop interface.",
    "trigger_text": [
      "Fifth, student volunteers are accustomed to, and even prefer, 'slippy-map', touch-screen interfaces on mobile devices over the point-and-click, desktop UI idiom",
      "We believed that the use of the former would make it easier for students to learn the system, and more likely to stick with digitisation",
      "The result was a simple, familiar mobile application interface that let novices begin work with little training"
    ],
    "trigger_locations": [
      "2.3 Using a mobile application, page 4",
      "2.3 Using a mobile application, page 4",
      "3.3 Digitisation comparison with desktop GIS, page 7"
    ],
    "supports_claims": [
      "C003",
      "C023",
      "C032"
    ],
    "location": {
      "section": "2.3 Using a mobile application",
      "page": 4
    }
  },
  {
    "implicit_argument_id": "IA014",
    "content": "Performance degradation problems (30 s delay after 2,500 records) are acceptable or manageable trade-offs given the benefits of the system",
    "argument_type": "unstated_assumption",
    "inference_reasoning": "The paper reports significant performance degradation (3-5 s delays becoming 30 s delays) but frames this as 'mitigated' by data export and reinstantiation. The assumption that this workaround is acceptable (rather than a significant usability problem) is never explicitly defended.",
    "trigger_text": [
      "In use, automated extraction of coordinates from GPS into the Latitude/Longitude and Northing/Easting fields, which took three to 5 s with an empty database, took as long as 30 s once a device exceeded about 2,500 records",
      "Deteriorating performance was mitigated by exporting all data and instantiating a new and empty version of the application",
      "Spatial data omissions resulted from a failure of the software to populate the latitude and longitude fields from the application's SpatiaLite geodatabase due to users moving through the forms too quickly"
    ],
    "trigger_locations": [
      "3.4 Application performance, page 7",
      "3.4 Application performance, page 7",
      "3.5.1 Recoverable data omissions, page 7"
    ],
    "supports_claims": [
      "C003",
      "C004",
      "C041"
    ],
    "location": {
      "section": "3.4 Application performance",
      "page": 7
    }
  },
  {
    "implicit_argument_id": "IA015",
    "content": "The specific thresholds calculated (3,500-4,500 vs staff, 7,500-10,000 vs desktop volunteers, 10,000-60,000 vs ML) have meaningful precision despite being based on single-project measurements with acknowledged variability",
    "argument_type": "bridging_claim",
    "inference_reasoning": "The paper presents specific numerical thresholds and even provides low/mid/high estimates in tables, but these are based on limited measurements from one project with acknowledged variations (2017 vs 2018 rates, individual student variations). The bridge from 'our measured rates' to 'generalizable thresholds' treats measurement uncertainty as less significant than stated.",
    "trigger_text": [
      "To summarise in round numbers, a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000–60,000 records",
      "Under the most conservative scenario that considers all invested time, the use of our system pays off between about 3,500–4,500 features versus direct digitisation by staff using desktop GIS, and about 7,500–10,000 features versus support for volunteers using desktop GIS",
      "The concentrated digitisation in 2017 was more productive than the intermittent work of 2018",
      "Students' individual error rates ranged from 1.3% to 10.6%"
    ],
    "trigger_locations": [
      "4.1.2 Machine learning versus crowdsourcing, page 10",
      "4.1.2 Machine learning versus crowdsourcing, page 10",
      "3.2 Student-volunteer digitisation velocity, page 7",
      "3.5.2 Digitisation errors, page 9"
    ],
    "supports_claims": [
      "C001",
      "C050",
      "C051",
      "C052"
    ],
    "location": {
      "section": "Discussion",
      "page": 10
    }
  },
  {
    "implicit_argument_id": "IA016",
    "content": "The approach is 'readily transferable' despite being tightly coupled to FAIMS Mobile's specific features (offline operation, data validation, controlled vocabularies, automated metadata)",
    "argument_type": "logical_implication",
    "inference_reasoning": "The conclusion claims the approach is 'readily transferable to other mobile GIS systems' but the Methods section emphasizes specific FAIMS Mobile capabilities that were essential to success. The transferability claim implies other systems have comparable features, but this is not verified.",
    "trigger_text": [
      "This approach is readily transferable to other mobile GIS systems and map corpora",
      "The decision to use mobile software, and FAIMS Mobile in particular, was based on several factors. First, FAIMS Mobile worked offline",
      "It supported the production of a customised map digitisation system with a simple UI and streamlined workflow, while still providing essential features including layer management, geometry creation and editing, capture and association of structured data, import and use of arbitrary rasters (scanned maps as geotiffs), automated metadata creation, and data validation",
      "No existing system met these requirements 'off-the-shelf', without significant customisation"
    ],
    "trigger_locations": [
      "5. Conclusion, page 11",
      "2.3 Using a mobile application, page 4",
      "2.3 Using a mobile application, page 4",
      "2.3 Using a mobile application, page 5"
    ],
    "supports_claims": [
      "C059"
    ],
    "location": {
      "section": "2.3 and Conclusion",
      "page": 4
    }
  }
]
