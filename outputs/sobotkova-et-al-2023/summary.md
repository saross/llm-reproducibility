# Extraction Summary
## Sobotkova et al. 2023

**Full Title:** Creating large, high-quality geospatial datasets from historical maps using novice volunteers

**Authors:** Adela Sobotkova, Shawn A. Ross, Christian Nassif-Haynes, Brian Ballsun-Stanton

**Journal:** Applied Geography
**Year:** 2023
**DOI:** 10.1016/j.apgeog.2023.102967

**Extraction Date:** 2025-10-25
**Schema Version:** 2.5
**Extractor:** Claude Code with research-assessor v2.6

---

## Extraction Statistics

| Category | Count |
|----------|-------|
| Evidence | 36 |
| Claims | 31 |
| Research Designs | 2 |
| Methods | 6 |
| Protocols | 10 |
| **Total Items** | **85** |

### RDMAP Sourcing Breakdown
- **Explicit Items:** 13 (documented in Methods section)
- **Implicit Items:** 5 (mentioned/inferred from other sections)
- **Sourcing Quality:** 100% compliant with v2.5 requirements

---

## Research Designs Extracted

### RD001: Case Study Design (Explicit)
Case study of crowdsourced map digitization using novice volunteers with mobile GIS during archaeological fieldwork in Bulgaria (2017-2018).

**Key Attributes:**
- Scope: Yambol region Bulgaria (~23,500 sq km, 10,827 features)
- Reasoning: Inductive (exploratory case study, post-hoc comparative analysis)
- Enables: M001, M002, M003, M005, M006

### RD002: Comparative Research Question (Implicit)
Under what conditions do different map digitization approaches (expert GIS, volunteer GIS, crowdsourcing with mobile, ML) become worthwhile?

**Key Attributes:**
- Timing: Pre-data (evaluation designed into 2017 season)
- Enables: M004

---

## Methods Extracted

### M001: Crowdsourcing with FAIMS Mobile (Explicit)
Mobile GIS platform customized for map digitization by novice volunteers, enabling offline multi-user feature digitization.

### M002: Platform Selection (Explicit)
FAIMS Mobile chosen over desktop GIS and competing platforms based on offline capability, functional requirements, usability, reuse, student preferences, and open-source transparency.

### M003: Participant Recruitment (Explicit)
Undergraduate field school students from Arts/Humanities backgrounds, most with no GIS training.

### M004: Comparative Evaluation (Explicit + Implicit consolidation)
Time-motion study comparing person-hours to features produced, with threshold extrapolation to estimate break-even points vs alternative approaches.

### M005: Feature Selection Strategy (Explicit)
Extracting burial/settlement mound symbols from 1:50,000 Soviet topographic maps (200 symbols/tile average, moderate obtrusiveness).

### M006: Data Structure Design (Explicit)
Simple records: point geometry + record number + 10 attributes.

---

## Protocols Extracted (10 total)

### Explicit Protocols (8)
1. **P001:** FAIMS customization (UI simplification + automation + map/form views)
2. **P002:** Implementation workflow (7-step process from modeling to QA)
3. **P003:** Map preparation (tiling, pyramids for GeoTIFFs)
4. **P004:** Symbol recognition and digitization procedure
5. **P007:** Time tracking (timesheets + timestamps + journals)
6. **P008:** Quality assurance (random 7% sample, error categorization)

### Implicit Protocols (4, documented in Results not Methods)
7. **P009:** Volunteer training (minimal, "minutes" duration - no curriculum documented)
8. **P010:** GPS coordinate extraction automation (performance degradation noted)
9. **P011:** Performance mitigation (export/reset after 2,500-6,000 records)
10. **P012:** Data omission correction (re-extract from geodatabase)

---

## Key Findings (Selected Claims)

### Core Claims
- **C001:** Crowdsourcing approach unexpectedly successful despite minimal resourcing
- **C020:** Compared to desktop GIS approaches, required little training/supervision, used open-source software, yet produced large accurate dataset
- **C028:** Approach most efficient for 10,000-60,000 features (conservative estimate)
- **C029:** Payoff threshold vs desktop GIS digitization: 4,500 features
- **C030:** Payoff threshold vs ML approaches: 60,000 features

### Notable Evidence
- **E024-E027:** Time measurements (35h programmer + 4h staff for customization in 2017; 1h+1h in 2018)
- **E028-E031:** Performance metrics (125.8h volunteers in 2017, 63.6h in 2018, 54s/feature avg)
- **E032-E036:** Quality metrics (<6% error rate, 241 total person-hours for 10,827 features)

---

## Methodological Transparency Assessment

### Well-Documented (Explicit)
- Customization approach and rationale
- Platform selection criteria
- Implementation workflow
- Time tracking methodology
- Quality assurance procedures
- Comparative evaluation framework

### Transparency Gaps (Implicit/Missing)
1. **Training protocol** (P009 - minimal documentation, marked implicit)
2. **GPS coordinate handling** (P010 - automation mentioned only when discussing performance)
3. **Performance mitigation** (P011 - workaround discovered, not planned procedure)
4. **Data cleaning** (P012 - correction procedures applied post-hoc)
5. **Symbol identification criteria** (M005 - how volunteers distinguished mound symbols)
6. **Volunteer selection/demographics** (M003 - participation rate, demographics)

### Assessment Impact
The implicit protocols represent procedural adaptations discovered during fieldwork rather than planned methodology. This is common in real-world implementations but reduces replicability. Core methodology (customization approach, evaluation framework) is well-documented and replicable.

---

## File Outputs

- **extraction.json** - Complete structured extraction (542 lines)
- **validation-pass3.md** - Structural validation report (100% pass rate)
- **summary.md** - This summary document

---

## Extraction Quality Metrics

- **Pass 1 Completeness:** 68 items extracted (32 claims, 36 evidence, then 21 RDMAP in Pass 3)
- **Pass 2 Reduction:** 3.1% for claims/evidence (minimal consolidation appropriate)
- **Pass 4 Reduction:** 14.3% for RDMAP (within 15-20% target range)
- **Source Verification:** 100% pass rate (85/85 items properly sourced)
- **Cross-Reference Integrity:** 100% valid after fixing 1 consolidation-related broken reference
- **Validation Status:** âœ… PASS - Ready for assessment

---

## Notes for Assessment

### Strengths
1. Comprehensive time-tracking enabling robust cost-benefit analysis
2. Clear comparative framework across 4 digitization approaches
3. Detailed customization documentation supporting transferability claims
4. Honest reporting of 2010 failed attempt with desktop GIS

### Consider for Assessment
1. Threshold calculations assume feature equivalence across studies (different map types, symbol densities)
2. Training protocol lack of documentation (P009) - claims "minimal training" but no curriculum/assessment
3. Implicit protocols (P010-P012) represent real-world adaptations but reduce method replicability
4. Small sample for quality assurance (4 maps, 7%) though error rate <6% suggests adequate

### Research Context
Study presents crowdsourcing as middle ground between manual desktop GIS (labor-intensive) and ML (expertise-intensive). The methodological contribution is the demonstration that mobile field data collection platforms can be repurposed for desktop digitization with usability benefits transferring across contexts.

---

**Extraction Complete:** 2025-10-25
**Validation:** PASSED
**Status:** Ready for Assessment
