# Cluster 2: Evidential Strength Assessment

**Paper:** ballsun-stanton-et-al-2018
**Assessment Date:** 2025-11-30
**Assessor Version:** v1.0

**Quality State:** HIGH
**Research Approach:** Methodological (confidence: high)
**Paper Type:** Methodological (software paper)

---

## Signal Scores Summary

| Signal | Score | Band | Approach Anchors |
|--------|-------|------|------------------|
| Plausibility | 74 | Good | Methodological |
| Validity | 62 | Good | Methodological |
| Robustness | 55 | Moderate | Methodological ‚ö†Ô∏è |
| Generalisability | 70 | Good | Methodological |

**Cluster Rating:** Adequate

---

## Signal 3: Plausibility

**Score:** 74/100 (Good)

**Approach anchors applied:** Methodological

### Assessment

The paper demonstrates good plausibility for its software design claims. The architecture decisions are consistent with established software engineering practice: modular design for customisation, offline-first for fieldwork, open source for academic sustainability. Claims about field data quality problems (C003-C008) are grounded in cited literature and field experience.

The iterative co-development approach (RD001) is consistent with participatory design literature. Claims about five years of deployment experience align with the project timeline and cited case studies. The software's cross-disciplinary applicability is plausibly supported by documented deployments in archaeology, geoscience, history, and ecology.

However, some claims about software benefits lack detailed supporting evidence in this paper. Efficiency gains, data quality improvements, and workflow benefits are asserted but detailed evidence is in cited papers rather than presented here.

### Evidence

**Strengths:**
- Architecture decisions consistent with software engineering best practice
- Problem framing (C003-C008) grounded in literature
- RD001: Five years of deployment experience documented
- Cross-disciplinary deployments cited

**Weaknesses:**
- Benefit claims rely on cited evidence rather than primary data
- Some efficiency/quality claims lack detailed support in paper

### Scoring Rationale

Score of 74 (Good) reflects: design decisions consistent with domain knowledge (60-79); theoretical grounding in software engineering practice present (60-79); claims generally plausible but some rely on external evidence.

---

## Signal 4: Validity

**Score:** 62/100 (Good)

**Approach anchors applied:** Methodological

### Assessment

The paper demonstrates moderate-to-good validity for a software description paper. Case studies (RD002) provide evidence of deployment impact, though details are in the cited Sobotkova et al. 2016 paper rather than presented here. This citation-based evidence is appropriate for a short software paper but limits independent validity assessment.

The claim that "three case studies involving archaeological deployments showed that users benefited from increased efficiency of fieldwork" (E018-E020) is supported by citation but not detailed methodology. User uptake statistics (E021) provide quantitative evidence of adoption though not effectiveness.

For methodological papers, validity is about whether claims are adequately supported. The architecture claims are validated by the existence of working software. The benefit claims rely on cited evidence and deployment experience.

### Evidence

**Strengths:**
- Case studies cited (Sobotkova et al. 2016)
- Uptake statistics provided
- Working software validates architecture claims
- Five years of deployment validates stability

**Weaknesses:**
- Case study methodology not detailed in this paper
- Benefit claims rely on cited rather than primary evidence
- User satisfaction data not presented

### Scoring Rationale

Score of 62 (Good) reflects: evidence supports main claims (60-79); methods appropriate for software paper format; alternatives not extensively considered; some reliance on external evidence.

---

## Signal 5: Robustness

**Score:** 55/100 (Moderate) ‚ö†Ô∏è

**Approach anchors applied:** Methodological

> ‚ö†Ô∏è **Context Flag:** Methodological advocacy papers argue for a position rather than testing multiple alternatives. A Moderate Robustness score is typical ‚Äî and not a severe criticism ‚Äî for this paper type.

### Assessment

The paper demonstrates moderate robustness, consistent with its advocacy nature. FAIMS is presented as the solution to field data collection problems, with limited comparison to alternatives. This is appropriate for a software paper promoting a particular approach but limits robustness assessment.

The paper acknowledges some limitations: development challenges (E006-E012) including testing infrastructure needs, GUI creation difficulties, and complexity costs. This honest acknowledgement strengthens robustness within the advocacy framing.

Alternative approaches are mentioned but not systematically compared. The paper notes that other tools exist (commercial software, ad-hoc solutions) but does not provide detailed comparison. Sensitivity to design choices (e.g., Android vs iOS, modular vs monolithic) is discussed qualitatively but not tested.

### Evidence

**Strengths:**
- Limitations acknowledged (E006-E012)
- Design trade-offs discussed qualitatively
- Five years of evolution suggests iterative refinement

**Weaknesses:**
- Limited comparison to alternative software
- Design choices not systematically tested against alternatives
- No sensitivity analysis of architecture decisions

### Scoring Rationale

Score of 55 (Moderate) with ‚ö†Ô∏è flag reflects: methodological advocacy paper argues for position (expected for type); limitations acknowledged (positive); alternatives mentioned but not systematically compared; robustness assessment limited by paper type.

---

## Signal 6: Generalisability

**Score:** 70/100 (Good)

**Approach anchors applied:** Methodological

### Assessment

The paper demonstrates good scope constraint with cross-disciplinary applicability claims. FAIMS is explicitly designed for field research across disciplines, with documented deployments in archaeology, geoscience, history, and ecology. This cross-disciplinary design is central to the software's value proposition.

The paper appropriately acknowledges limitations. The focus on large multi-year projects reflects uptake demographics. The modular architecture is designed to accommodate disciplinary differences through customisation rather than one-size-fits-all design.

However, transfer conditions are implicit. The paper doesn't systematically specify what conditions would make FAIMS suitable or unsuitable for a particular field research context. Hardware requirements (Android devices), connectivity constraints (offline-first design), and customisation needs are discussed but not formalised.

### Evidence

**Strengths:**
- Cross-disciplinary design explicit
- Multiple discipline deployments documented
- Modular architecture accommodates variation
- Large multi-year project focus acknowledged

**Weaknesses:**
- Transfer conditions implicit
- Hardware/infrastructure requirements not formalised
- Customisation complexity may limit some uptake

### Scoring Rationale

Score of 70 (Good) reflects: claims bounded to field research context; disciplinary scope appropriate; limitations acknowledged; transfer conditions implicit rather than formalised.

---

## Cluster Synthesis

**Overall Evidential Strength:** Adequate

This methodological software paper demonstrates adequate evidential strength with scores ranging from 55-74 (Moderate to Good). The pattern reflects the advocacy nature of software papers: clear design rationale with limited systematic comparison to alternatives.

The Robustness score (55, Moderate ‚ö†Ô∏è) requires contextual interpretation. For a software paper promoting FAIMS as a solution, this score reflects appropriate expectations rather than a critical flaw. The paper honestly acknowledges limitations and discusses design trade-offs.

### Pattern Summary

The dominant pattern is software advocacy with honest limitation acknowledgement. Plausibility is good (design decisions sensible), Validity is adequate (case studies cited), Robustness is moderate (limited comparison, expected for type), Generalisability is good (cross-disciplinary design).

### Implications for Cluster 3

- **For Reproducibility:** Excellent code availability (GitHub, GPLv3) suggests high software reproducibility. Methodological Transparency variant (üîß) applies as this is software being described, not analytical workflow.

---

## Structured Output

```yaml
cluster_2_evidential_strength:
  paper_slug: "ballsun-stanton-et-al-2018"
  assessment_date: "2025-11-30"
  quality_state: "high"
  research_approach: "methodological"

  plausibility:
    score: 74
    band: "good"
    strengths:
      - "Architecture decisions consistent with software engineering practice"
      - "Problem framing grounded in literature"
      - "Cross-disciplinary deployments documented"
    weaknesses:
      - "Some benefit claims rely on cited evidence"
    rationale: "Good plausibility. Design decisions sensible, claims grounded, some reliance on external evidence."

  validity:
    score: 62
    band: "good"
    strengths:
      - "Case studies cited"
      - "Working software validates architecture"
      - "Five years deployment validates stability"
    weaknesses:
      - "Case study methodology not detailed"
      - "Benefit claims rely on cited evidence"
    rationale: "Good validity for software paper. Evidence supports claims, some external reliance."

  robustness:
    score: 55
    band: "moderate"
    context_flag: "advocacy_paper"
    strengths:
      - "Limitations acknowledged"
      - "Design trade-offs discussed"
    weaknesses:
      - "Limited comparison to alternatives"
      - "No systematic sensitivity analysis"
    rationale: "Moderate robustness with advocacy context flag. Expected for software paper promoting particular approach."

  generalisability:
    score: 70
    band: "good"
    strengths:
      - "Cross-disciplinary design explicit"
      - "Multiple discipline deployments"
      - "Modular architecture accommodates variation"
    weaknesses:
      - "Transfer conditions implicit"
    rationale: "Good scope constraint. Cross-disciplinary applicability documented, transfer conditions informal."

  cluster_synthesis:
    overall_rating: "adequate"
    pattern_summary: "Software advocacy with honest limitations. Moderate robustness expected for paper type."
    consistency_check: "consistent"
    implications:
      cluster_3: "Excellent code availability; Methodological Transparency variant applies"
```
