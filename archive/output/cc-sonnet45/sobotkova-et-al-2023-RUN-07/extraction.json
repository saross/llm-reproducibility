{
  "schema_version": "2.5",
  "extraction_timestamp": "2025-10-28T00:00:00Z",
  "extractor": "Claude Sonnet 4.5",
  "project_metadata": {
    "paper_title": "Creating large, high-quality geospatial datasets from historical maps using novice volunteers",
    "authors": [
      "Adela Sobotkova",
      "Shawn A. Ross",
      "Christian Nassif-Haynes",
      "Brian Ballsun-Stanton"
    ],
    "publication_year": 2023,
    "journal": "Applied Geography",
    "doi": "10.1016/j.apgeog.2023.102967",
    "paper_type": "research article",
    "discipline": "archaeology",
    "research_context": "Crowdsourced digitisation of burial mounds from Soviet military topographic maps in Bulgaria using FAIMS Mobile platform with novice volunteers during archaeological fieldwork"
  },
  "evidence": [
    {
      "evidence_id": "E001",
      "evidence_text": "FAIMS Mobile was used to digitise 10,827 mound features from Soviet military topographic maps",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "author_assertion",
      "verbatim_quote": "Deployed in Bulgaria as an ancillary activity during 2017-2018 archaeological fieldwork, FAIMS Mobile was used to digitise 10,827 mound features from Soviet military topographic maps.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C001", "C002", "C003"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E002",
      "evidence_text": "Digitisation required 241 person-hours: 57 from staff and 184 from novice volunteers",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "author_assertion",
      "verbatim_quote": "This digitisation required 241 person-hours (57 from staff; 184 from novice volunteers), with an error rate under 6%.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C001", "C004", "C005"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E003",
      "evidence_text": "Error rate was under 6%",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "author_assertion",
      "verbatim_quote": "This digitisation required 241 person-hours (57 from staff; 184 from novice volunteers), with an error rate under 6%.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C003", "C006"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E004",
      "evidence_text": "The resulting dataset was consistent, well-documented, and ready for analysis with a few hours of processing",
      "evidence_type": "qualitative_outcome",
      "evidence_basis": "author_assertion",
      "verbatim_quote": "The resulting dataset was consistent, well-documented, and ready for analysis with a few hours of processing.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C003", "C007"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E005",
      "evidence_text": "An estimated 50,000 burial mounds were built in Bulgarian lands from the Early Bronze Age through the Middle Ages",
      "evidence_type": "background_context",
      "evidence_basis": "archival_document",
      "verbatim_quote": "An estimated 50,000 burial mounds were built in Bulgarian lands from the Early Bronze Age through the Middle Ages (Shkorpil & Shkorpil, 1989, p. 20; Kitov, 1993, p. 42).",
      "location": {
        "section": "Introduction",
        "subsection": "1.2. Burial mounds in Bulgarian archaeology",
        "page": 2,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C012"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E006",
      "evidence_text": "Mounds range in size from 10 to 50 m in diameter and 0.5-20 m in height",
      "evidence_type": "background_context",
      "evidence_basis": "observational_record",
      "verbatim_quote": "They were constructed of earth and rubble, and range in size from 10 to 50 m in diameter and 0.5-20 m in height (see Fig. 1).",
      "location": {
        "section": "Introduction",
        "subsection": "1.2. Burial mounds in Bulgarian archaeology",
        "page": 2,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": [],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E007",
      "evidence_text": "Between 2008 and 2016, TRAP catalogued 773 mounds in the Kazanlak Valley and 431 mounds in the Yambol region using pedestrian surface survey",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "author_assertion",
      "verbatim_quote": "Between 2008 and 2016, TRAP catalogued some 773 mounds in the Kazanlak Valley and 431 mounds in the Yambol region using pedestrian surface survey supported by manual digitisation of satellite imagery and maps.",
      "location": {
        "section": "Introduction",
        "subsection": "1.1. The Tundzha Regional Archaeology Project",
        "page": 2,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supports_claims": [],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E008",
      "evidence_text": "The 2010 digitisation effort using desktop GIS produced a dataset of 915 features and required 5-7 hours of staff training, support, and error-checking",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "author_assertion",
      "verbatim_quote": "Although we did not maintain detailed volunteer time-on-task records, we know this effort produced a dataset of 915 features and required about 5-7 h of staff training, support, and error-checking over a three-week period (based on our field journals).",
      "location": {
        "section": "Results",
        "subsection": "3.3. Digitisation comparison with desktop GIS",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C013", "C014"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E009",
      "evidence_text": "Students' digitisation rates ranged from 44-73 seconds per feature, with fastest digitisers having lowest error rates",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "observational_record",
      "verbatim_quote": "The average time to record a point feature was 54 s, based on start and end times of feature creation as recorded by the devices (representing work time excluding pauses between records).",
      "location": {
        "section": "Results",
        "subsection": "3.2. Student-volunteer digitisation velocity and volume",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": ["C015"],
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E010",
      "evidence_text": "Error rates ranged from 1.3% to 10.6% with overall rate of 5.9% across four randomly selected maps",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "observational_record",
      "verbatim_quote": "Second, a review by project staff of four randomly selected maps (7% of the total) found 49 errors from a true count of 834 features, a 5.87% error rate (see Table 3).",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "page": 7,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supports_claims": ["C016"],
      "extraction_confidence": "high"
    }
  ],
  "claims": [
    {
      "claim_id": "C001",
      "claim_text": "The crowdsourcing approach is most efficient for digitisation projects of 10,000-60,000 features",
      "claim_type": "empirical",
      "claim_role": "core",
      "primary_function": "empirical_pattern",
      "claim_nature": "evaluative",
      "verbatim_quote": "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000-60,000 features, but may offer advantages for datasets as small as a few hundred records.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E001", "E002"],
      "supports_claims": [],
      "implicit_assumptions": ["IA001", "IA002"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C002",
      "claim_text": "The crowdsourcing approach may offer advantages for datasets as small as a few hundred records",
      "claim_type": "empirical",
      "claim_role": "core",
      "primary_function": "empirical_pattern",
      "claim_nature": "evaluative",
      "verbatim_quote": "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000-60,000 features, but may offer advantages for datasets as small as a few hundred records.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E001"],
      "supports_claims": [],
      "implicit_assumptions": ["IA003"],
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C003",
      "claim_text": "Systems designed for field data collection running on mobile devices can be profitably customised to serve as participatory geospatial data systems accessible to novice volunteers",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "theoretical_interpretation",
      "claim_nature": "evaluative",
      "verbatim_quote": "Furthermore, it indicates that systems designed for field data collection, running on mobile devices, can be profitably customised to serve as participatory geospatial data systems accessible to novice volunteers.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E001", "E003", "E004"],
      "supports_claims": [],
      "implicit_assumptions": ["IA004"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C004",
      "claim_text": "The crowdsourcing approach scales better than direct digitisation by experts but requires an appropriate platform and technical skills to adapt it",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "comparative_assessment",
      "claim_nature": "comparative",
      "verbatim_quote": "Crowdsourcing scales better than direct digitisation by experts, but requires an appropriate platform and the technical skills to adapt it.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E002"],
      "supports_claims": [],
      "implicit_assumptions": ["IA005"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C005",
      "claim_text": "Automatic extraction using Machine Learning requires extensive preparation and expertise",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "verbatim_quote": "Automatic extraction using Machine Learning (ML) requires extensive preparation and expertise.",
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E002"],
      "supports_claims": ["C004"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C006",
      "claim_text": "The digitisation produced a large, accurate, analysis-ready dataset",
      "claim_type": "empirical",
      "claim_role": "intermediate",
      "primary_function": "empirical_pattern",
      "claim_nature": "descriptive",
      "verbatim_quote": "yet produced a large, accurate, analysis-ready dataset.",
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": ["E003", "E004"],
      "supports_claims": ["C003"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "error rate, dataset size",
        "comparison_type": null,
        "statistical_test": null,
        "effect_size": null
      },
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C007",
      "claim_text": "Compared to manual digitisation approaches based on desktop GIS, the approach required little training or supervision of students, used open-source software and low-cost equipment",
      "claim_type": "empirical",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "comparative",
      "verbatim_quote": "Compared to manual digitisation approaches based on desktop GIS, it required little training or supervision of students, used open-source software and low-cost equipment, yet produced a large, accurate, analysis-ready dataset.",
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": ["E004"],
      "supports_claims": ["C003", "C004"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C008",
      "claim_text": "The approach complements Machine Learning in that it requires less technical expertise, time, and resourcing to undertake",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "comparative_assessment",
      "claim_nature": "comparative",
      "verbatim_quote": "It complements Machine Learning (ML) and other automated approaches in that it requires less technical expertise, time, and resourcing to undertake.",
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": [],
      "supports_claims": [],
      "implicit_assumptions": ["IA006"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C009",
      "claim_text": "The approach is suitable for projects working with small to mid-sized data sources (100s-10,000s of features) that do not warrant the investment needed for successful ML-based data extraction",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "methodological_justification",
      "claim_nature": "evaluative",
      "verbatim_quote": "Such an approach is suitable for projects working with small to mid-sized data sources (100s-10,000s of features) that do not warrant the investment needed for successful ML-based data extraction - as well as for exploratory work preceding automated analyses or the production of the training and quality assurance datasets needed for ML.",
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": [],
      "supports_claims": [],
      "implicit_assumptions": ["IA007"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C010",
      "claim_text": "The approach taken can be replicated using other mobile GIS systems, scaled up, or applied to other types of archaeological features",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "future_direction",
      "claim_nature": "predictive",
      "verbatim_quote": "The approach taken here can be replicated using other mobile GIS systems, scaled up, or applied to other types of archaeological features.",
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": [],
      "supports_claims": ["C003"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C011",
      "claim_text": "The approach shows promise as a low-cost means of extracting locations and other data from historical maps concerning endangered and poorly documented material remains",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "theoretical_interpretation",
      "claim_nature": "evaluative",
      "verbatim_quote": "As such, it shows promise as a low-cost means of extracting locations and other data from historical maps concerning endangered and poorly documented material remains.",
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": [],
      "supports_claims": [],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C012",
      "claim_text": "Burial mounds are an irreplaceable but endangered aspect of Bulgarian cultural heritage, making their systematic recording and registration an urgent undertaking",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "methodological_justification",
      "claim_nature": "evaluative",
      "verbatim_quote": "Burial mounds are an irreplaceable - but endangered - aspect of Bulgarian cultural heritage, making their systematic recording and registration an urgent undertaking for both research and cultural heritage management.",
      "location": {
        "section": "Introduction",
        "subsection": "1.2. Burial mounds in Bulgarian archaeology",
        "page": 2,
        "start_paragraph": 3,
        "end_paragraph": 3
      },
      "supported_by": ["E005"],
      "supports_claims": [],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C013",
      "claim_text": "The 2010 desktop GIS digitisation effort failed due to volunteer attrition and demands on staff time",
      "claim_type": "empirical",
      "claim_role": "supporting",
      "primary_function": "comparative_assessment",
      "claim_nature": "descriptive",
      "verbatim_quote": "In the end, volunteer attrition combined with demands on staff time during the height of fieldwork rendered this approach unsuccessful.",
      "location": {
        "section": "Approach",
        "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
        "page": 4,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": ["E008"],
      "supports_claims": ["C004", "C007"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C014",
      "claim_text": "The customised mobile application met fundamental usability requirements resulting in low volunteer attrition",
      "claim_type": "empirical",
      "claim_role": "intermediate",
      "primary_function": "empirical_pattern",
      "claim_nature": "evaluative",
      "verbatim_quote": "By contrast, our customised application met fundamental usability requirements (e.g., Nielsen, 2012), both due to careful design of the customisation itself, and the underlying platform's implementation of Google's Material Design guidelines.",
      "location": {
        "section": "Results",
        "subsection": "3.3. Digitisation comparison with desktop GIS",
        "page": 7,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supported_by": ["E008"],
      "supports_claims": ["C003", "C006"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C015",
      "claim_text": "The best digitisers could be both fast and accurate, while the poorest digitisers were neither fast nor accurate",
      "claim_type": "empirical",
      "claim_role": "supporting",
      "primary_function": "empirical_pattern",
      "claim_nature": "descriptive",
      "verbatim_quote": "The best digitisers, furthermore, could be both fast and accurate, while the poorest digitisers were often neither fast nor accurate (see Tables 1-3).",
      "location": {
        "section": "Results",
        "subsection": "3.5. Data quality",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E009"],
      "supports_claims": ["C006"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C016",
      "claim_text": "Overall digitisation accuracy was high, over 94% for processed maps",
      "claim_type": "empirical",
      "claim_role": "intermediate",
      "primary_function": "empirical_pattern",
      "claim_nature": "evaluative",
      "verbatim_quote": "Unlike some volunteer digitisation projects, overall accuracy was high, over 94% for processed maps.",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E003", "E010"],
      "supports_claims": ["C003", "C006"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C017",
      "claim_text": "Crowdsourcing approach payoff threshold versus staff digitisation is 3,400-4,300 features",
      "claim_type": "empirical",
      "claim_role": "core",
      "primary_function": "empirical_pattern",
      "claim_nature": "evaluative",
      "verbatim_quote": "After brief workspace setup, project staff with desktop GIS experience could digitise at a sustained rate of 60-75 features per staff-hour.",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "page": 9,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": ["E002"],
      "supports_claims": ["C001"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "feature count threshold",
        "comparison_type": "threshold",
        "statistical_test": null,
        "effect_size": null
      },
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C018",
      "claim_text": "Crowdsourcing approach becomes worthwhile above approximately 60,000 records when compared to machine learning",
      "claim_type": "empirical",
      "claim_role": "core",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "verbatim_quote": "To summarise in round numbers, a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000-60,000 records, assuming similar feature characteristics and data collection requirements (see Table 5).",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2. Machine learning versus crowdsourcing",
        "page": 10,
        "start_paragraph": 7,
        "end_paragraph": 7
      },
      "supported_by": ["E001", "E002"],
      "supports_claims": ["C001", "C008"],
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C019",
      "claim_text": "A typical project in history or archaeology may not be able to incorporate ML successfully but could deploy a collaborative geospatial system for crowdsourcing",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "methodological_justification",
      "claim_nature": "evaluative",
      "verbatim_quote": "Today, a typical project in history or archaeology - often small, under-resourced, and pursuing several research activities - may not be able to dedicate the personnel, infrastructure, or attention needed to incorporate ML successfully, but could deploy a collaborative geospatial system for crowdsourcing map digitisation.",
      "location": {
        "section": "Discussion",
        "subsection": "4.3. Overall feasibility",
        "page": 10,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supported_by": [],
      "supports_claims": ["C008", "C009", "C011"],
      "extraction_confidence": "high"
    }
  ],
  "implicit_arguments": [
    {
      "implicit_id": "IA001",
      "implicit_text": "Efficiency is measured primarily by staff time required rather than total person-hours or other metrics",
      "type": "unstated_assumption",
      "verbatim_quote": null,
      "trigger_text": [
        "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000-60,000 features"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "start_paragraph": 1,
          "end_paragraph": 1
        }
      ],
      "inference_reasoning": "The paper uses 'most efficient' without defining efficiency criteria. Given the emphasis throughout on staff time constraints and the 57 vs 184 hour split between staff and volunteers, the claim assumes efficiency is evaluated primarily from the perspective of staff time investment rather than total time, cost, or other efficiency metrics.",
      "connects_evidence": [],
      "enables_claim": ["C001"],
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "extraction_confidence": "high"
    },
    {
      "implicit_id": "IA002",
      "implicit_text": "The 10,000-60,000 feature range generalises beyond the specific characteristics of burial mound symbols in Soviet topographic maps",
      "type": "unstated_assumption",
      "verbatim_quote": null,
      "trigger_text": [
        "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000-60,000 features",
        "Such symbols occurred at a high density, averaging about 200 per tile (0.5 per sq km), with counts per tile ranging from about 50 to 400.",
        "The mound symbols were moderately obtrusive; some aspects of shape or colour were shared with other map symbols."
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Approach",
          "subsection": "2.1. Archaeological features in Soviet topographic maps",
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Approach",
          "subsection": "2.1. Archaeological features in Soviet topographic maps",
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "The threshold estimate is presented as generalisable guidance for 'digitisation projects' broadly, but is derived from a single case involving symbols with specific characteristics (high density, moderate obtrusiveness, moderate similarity to other symbols). The paper assumes these findings transfer to features with different densities, obtrusiveness, or complexity without explicitly testing or qualifying this assumption.",
      "connects_evidence": ["E001", "E002"],
      "enables_claim": ["C001"],
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "extraction_confidence": "high"
    },
    {
      "implicit_id": "IA003",
      "implicit_text": "Staff time is at a premium or operating alongside fieldwork with competing demands makes smaller datasets viable",
      "type": "bridging_claim",
      "verbatim_quote": null,
      "trigger_text": [
        "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000-60,000 features, but may offer advantages for datasets as small as a few hundred records.",
        "Projects where staff time is at a premium, or that operate alongside fieldwork where staff have many competing demands, may find it valuable for smaller datasets (even those below 1,000 records)."
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2. Machine learning versus crowdsourcing",
          "start_paragraph": 7,
          "end_paragraph": 7
        }
      ],
      "inference_reasoning": "The Abstract claims advantages for 'datasets as small as a few hundred records' but doesn't explain what conditions make this true. The Discussion section reveals this depends on whether 'staff time is at a premium' or work occurs 'alongside fieldwork'. This conditional logic is absent from the Abstract claim, creating a gap between the stated claim and the conditions under which it holds.",
      "connects_evidence": ["E001", "E002"],
      "enables_claim": ["C002"],
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "extraction_confidence": "high"
    },
    {
      "implicit_id": "IA004",
      "implicit_text": "Usability principles from kinetic fieldwork translate effectively to deskbound digitisation work",
      "type": "logical_implication",
      "verbatim_quote": null,
      "trigger_text": [
        "Furthermore, it indicates that systems designed for field data collection, running on mobile devices, can be profitably customised to serve as participatory geospatial data systems accessible to novice volunteers.",
        "Third, it allowed us to test the idea that usability approaches from data capture during kinetic fieldwork were beneficially transferable to digitisation work."
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Approach",
          "subsection": "2.3. Using a mobile application for map digitisation",
          "start_paragraph": 3,
          "end_paragraph": 3
        }
      ],
      "inference_reasoning": "The Abstract claims that field data collection systems 'can be profitably customised' for participatory digitisation, implying successful transfer of design principles. The Approach section reveals this was actually a hypothesis being tested ('test the idea'), not an established principle. The successful outcome depends on the unstated assumption that human-computer interaction principles for kinetic fieldwork (minimising interactions, conforming to workflows, aggressive validation) apply equally to stationary digitisation work.",
      "connects_evidence": ["E003", "E004"],
      "enables_claim": ["C003"],
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "extraction_confidence": "medium"
    },
    {
      "implicit_id": "IA005",
      "implicit_text": "Comparison baseline for 'scales better' is the direct digitisation by experts approach attempted in 2010",
      "type": "unstated_assumption",
      "verbatim_quote": null,
      "trigger_text": [
        "Crowdsourcing scales better than direct digitisation by experts, but requires an appropriate platform and the technical skills to adapt it.",
        "In 2010, project staff worked with student volunteers to digitise map features using ArcGIS.",
        "Our experience was much like that of other projects: novice volunteers found learning to configure and navigate desktop GIS challenging; many quit and those who continued required ongoing support."
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Approach",
          "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Approach",
          "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "The Abstract claim that crowdsourcing 'scales better than direct digitisation by experts' is made without defining what 'direct digitisation by experts' means or what the comparison is based on. Later sections reveal this refers to their failed 2010 attempt using volunteers with desktop GIS, which they characterise as volunteer-based desktop GIS rather than true 'expert' digitisation. The comparison baseline is assumed from context but never explicitly stated in the Abstract.",
      "connects_evidence": [],
      "enables_claim": ["C004"],
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "extraction_confidence": "high"
    },
    {
      "implicit_id": "IA006",
      "implicit_text": "'Complements' means suitable for different use cases rather than being a substitute or alternative",
      "type": "unstated_assumption",
      "verbatim_quote": null,
      "trigger_text": [
        "It complements Machine Learning (ML) and other automated approaches in that it requires less technical expertise, time, and resourcing to undertake.",
        "The approaches are not exclusive, therefore, but complementary."
      ],
      "trigger_locations": [
        {
          "section": "Introduction",
          "subsection": null,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Discussion",
          "subsection": "4.2. Combining crowdsourcing and ML approaches",
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "The claim that crowdsourcing 'complements' ML uses 'complements' to mean 'fills a different niche' (different dataset sizes, resource requirements), not that the two approaches work together on the same dataset. This definitional assumption is unstated in Introduction but clarified in Discussion where they note ML training sets could use crowdsourcing, suggesting complementarity through sequential use rather than mutual exclusivity.",
      "connects_evidence": [],
      "enables_claim": ["C008"],
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "extraction_confidence": "medium"
    },
    {
      "implicit_id": "IA007",
      "implicit_text": "There exists a quantifiable 'investment threshold' for ML that makes it unwarranted below certain dataset sizes",
      "type": "unstated_assumption",
      "verbatim_quote": null,
      "trigger_text": [
        "Such an approach is suitable for projects working with small to mid-sized data sources (100s-10,000s of features) that do not warrant the investment needed for successful ML-based data extraction",
        "A minimum threshold for automation can be extrapolated from our 2017-18 fieldwork and the Urban Occupations Project."
      ],
      "trigger_locations": [
        {
          "section": "Introduction",
          "subsection": null,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2. Machine learning versus crowdsourcing",
          "start_paragraph": 4,
          "end_paragraph": 4
        }
      ],
      "inference_reasoning": "The Introduction claims certain dataset sizes 'do not warrant the investment needed for successful ML' without explaining how such thresholds are determined. This assumes there exists a definable, generalisable break-even point for ML investment. The Discussion attempts to extrapolate such a threshold but acknowledges significant assumptions (feature characteristics, complexity, ML expert time availability) that affect where this threshold actually falls, revealing the assumed threshold is context-dependent rather than universal.",
      "connects_evidence": [],
      "enables_claim": ["C009"],
      "location": {
        "section": "Introduction",
        "page": 1,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "extraction_confidence": "high"
    }
  ],
  "research_designs": [
    {
      "design_id": "RD001",
      "design_text": "Comparative research design evaluating crowdsourcing approach against desktop GIS and machine learning alternatives",
      "design_type": "study_design",
      "design_status": "explicit",
      "verbatim_quote": "This paper argues that crowdsourcing offers advantages compared to alternative approaches under many, if not most, map digitisation scenarios.",
      "location": {
        "section": "Introduction",
        "page": 3,
        "start_paragraph": 3,
        "end_paragraph": 3
      },
      "enables_methods": ["M001", "M002", "M003"],
      "validates_claims": ["C001", "C004", "C008", "C017", "C018"]
    },
    {
      "design_id": "RD002",
      "design_text": "Usability-driven system design adapting field data collection principles to deskbound digitisation",
      "design_type": "theoretical_framework",
      "design_status": "explicit",
      "verbatim_quote": "Third, it allowed us to test the idea that usability approaches from data capture during kinetic fieldwork were beneficially transferable to digitisation work.",
      "location": {
        "section": "Approach",
        "subsection": "2.3. Using a mobile application for map digitisation",
        "page": 4,
        "start_paragraph": 3,
        "end_paragraph": 3
      },
      "enables_methods": ["M004"],
      "validates_claims": ["C003", "C014"]
    },
    {
      "design_id": "RD-IMP-001",
      "design_text": "Dataset size threshold determination for crowdsourcing viability",
      "design_type": "study_design",
      "design_status": "implicit",
      "trigger_text": [
        "After brief workspace setup, project staff with desktop GIS experience could digitise at a sustained rate of 60-75 features per staff-hour.",
        "To summarise in round numbers, a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000-60,000 records"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
          "page": 9,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2. Machine learning versus crowdsourcing",
          "page": 10,
          "start_paragraph": 7,
          "end_paragraph": 7
        }
      ],
      "inference_reasoning": "The Discussion systematically calculates payoff thresholds (3,400-4,300 features vs staff digitisation; 10,000-60,000 optimal range; 60,000+ for ML comparison), but the Introduction and Methods never state that threshold determination was a design objective. The extensive threshold analysis in Discussion reveals this was a strategic design goal, but it was never explicitly framed as such in the research design sections.",
      "implicit_metadata": {
        "basis": "inferred_from_results",
        "transparency_gap": "Design objective of threshold determination not stated in Methods or Introduction. Only becomes apparent through Discussion analysis focus on calculating specific breakeven points.",
        "assessability_impact": "Cannot assess whether threshold determination was pre-planned (part of original design) or emerged post-hoc during analysis. This affects interpretation of whether findings genuinely generalise or represent post-hoc rationalisation of observed data.",
        "reconstruction_confidence": "high"
      },
      "location": {
        "section": "Discussion",
        "page": 9,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "validates_claims": ["C001", "C017", "C018"]
    },
    {
      "design_id": "RD-IMP-002",
      "design_text": "Comparative positioning relative to machine learning approaches",
      "design_type": "study_design",
      "design_status": "implicit",
      "trigger_text": [
        "Crowdsourcing scales better than direct digitisation by experts, but requires an appropriate platform and the technical skills to adapt it.",
        "It complements Machine Learning (ML) and other automated approaches in that it requires less technical expertise, time, and resourcing to undertake.",
        "A minimum threshold for automation can be extrapolated from our 2017-18 fieldwork and the Urban Occupations Project."
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "page": 1,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Introduction",
          "page": 1,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2. Machine learning versus crowdsourcing",
          "page": 10,
          "start_paragraph": 4,
          "end_paragraph": 4
        }
      ],
      "inference_reasoning": "Throughout the paper, the approach is systematically positioned relative to ML (Abstract, Introduction, Discussion 4.1.2, Discussion 4.2), including detailed comparison calculations. However, the Introduction and Methods never explicitly state that ML comparison was a strategic design objective. The comparative framing pervades the entire paper, suggesting this was a core design choice, but it's never formally articulated as such.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Comparative positioning relative to ML pervades the paper but is never stated as explicit design objective in Methods. The Introduction mentions ML but doesn't frame ML comparison as a strategic research goal.",
        "assessability_impact": "Cannot determine if ML comparison was pre-planned design choice or emerged during writing. Affects assessment of whether comparison criteria and threshold calculations represent genuine research design or post-hoc justification.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Abstract",
        "page": 1,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "validates_claims": ["C005", "C008", "C009", "C018", "C019"]
    }
  ],
  "methods": [
    {
      "method_id": "M001",
      "method_text": "FAIMS Mobile platform customisation for crowdsourced map digitisation",
      "method_type": "data_collection",
      "method_status": "explicit",
      "verbatim_quote": "For the 2017-2018 field seasons, TRAP staff created a simplified and streamlined data capture system built using the FAIMS Mobile platform.",
      "location": {
        "section": "Approach",
        "page": 3,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "realized_through_protocols": ["P001", "P002", "P003", "P004"],
      "implements_designs": ["RD001"],
      "supports_claims": ["C003", "C006", "C014"]
    },
    {
      "method_id": "M002",
      "method_text": "Volunteer-based crowdsourcing using undergraduate field school participants",
      "method_type": "sampling",
      "method_status": "explicit",
      "verbatim_quote": "The task of digitising potentially thousands of mounds provided an opportunity to involve students in authentic research.",
      "location": {
        "section": "Approach",
        "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
        "page": 4,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "sampling_strategy": {
        "selection_criteria": "Undergraduate students in TRAP field school",
        "sample_characteristics": "Novice volunteers with no GIS experience"
      },
      "realized_through_protocols": ["P005"],
      "implements_designs": ["RD001"],
      "supports_claims": ["C007", "C013", "C014"]
    },
    {
      "method_id": "M003",
      "method_text": "Time-on-task measurement comparing digitisation approaches",
      "method_type": "data_collection",
      "method_status": "explicit",
      "verbatim_quote": "To measure inputs, we collated the amount of time spent by various participants in the process, including the student programmer who instantiated the customisation, the student volunteers who undertook the digitisation, and project staff who configured the system, supported volunteers, exported data, and checked for errors.",
      "location": {
        "section": "Approach",
        "subsection": "2.5. Evaluating the digitisation approach",
        "page": 6,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "realized_through_protocols": ["P006"],
      "implements_designs": ["RD001"],
      "supports_claims": ["C001", "C017", "C018"]
    },
    {
      "method_id": "M004",
      "method_text": "Streamlined GIS functionality limiting interface to essential digitisation tasks",
      "method_type": "quality_control",
      "method_status": "explicit",
      "verbatim_quote": "As such, it stripped GIS functionality to its essentials, focusing on three tasks: layer selection, shape digitisation, and annotation, with validation and automation to improve data quality.",
      "location": {
        "section": "Approach",
        "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
        "page": 4,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "realized_through_protocols": ["P002", "P003"],
      "implements_designs": ["RD002"],
      "supports_claims": ["C007", "C014"]
    },
    {
      "method_id": "M-IMP-001",
      "method_text": "Quality assurance methodology for student digitisation work",
      "method_type": "quality_control",
      "method_status": "implicit",
      "trigger_text": [
        "Finally, project staff reviewed randomly selected digitisation work completed by volunteers to characterise errors.",
        "Reports from on-device validation, as well as quality assurance by project leaders, suggest that digitisation accuracy was good."
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "2.5. Evaluating the digitisation approach",
          "page": 6,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Results",
          "subsection": "3.5. Data quality",
          "page": 7,
          "start_paragraph": 1,
          "end_paragraph": 1
        }
      ],
      "inference_reasoning": "Methods 2.5 mentions that 'project staff reviewed randomly selected digitisation work completed by volunteers to characterise errors', and Results 3.5 refers to 'quality assurance by project leaders'. However, the Methods section provides no description of the QA methodology: what criteria were used, what constituted an error, how errors were categorised, or who performed the QA. Results Table 3 presents a detailed error taxonomy (false positives, double-marked, false negatives, classification errors) suggesting a sophisticated QA system, but this system is never described.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "QA mentioned in Methods and Results but methodology never described. No information on: QA criteria, error definitions, categorisation system, personnel training, inter-rater reliability, or QA workflow.",
        "assessability_impact": "Cannot assess appropriateness or rigor of QA methodology. The error taxonomy in Table 3 appears sophisticated but its development and application are opaque. Affects credibility of reported error rates.",
        "reconstruction_confidence": "low"
      },
      "location": {
        "section": "Approach",
        "subsection": "2.5. Evaluating the digitisation approach",
        "page": 6,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "realized_through_protocols": ["P-IMP-008"],
      "implements_designs": ["RD001"],
      "supports_claims": ["C006", "C016"]
    },
    {
      "method_id": "M-IMP-002",
      "method_text": "Map selection and prioritisation methodology",
      "method_type": "sampling",
      "method_status": "implicit",
      "trigger_text": [
        "In 2017, it was used for a total of 125.8 person-hours concentrated across five rainy days, during which time 8,343 features were digitised from 42 Soviet topographic maps (ca. 17,000 sq km).",
        "In 2018, use was more sporadic; participants who stayed at the base for any reason sometimes undertook digitisation. The system was used for 63.6 person-hours, with 2,484 features recorded from 16 maps (ca. 6,500 sq km)"
      ],
      "trigger_locations": [
        {
          "section": "Results",
          "subsection": "3.2. Student-volunteer digitisation velocity and volume",
          "page": 7,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Results",
          "subsection": "3.2. Student-volunteer digitisation velocity and volume",
          "page": 7,
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "Results reports that 42 maps were digitised in 2017 and 16 maps in 2018, covering specific regions (Yambol). Approach 2.1 states maps were available covering 'over 20,000 sq km of Soviet military 1:50,000 topographic maps covering southeast Bulgaria', suggesting a larger corpus was available than was digitised. However, no method is described for selecting which 58 maps (out of the larger set) were prioritised for digitisation.",
      "implicit_metadata": {
        "basis": "inferred_from_results",
        "transparency_gap": "Results reveal subset of available maps were digitised (58 out of larger set covering 20,000+ sq km) but Methods contains no description of selection criteria, prioritisation logic, or sampling strategy.",
        "assessability_impact": "Cannot assess whether map selection introduced sampling bias. Unknown if maps were selected for mound density, coverage completeness, map quality, or opportunistic availability. Affects generalisability of efficiency and accuracy findings.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Results",
        "subsection": "3.2. Student-volunteer digitisation velocity and volume",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "sampling_strategy": {
        "selection_criteria": "Not documented - inferred to be availability and project priorities",
        "sample_characteristics": "58 maps covering ca. 23,500 sq km from larger corpus"
      },
      "implements_designs": ["RD001"],
      "supports_claims": []
    },
    {
      "method_id": "M-IMP-003",
      "method_text": "Application performance monitoring methodology",
      "method_type": "quality_control",
      "method_status": "implicit",
      "trigger_text": [
        "FAIMS Mobile supports automated performance testing (Sobotkova et al., 2021). Such testing, however, works best for structured data input rather than map-driven recording.",
        "In use, automated extraction of coordinates from GPS into the Latitude/Longitude and Northing/Easting fields, which took three to 5 s with an empty database, took as long as 30 s once a device exceeded about 2,500 records."
      ],
      "trigger_locations": [
        {
          "section": "Results",
          "subsection": "3.4. Application performance",
          "page": 7,
          "start_paragraph": 1,
          "end_paragraph": 1
        },
        {
          "section": "Results",
          "subsection": "3.4. Application performance",
          "page": 7,
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "Results 3.4 reports specific performance degradation thresholds ('approximately 3,000-6,000 records', 'about 2,500 records', timing degradation from '3 to 5 s' to 'as long as 30 s'). This level of precision implies systematic performance monitoring. Methods mentions 'automated performance testing' exists but states it 'works best for structured data input rather than map-driven recording', suggesting standard testing was adapted. However, no description of the actual monitoring methodology used for this project is provided.",
      "implicit_metadata": {
        "basis": "inferred_from_results",
        "transparency_gap": "Results report precise performance metrics and degradation thresholds, implying systematic monitoring. Methods mentions automated testing exists but doesn't describe monitoring methodology used for this project: what was measured, how frequently, on which devices, or how thresholds were determined.",
        "assessability_impact": "Cannot assess rigor of performance monitoring or validity of reported thresholds. Unknown if monitoring was real-time, retrospective, comprehensive across all devices, or based on sample. Affects confidence in performance claims.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Results",
        "subsection": "3.4. Application performance",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "supports_claims": []
    },
    {
      "method_id": "M-IMP-004",
      "method_text": "Error categorisation and taxonomy development methodology",
      "method_type": "analysis",
      "method_status": "implicit",
      "trigger_text": [
        "Finally, project staff reviewed randomly selected digitisation work completed by volunteers to characterise errors."
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "2.5. Evaluating the digitisation approach",
          "page": 6,
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "Methods 2.5 says staff 'reviewed randomly selected digitisation work completed by volunteers to characterise errors', but Results Table 3 presents a sophisticated error taxonomy with five categories (false positive, double-marked, false negative, classification error, total errors) plus calculated rates. The development of this taxonomy and the method for categorising errors into these types is never described. This analytical framework must have been developed but is undocumented.",
      "implicit_metadata": {
        "basis": "inferred_from_results",
        "transparency_gap": "Error taxonomy presented in Results (Table 3) with no description of taxonomy development, definitions of error types, or categorisation procedure. Unknown if taxonomy was developed a priori or emerged from observed errors. No inter-coder reliability reported.",
        "assessability_impact": "Cannot assess whether error taxonomy is appropriate or comprehensive. Definitions of error types (e.g., what constitutes 'classification error' vs 'false positive') are opaque. Affects interpretation of error rates and comparability to other studies.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Approach",
        "subsection": "2.5. Evaluating the digitisation approach",
        "page": 6,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "supports_claims": ["C015", "C016"]
    }
  ],
  "protocols": [
    {
      "protocol_id": "P001",
      "protocol_text": "System customisation via FAIMS Mobile definition files",
      "protocol_type": "processing",
      "protocol_status": "explicit",
      "verbatim_quote": "Customisation is accomplished via definition files that can be shared, modified, and redeployed.",
      "location": {
        "section": "Approach",
        "subsection": "2.3. Using a mobile application for map digitisation",
        "page": 4,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "procedure_steps": ["Create definition files", "FAIMS Mobile interprets files", "Generate customised Android application"],
      "implements_methods": ["M001"]
    },
    {
      "protocol_id": "P002",
      "protocol_text": "Map preparation: tiling and adding pyramids to GeoTIFF files",
      "protocol_type": "processing",
      "protocol_status": "explicit",
      "verbatim_quote": "Map preparation (tiling, adding pyramids) required about 1.5 h.",
      "location": {
        "section": "Results",
        "subsection": "3.1. Project staff time for setup, support, and accuracy-checking",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "procedure_steps": ["Tile maps", "Add pyramids to GeoTIFF files"],
      "implements_methods": ["M001", "M004"]
    },
    {
      "protocol_id": "P003",
      "protocol_text": "Offline data collection with opportunistic synchronisation",
      "protocol_type": "recording",
      "protocol_status": "explicit",
      "verbatim_quote": "Data collection works offline, and can employ as many devices as necessary.",
      "location": {
        "section": "Approach",
        "subsection": "2.3. Using a mobile application for map digitisation",
        "page": 4,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "procedure_steps": ["Collect data offline on devices", "Synchronise opportunistically when network available"],
      "implements_methods": ["M001", "M004"]
    },
    {
      "protocol_id": "P004",
      "protocol_text": "Automated metadata creation with timestamps and author tracking",
      "protocol_type": "recording",
      "protocol_status": "explicit",
      "verbatim_quote": "It applied the spatial reference system, rendered maps in the workspace, provided layer management (including a data entry layer), enforced shape topology, displayed pre-defined controlled vocabularies for attribute terms, recorded creation time and author for each record, maintained a history of all changes to data, applied validation to ensure record completeness, merged data from multiple devices, and exported data in common formats.",
      "location": {
        "section": "Approach",
        "subsection": "2.4. Design and implementation of the recording system",
        "page": 5,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "procedure_steps": ["Record creation time automatically", "Record author for each record", "Maintain history of all changes"],
      "implements_methods": ["M001"]
    },
    {
      "protocol_id": "P005",
      "protocol_text": "Minimal volunteer training protocol (minutes of instruction)",
      "protocol_type": "recording",
      "protocol_status": "explicit",
      "verbatim_quote": "Training and supervision of students took no more than half an hour of staff time across the entire season.",
      "location": {
        "section": "Results",
        "subsection": "3.1. Project staff time for setup, support, and accuracy-checking",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "procedure_steps": ["Minimal training session", "Students begin digitising", "Minimal ongoing supervision"],
      "implements_methods": ["M002"]
    },
    {
      "protocol_id": "P006",
      "protocol_text": "Quality assurance through random map sampling and error categorisation",
      "protocol_type": "validation",
      "protocol_status": "explicit",
      "verbatim_quote": "Finally, project staff reviewed randomly selected digitisation work completed by volunteers to characterise errors.",
      "location": {
        "section": "Approach",
        "subsection": "2.5. Evaluating the digitisation approach",
        "page": 6,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "procedure_steps": ["Select maps randomly (7% sample)", "Review by project staff", "Tabulate errors and error rates"],
      "implements_methods": ["M003"]
    },
    {
      "protocol_id": "P-IMP-001",
      "protocol_text": "Map tile assignment protocol for student volunteers",
      "protocol_type": "recording",
      "protocol_status": "implicit",
      "trigger_text": [
        "Map distribution (project staff; 2.5 hours)",
        "Map distribution (project staff; 1.5 hours)"
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "Figure 3 workflow diagram",
          "page": 5,
          "start_paragraph": 1,
          "end_paragraph": 1
        }
      ],
      "inference_reasoning": "Figure 3 shows 'Map distribution' as explicit workflow step requiring staff time (2.5h in 2017, 1.5h in 2018). Results mentions 42 maps digitised in 2017 and 16 in 2018, and Results 3.5.2 mentions 'participants failed to digitise some assigned maps'. This indicates maps were assigned to specific students, but no assignment protocol is described: how maps were allocated, whether by region/difficulty/random selection, or whether students could choose.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Map assignment mentioned (Figure 3 workflow, Results language about 'assigned maps') but assignment procedure never described. Unknown: assignment criteria, allocation method, whether students had choice, how workload was balanced.",
        "assessability_impact": "Cannot assess whether assignment strategy affected outcomes. If complex maps were assigned to experienced digitisers, reported accuracy/velocity may not reflect novice capability. Assignment strategy could affect scalability claims.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Approach",
        "subsection": "Figure 3",
        "page": 5,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "implements_methods": ["M002"]
    },
    {
      "protocol_id": "P-IMP-002",
      "protocol_text": "Student volunteer recruitment and selection protocol",
      "protocol_type": "sampling",
      "protocol_status": "implicit",
      "trigger_text": [
        "The task of digitising potentially thousands of mounds provided an opportunity to involve students in authentic research.",
        "Our students came from a range of academic backgrounds in Arts and Humanities. Most had no training in archaeology, cartography, or digital methods"
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
          "page": 4,
          "start_paragraph": 1,
          "end_paragraph": 1
        }
      ],
      "inference_reasoning": "Approach 2.2 describes student characteristics ('range of academic backgrounds in Arts and Humanities', 'Most had no training in archaeology, cartography, or digital methods', motivations included 'curiosity about field archaeology, the desire to travel outside Australia'). Tables 1-2 show 5 students in 2017 and 4 students in 2018, but no recruitment protocol is described: how students were approached, selection criteria, informed consent, or whether participation was voluntary vs required for field school credit.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Student characteristics described but recruitment protocol never documented. Unknown: recruitment method, selection criteria, informed consent procedure, whether participation was voluntary or required, compensation/credit arrangements.",
        "assessability_impact": "Cannot assess selection bias or generalisability to other volunteer populations. If participation was required rather than voluntary, motivation and persistence may differ from true crowdsourcing contexts. Affects external validity of volunteer performance findings.",
        "reconstruction_confidence": "low"
      },
      "location": {
        "section": "Approach",
        "subsection": "2.2. Crowdsourcing digitisation with field-school participants",
        "page": 4,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "implements_methods": ["M002"]
    },
    {
      "protocol_id": "P-IMP-003",
      "protocol_text": "Performance load monitoring protocol for database size thresholds",
      "protocol_type": "validation",
      "protocol_status": "implicit",
      "trigger_text": [
        "In use, automated extraction of coordinates from GPS into the Latitude/Longitude and Northing/Easting fields, which took three to 5 s with an empty database, took as long as 30 s once a device exceeded about 2,500 records.",
        "Deteriorating performance was mitigated by exporting all data and instantiating a new and empty version of the application."
      ],
      "trigger_locations": [
        {
          "section": "Results",
          "subsection": "3.4. Application performance",
          "page": 7,
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "Results 3.4 reports that staff detected performance degradation 'once a device exceeded about 2,500 records' and implemented a mitigation ('exporting all data and instantiating a new and empty version'). This indicates a monitoring protocol must have existed to detect the threshold and trigger mitigation, but this protocol is never described: how performance was measured, who monitored it, how frequently, or what triggered the refresh decision.",
      "implicit_metadata": {
        "basis": "inferred_from_results",
        "transparency_gap": "Results report specific performance threshold (2,500 records) and mitigation response but monitoring protocol never described. Unknown: measurement method, monitoring frequency, who performed monitoring, decision criteria for triggering database refresh.",
        "assessability_impact": "Cannot assess whether performance monitoring was systematic or ad-hoc. Unknown if 2,500 threshold is precise or approximate. Unclear if all devices were monitored equally or if some experienced worse degradation. Affects confidence in scalability claims.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Results",
        "subsection": "3.4. Application performance",
        "page": 7,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "parameters": {
        "degradation_threshold": "2500 records per device"
      },
      "expected_information_missing": [
        "Performance monitoring method",
        "Monitoring frequency",
        "Decision criteria for database refresh",
        "Whether threshold varied by device"
      ],
      "implements_methods": ["M-IMP-003"]
    },
    {
      "protocol_id": "P-IMP-004",
      "protocol_text": "Data export and database refresh protocol",
      "protocol_type": "processing",
      "protocol_status": "implicit",
      "trigger_text": [
        "Deteriorating performance was mitigated by exporting all data and instantiating a new and empty version of the application. Since data structures were identical, aggregation of multiple exports was trivial."
      ],
      "trigger_locations": [
        {
          "section": "Results",
          "subsection": "3.4. Application performance",
          "page": 7,
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "Results 3.4 describes a data management procedure triggered by performance degradation: export data, refresh application, aggregate exports. This procedure must have been defined and executed multiple times (Tables 1-2 show students creating 2,302 and 1,799 records, well above 2,500 threshold), but the procedure is never documented: export format, validation steps, aggregation method, backup procedures, or how student work was coordinated during refresh.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Data export/refresh procedure mentioned as mitigation strategy but procedural details never described. Unknown: export format and validation, aggregation methodology, backup procedures, coordination with active users, testing after refresh.",
        "assessability_impact": "Cannot assess data integrity risks during export/refresh cycles. Unknown if all data was successfully exported and aggregated or if some records were lost. Procedure mentions aggregation was 'trivial' but provides no verification. Affects confidence in dataset completeness.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Results",
        "subsection": "3.4. Application performance",
        "page": 7,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "procedure_steps": ["Export all data from device", "Instantiate new empty application", "Aggregate multiple exports"],
      "expected_information_missing": [
        "Export format and validation",
        "Aggregation procedure and verification",
        "Backup procedures",
        "User coordination during refresh"
      ],
      "implements_methods": ["M001"]
    },
    {
      "protocol_id": "P-IMP-005",
      "protocol_text": "FAIMS Mobile server infrastructure configuration and setup",
      "protocol_type": "processing",
      "protocol_status": "implicit",
      "trigger_text": [
        "Briefly, FAIMS Mobile is a server-client platform that generates customised Android applications for data collection during offline field research.",
        "Setup of the server and configuration of the client devices in the field required 3 h from staff."
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "2.3. Using a mobile application for map digitisation",
          "page": 4,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Results",
          "subsection": "3.1. Project staff time for setup, support, and accuracy-checking",
          "page": 7,
          "start_paragraph": 1,
          "end_paragraph": 1
        }
      ],
      "inference_reasoning": "Approach 2.3 describes FAIMS Mobile as 'server-client platform' and Results 3.1 mentions 'Setup of the server and configuration of the client devices in the field required 3 h from staff'. This indicates server infrastructure existed and was configured, but no details are provided: server hardware specifications, network configuration, backup infrastructure, capacity planning for simultaneous users, or server location (local vs cloud).",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Server setup mentioned (3h staff time) but infrastructure specifications never described. Unknown: server hardware, network architecture, backup systems, capacity planning, physical location, reliability measures.",
        "assessability_impact": "Cannot assess infrastructure adequacy for claimed scalability. Server specifications affect ability to support 'as many devices as necessary' (Approach 2.3). Missing backup details raise data integrity questions. Affects reproducibility by other projects.",
        "reconstruction_confidence": "low"
      },
      "location": {
        "section": "Results",
        "subsection": "3.1. Project staff time for setup, support, and accuracy-checking",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "expected_information_missing": [
        "Server hardware specifications",
        "Network architecture and connectivity",
        "Backup and redundancy systems",
        "Capacity planning for concurrent users",
        "Server physical location"
      ],
      "implements_methods": ["M001"]
    },
    {
      "protocol_id": "P-IMP-006",
      "protocol_text": "Mobile device distribution and management protocol",
      "protocol_type": "recording",
      "protocol_status": "implicit",
      "trigger_text": [
        "Data collection works offline, and can employ as many devices as necessary.",
        "only two of 12 students brought computers, and none brought mice, but all had mobile devices"
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "2.3. Using a mobile application for map digitisation",
          "page": 4,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Approach",
          "subsection": "2.3. Using a mobile application for map digitisation",
          "page": 4,
          "start_paragraph": 3,
          "end_paragraph": 3
        }
      ],
      "inference_reasoning": "Approach 2.3 states system 'can employ as many devices as necessary' and mentions 'all had mobile devices', implying students used their own devices. However, device management is never described: whether project provided devices or students used personal phones, device specifications/requirements, application installation procedure, device security/backup, or synchronisation coordination among multiple devices.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Multiple device usage mentioned but device management never described. Unknown: device ownership (project vs personal), specifications/requirements, installation procedure, device security, data backup, synchronisation scheduling, heterogeneity handling.",
        "assessability_impact": "Cannot assess device heterogeneity impact on performance. If students used personal devices with varying specs, reported performance may not be reproducible. Device ownership affects replicability and cost claims. Security of personal devices for research data raises ethical questions.",
        "reconstruction_confidence": "low"
      },
      "location": {
        "section": "Approach",
        "subsection": "2.3. Using a mobile application for map digitisation",
        "page": 4,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "expected_information_missing": [
        "Device ownership (project vs personal)",
        "Device specifications and requirements",
        "Application installation procedure",
        "Device security and data protection",
        "Synchronisation scheduling and coordination"
      ],
      "implements_methods": ["M001"]
    },
    {
      "protocol_id": "P-IMP-007",
      "protocol_text": "GPS coordinate extraction and repair protocol",
      "protocol_type": "processing",
      "protocol_status": "implicit",
      "trigger_text": [
        "Spatial data omissions resulted from a failure of the software to populate the latitude and longitude fields from the application's SpatiaLite geodatabase due to users moving through the forms too quickly",
        "Since the geodatabase preserved geometries, spatial omissions were corrected by re-extracting latitude and longitude; only two data points could not be recovered."
      ],
      "trigger_locations": [
        {
          "section": "Results",
          "subsection": "3.5.1. Recoverable data omissions and incomplete records",
          "page": 7,
          "start_paragraph": 1,
          "end_paragraph": 1
        }
      ],
      "inference_reasoning": "Results 3.5.1 describes a data recovery procedure: spatial omissions were 'corrected by re-extracting latitude and longitude' from the geodatabase. This correction was applied to 192 records in 2017 and 13 in 2018, indicating a systematic repair procedure, but the procedure is never described: what tool was used for extraction, how geometries were accessed, what verification was performed, or why two records could not be recovered.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Data repair procedure mentioned (re-extracting lat/long from geodatabase) but protocol never described. Unknown: extraction tool/script, verification procedure, handling of failed extractions, documentation of changes made to records.",
        "assessability_impact": "Cannot assess repair procedure accuracy or completeness. Unknown why two records failed recovery. No information on whether repaired records were validated against original source maps. Affects confidence in spatial data quality for affected records (2.06% of dataset).",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Results",
        "subsection": "3.5.1. Recoverable data omissions and incomplete records",
        "page": 7,
        "start_paragraph": 1,
        "end_paragraph": 1
      },
      "procedure_steps": ["Identify records with empty lat/long", "Extract coordinates from geodatabase", "Populate lat/long fields", "Verify extraction"],
      "expected_information_missing": [
        "Extraction tool or script used",
        "Verification procedure",
        "Why two records failed recovery",
        "Documentation of repairs made"
      ],
      "implements_methods": ["M001"]
    },
    {
      "protocol_id": "P-IMP-008",
      "protocol_text": "Random map sampling protocol for quality assurance",
      "protocol_type": "validation",
      "protocol_status": "implicit",
      "trigger_text": [
        "Finally, project staff reviewed randomly selected digitisation work completed by volunteers to characterise errors.",
        "Second, a review by project staff of four randomly selected maps (7% of the total) found 49 errors from a true count of 834 features, a 5.87% error rate (see Table 3)."
      ],
      "trigger_locations": [
        {
          "section": "Approach",
          "subsection": "2.5. Evaluating the digitisation approach",
          "page": 6,
          "start_paragraph": 2,
          "end_paragraph": 2
        },
        {
          "section": "Results",
          "subsection": "3.5.2. Digitisation errors",
          "page": 7,
          "start_paragraph": 2,
          "end_paragraph": 2
        }
      ],
      "inference_reasoning": "Methods 2.5 says staff 'reviewed randomly selected digitisation work' and Results 3.5.2 specifies 'four randomly selected maps (7% of the total)'. Random selection implies a sampling protocol, but the protocol is never described: what randomisation method was used, whether stratification was employed, sampling frame, or justification for 7% sample size.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Random sampling mentioned but sampling protocol never described. Unknown: randomisation method (random number generator, drawing lots, systematic sampling), stratification strategy, sampling frame, sample size justification, whether sampling occurred independently or by student.",
        "assessability_impact": "Cannot assess sampling representativeness or generalisability of error rates from 7% sample to full dataset. Unknown if sample was stratified by student, time period, map complexity, or region. Sampling method affects statistical validity of error rate estimates.",
        "reconstruction_confidence": "medium"
      },
      "location": {
        "section": "Approach",
        "subsection": "2.5. Evaluating the digitisation approach",
        "page": 6,
        "start_paragraph": 2,
        "end_paragraph": 2
      },
      "parameters": {
        "sample_size": "4 maps",
        "sample_proportion": "7% of total maps"
      },
      "expected_information_missing": [
        "Randomisation method",
        "Stratification strategy",
        "Sample size justification",
        "Sampling frame definition"
      ],
      "implements_methods": ["M-IMP-001"]
    }
  ],
  "extraction_notes": {
    "pass": 1,
    "section_extracted": "RDMAP Pass 1: Liberal extraction with systematic implicit scanning complete",
    "extraction_strategy": "Followed updated prompt 03 (v2.5) with Phase A/B workflow for each tier. Phase A extracted explicit RDMAP from Methods/Approach sections. Phase B conducted systematic 4-pattern implicit scans across Abstract, Introduction, Methods, Results, and Discussion sections for each tier. Conservative extraction focused on clearly documented or strongly implied items.",
    "total_rdmap_items": 26,
    "designs": 4,
    "methods": 8,
    "protocols": 14,
    "explicit_items": 12,
    "implicit_items": 14,
    "implicit_percentage": 54,
    "claims_evidence_extraction_complete": true,
    "rdmap_extraction_complete": true,
    "known_uncertainties": [
      "Implicit ratio (54%) is higher than typical 20-40% range, likely because paper has exceptionally detailed Discussion section (4.1-4.3) revealing many undocumented operational details",
      "Some implicit protocols (P-IMP-001 through P-IMP-008) have low reconstruction confidence due to minimal mentions in paper",
      "Quality checkpoint: Zero implicit RDMAP in previous extraction vs 14 implicit RDMAP in this extraction confirms Phase 1 fixes successfully enabled implicit extraction",
      "Expected implicit RDMAP items found: map assignment protocol, recruitment protocol, performance monitoring, error taxonomy development, QA methodology, device management, data processing - all mentioned but undocumented"
    ],
    "assessment_blockers": [],
    "implicit_scanning_notes": "Systematic 4-pattern scanning completed for all major sections. Research Designs: Found 2 implicit (threshold determination design, ML comparison design) - both strategic objectives apparent from analysis focus but never stated in Methods. Methods: Found 4 implicit (QA methodology, map selection, performance monitoring, error categorisation) - all mentioned in Results but undocumented in Methods. Protocols: Found 8 implicit (map assignment, volunteer recruitment, load monitoring, data transfer, server config, device management, GPS extraction, random sampling) - operational details mentioned throughout but procedures never described. High implicit finding rate reflects paper's focus on reporting outcomes rather than documenting procedures."
  }
}
