{
  "schema_version": "2.5",
  "extraction_timestamp": "2025-10-23T11:13:16.909931Z",
  "extractor": "Claude Sonnet 4.5",
  "paper_metadata": {
    "title": "Creating large, high-quality geospatial datasets from historical maps using novice volunteers",
    "authors": [
      "Adela Sobotkova",
      "Shawn A. Ross",
      "Christian Nassif-Haynes",
      "Brian Ballsun-Stanton"
    ],
    "year": 2023,
    "doi": "10.1016/j.apgeog.2023.102967",
    "journal": "Applied Geography"
  },
  "evidence": [
    {
      "evidence_id": "E001",
      "evidence_text": "FAIMS Mobile was used to digitise 10,827 mound features",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "FAIMS Mobile was used to digitise 10,827 mound features from Soviet military topographic maps",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_count",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C001",
        "C002",
        "C011"
      ],
      "related_evidence": [
        "E002",
        "E003",
        "E004"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E002",
      "evidence_text": "Digitisation required 241 person-hours total",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "This digitisation required 241 person-hours",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C002",
        "C003",
        "C012"
      ],
      "related_evidence": [
        "E001",
        "E003"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E003",
      "evidence_text": "57 person-hours from staff, 184 from novice volunteers",
      "evidence_type": "quantitative_resource_breakdown",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "57 from staff; 184 from novice volunteers",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C003",
        "C017"
      ],
      "related_evidence": [
        "E002"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E004",
      "evidence_text": "Error rate under 6%",
      "evidence_type": "quantitative_quality",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "with an error rate under 6%",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "bounded_range",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "type": "bounded_range",
        "indicator": "under",
        "quantification": "<6%",
        "author_explanation": null,
        "severity": "minor"
      },
      "supports_claims": [
        "C004",
        "C011"
      ],
      "related_evidence": [
        "E001"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E005",
      "evidence_text": "Quality assurance check covering 7% of digitised features found 49 errors from 834 features, yielding 5.87% error rate",
      "evidence_type": "quantitative_quality",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "found 49 errors from a true count of 834 features, a 5.87% error rate",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "sample_check"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C004",
        "C040"
      ],
      "related_evidence": [
        "E004"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E006",
      "evidence_text": "Error breakdown from QA check: 42 false negatives (missed symbols), 6 double-marked features, 1 classification error, and no false positives",
      "evidence_type": "error_breakdown",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Forty-two of these errors were false negatives (symbols missed by students). Six were double-marked (Student C digitised a section of a map twice). Students made only one classification error (a similar symbol mistaken for a benchmark), and no outright false positives",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "sample_check"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C040",
        "C041"
      ],
      "related_evidence": [
        "E005"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E006",
          "P1_E007",
          "P1_E008"
        ],
        "consolidation_type": "profile_consolidation",
        "information_preserved": "complete",
        "granularity_available": "Individual error types available in source: false negatives (42), double-marks (6), classification errors (1), false positives (0)",
        "rationale": "All error types from same QA check form complete error profile assessed together"
      }
    },
    {
      "evidence_id": "E009",
      "evidence_text": "Individual student error rates ranged from 1.3% to 10.6%",
      "evidence_type": "quantitative_quality_range",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Students' individual error rates ranged from 1.3% to 10.6%",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "per_student_breakdown"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E005"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E010",
      "evidence_text": "Two fastest digitisers (44-45s per feature) had lowest error rates (1.3% and 2.9%)",
      "evidence_type": "comparative_performance",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Note that the two fastest digitisers (Students A and B; 44 and 45 s per feature respectively) also had the lowest error rates (1.3 and 2.9%)",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "per_student_breakdown"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E009"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E011",
      "evidence_text": "Two slowest digitisers (61-73s per feature) had highest error rates (10.6% and 7.4%)",
      "evidence_type": "comparative_performance",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "while the two slowest (Students C and D; 61 and 73 s) had the highest error rates (10.6 and 7.4%)",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "per_student_breakdown"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E010"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E012",
      "evidence_text": "35 of 49 false negatives resulted from one student failing to digitise three contiguous map sections",
      "evidence_type": "error_pattern",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "35 of the 49 false negatives were the result of Student C failing to digitise three contiguous sections of an assigned map",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "error_pattern_analysis"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C041"
      ],
      "related_evidence": [
        "E006"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E013",
      "evidence_text": "Excluding Student C outlier would reduce cumulative error rate from 5.9% to 2.8%",
      "evidence_type": "adjusted_error_calculation",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "excluding Student C would have cut the cumulative error rate in half to 2.8%",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "adjusted_calculation"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E012"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E014",
      "evidence_text": "Recoverable data omissions totaled 223 records (2.06%)",
      "evidence_type": "data_quality_issue",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Recoverable data omissions across both years totaled 223 (2.06% of records)",
      "location": {
        "section": "Results",
        "subsection": "3.5.1. Recoverable data omissions and incomplete records",
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E015",
      "evidence_text": "Project staff could digitise at 60-75 features per staff-hour using desktop GIS",
      "evidence_type": "comparative_performance_benchmark",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "project staff with desktop GIS experience could digitise at a sustained rate of 60 \u2013 75 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "bounded_range",
        "sample_representativeness": "staff_performance"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C050",
        "C051"
      ],
      "related_evidence": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E016",
      "evidence_text": "57 hours of staff time for crowdsourcing system yielded 10,827 features (190 features per staff-hour)",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_calculation",
      "verbatim_quote": "the 57 h of staff time required for our digitisation approach using a customisation of FAIMS Mobile produced 10,827 features, or about 190 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "calculated",
        "sample_representativeness": "complete_project"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C052",
        "C053"
      ],
      "related_evidence": [
        "E003"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E017",
      "evidence_text": "21 internal staff hours represents over 500 features per staff-hour",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_calculation",
      "verbatim_quote": "Those 21 internal staff hours represent a digitisation rate of over 500 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "bounded_range",
        "sample_representativeness": "internal_staff_subset"
      },
      "declared_uncertainty": {
        "type": "bounded_range",
        "indicator": "over",
        "quantification": ">500",
        "author_explanation": null,
        "severity": "minor"
      },
      "supports_claims": [
        "C054"
      ],
      "related_evidence": [
        "E016"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E018",
      "evidence_text": "Project staff digitise at sustained rate of 60-75 features per staff-hour; at this rate, 57h of staff time could produce 3,420-4,275 staff-digitised features",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Project staff can digitise mounds at a sustained rate of 60 \u2013 75 features per staff-hour. At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420 \u2013 4,275 staff-digitised features",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "range_estimate",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": "60-75 features per staff-hour",
        "hedging_language": null
      },
      "supports_claims": [
        "C052",
        "C051",
        "C064",
        "C053"
      ],
      "related_evidence": [
        "E020"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E018",
          "P1_E019"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "Rate (60-75/h) and projection (3,420-4,275 features) both preserved",
        "rationale": "Rate and projection assessed together as staff capability; both needed for comparative analysis"
      }
    },
    {
      "evidence_id": "E020",
      "evidence_text": "2010 digitisation rate with volunteers using desktop GIS: 130-180 features per staff-hour of support; 57h with this approach might have produced 7,410-10,260 features",
      "evidence_type": "comparative_performance_benchmark",
      "evidence_basis": "prior_project_data",
      "verbatim_quote": "In 2010, we attempted to use desktop GIS to crowdsource digitisation but found volunteers required extensive and continuous staff support, resulting in a de facto production rate of 130 \u2013 180 features per staff-hour (of desktop GIS support and troubleshooting). This compares to 190 features per staff-hour for the FAIMS Mobile approach and suggests that using volunteers with desktop GIS might have produced 7,410 \u2013 10,260 features for 57 h, yet at a much higher administrative cost",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "range_estimate",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": "130-180 features per staff-hour",
        "hedging_language": null
      },
      "supports_claims": [
        "C056",
        "C064",
        "C055"
      ],
      "related_evidence": [
        "E018",
        "E022"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E020",
          "P1_E021"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "Rate (130-180/h) and projection (7,410-10,260 features) both preserved",
        "rationale": "Rate and projection assessed together as desktop GIS approach capability"
      }
    },
    {
      "evidence_id": "E022",
      "evidence_text": "FAIMS Mobile approach: 190 features per staff-hour",
      "evidence_type": "quantitative_performance",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "the 57 h of staff time required for our digitisation approach using a customisation of FAIMS Mobile produced 10,827 features, or about 190 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "approximate_calculation",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C057",
        "C064"
      ],
      "related_evidence": [
        "E001",
        "E002"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E023",
      "evidence_text": "Of 57h total, only 21h came from internal project staff (36h from student programmer); 21 internal staff hours represent over 500 features per staff-hour, and would yield 1,260-1,575 features if staff digitised directly, or 2,730-3,780 if supervising desktop GIS volunteers",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Only 21 of those 57 h came from the project's in ternal archaeologists, because a programming student donated 36 h to app configuration and customisation (at a cost of ca. AUD $2,000). This 21 h of internal staff time was sufficient to crowdsource production of 10,827 features, a rate of over 500 features per staff-hour. By way of comparison, dedicating those 21 h to staff digitisation would have yielded 1,260 \u2013 1,575 features, while using them to supervise a desktop-GIS crowdsourcing approach would have produced 2,730 \u2013 3,780 features",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C058",
        "C059",
        "C064"
      ],
      "related_evidence": [
        "E022",
        "E024"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E023",
          "P1_E025",
          "P1_E026"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "21h internal staff, 500+ features/h rate, comparative projections all preserved",
        "rationale": "Internal staff time breakdown with all comparative projections assessed together"
      }
    },
    {
      "evidence_id": "E024",
      "evidence_text": "Student programmer cost: approximately AUD $2,000",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "completed by a student programmer for a modest cost (ca. AUD $2,000)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "approximate_value",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "ca."
      },
      "supports_claims": [
        "C058"
      ],
      "related_evidence": [
        "E023"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E027",
      "evidence_text": "In-field support for volunteers was only 7h across two seasons (about 1,550 features per in-field staff-hour); 7h would allow staff to directly digitise 420-525 features, or supervise desktop GIS digitisation of 910-1,260 features",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "It is also worth considering how the project used its most-precious resource: staff time in the field, which totalled just 7 h across two field seasons. This time was spent supporting volunteers (troubleshooting), demonstrating the system, and locating maps that corresponded to digitisation targets. This in-field supervision represented about 1,550 features per in-field staff-hour. If dedicated to direct staff digitisation, those 7 h would have yielded about 420 \u2013 525 features, or 910 \u2013 1,260 features if used to support desktop GIS digitisation by students",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 6
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C064",
        "C060",
        "C061"
      ],
      "related_evidence": [
        "E023",
        "E030"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E027",
          "P1_E028",
          "P1_E029"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "7h total, 1,550 features/h rate, comparative projections all preserved",
        "rationale": "In-field support time with all comparative projections assessed together"
      }
    },
    {
      "evidence_id": "E030",
      "evidence_text": "In-field support and quality assurance totalled 13h; marginal cost per additional feature: 4.3 seconds of staff support",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Combining the 7 h of in-field support and 6 h of quality assurance, the ' marginal cost ' to produce an additional feature was 4.3 s of staff support (13 h divided by 10,827 features)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C062"
      ],
      "related_evidence": [
        "E027",
        "E032"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E030",
          "P1_E031"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "13h total (7h field + 6h QA), 4.3s marginal cost per feature",
        "rationale": "Support time total and per-feature metric assessed together as system efficiency"
      }
    },
    {
      "evidence_id": "E032",
      "evidence_text": "Map preparation: 6 minutes per map (6h for 58 maps)",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Preparing and distributing additional maps took only 6 min per map (6 h for 58 maps)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C063"
      ],
      "related_evidence": [
        "E033"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E033",
      "evidence_text": "2018 redeployment required only 1h additional setup time",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "Even adding another field season only costs one additional hour of setup time (based on our 2018 redeployment)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C063"
      ],
      "related_evidence": [
        "E032"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E034",
      "evidence_text": "Urban Occupations Project: 1,250h manual digitisation for training data",
      "evidence_type": "quantitative_benchmark",
      "evidence_basis": "literature_source",
      "verbatim_quote": "This project reported 1,250 h of manual digitisation to create enough training data to classify roads visible in historical maps of the Ottoman Empire",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "secondary_source",
        "measurement_precision": "exact_count",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C067",
        "C068"
      ],
      "related_evidence": [
        "E035",
        "E036"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E035",
      "evidence_text": "ML expert spent 7 days testing and fine-tuning the model",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "literature_source",
      "verbatim_quote": "an ML expert spent seven days testing and fine tuning the model",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "secondary_source",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C067",
        "C068"
      ],
      "related_evidence": [
        "E034",
        "E036"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E036",
      "evidence_text": "ML output: 300,000 km of roads digitised",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "literature_source",
      "verbatim_quote": "The output was impressive: some 300,000 km of roads were digitised",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "secondary_source",
        "measurement_precision": "approximate_value",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "some"
      },
      "supports_claims": [
        "C067"
      ],
      "related_evidence": [
        "E034",
        "E035"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E037",
      "evidence_text": "ML approach required minimum of about 1,300h preparation time",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "which appears to have required a minimum of about 1,300 h of preparation time alone",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_calculation",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C068"
      ],
      "related_evidence": [
        "E034",
        "E035"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E038",
      "evidence_text": "Project time breakdown: 44 staff hours customising, 184 participant-hours digitising, 7 staff-hours supporting, 6 staff hours checking = 241h total",
      "evidence_type": "quantitative_resource_breakdown",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "We spent 44 staff hours customising and deploying a streamlined geospatial system in FAIMS Mobile, 184 participant-hours digitising features, seven staff-hours directly supporting that digitisation, and six staff hours checking for errors. These 241 h produced a dataset of 10,827 features",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C069"
      ],
      "related_evidence": [
        "E001",
        "E002",
        "E039"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E039",
      "evidence_text": "241h produced 10,827 features at rate of 44.9 features per person-hour",
      "evidence_type": "quantitative_performance",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "These 241 h produced a dataset of 10,827 features, a rate of 44.9 features/person-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "precise_calculation",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C069"
      ],
      "related_evidence": [
        "E038"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E040",
      "evidence_text": "At 44.9 features/person-hour rate, 1,300h would yield about 58,400 records",
      "evidence_type": "quantitative_calculation",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "At that rate, the 1,300 h it took to deploy the ML approach taken by Can, Gerrits, and Kabadayi would yield about 58,400 records",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_calculation",
        "sample_representativeness": "comparative_estimate"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C070"
      ],
      "related_evidence": [
        "E037",
        "E039"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E041",
      "evidence_text": "2% of records had recoverable data omissions",
      "evidence_type": "quantitative_error",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Some 2% of records had recoverable data omissions which were corrected during post-processing",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_percentage",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "Some"
      },
      "supports_claims": [
        "C090",
        "C091"
      ],
      "related_evidence": [
        "E042"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E042",
      "evidence_text": "Accuracy check covering 7% of digitised features indicated error rate under 6%",
      "evidence_type": "quantitative_error",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "An accuracy check by staff covering 7% of digitised features indicated an error rate of under 6%",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "threshold_estimate",
        "sample_representativeness": "sample_based"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "under"
      },
      "supports_claims": [
        "C090",
        "C091",
        "C092"
      ],
      "related_evidence": [
        "E041"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E043",
      "evidence_text": "Data ready for analysis with less than 2h processing after collection",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "a comprehensive, FAIR-compliant dataset was ready for analysis with less than 2 h of processing after collection",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "threshold_estimate",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "less than"
      },
      "supports_claims": [
        "C089"
      ],
      "related_evidence": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E044",
      "evidence_text": "Payoff threshold: 4,500 features vs direct digitisation by staff",
      "evidence_type": "quantitative_threshold",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "our approach becomes worthwhile for datasets no larger than about 4,500 features versus direct digitisation of features by expert staff",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_threshold",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C093"
      ],
      "related_evidence": [
        "E045",
        "E046"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E045",
      "evidence_text": "Payoff threshold: 10,000 features vs volunteer digitisation using desktop GIS",
      "evidence_type": "quantitative_threshold",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "no larger than about 10,000 features versus volunteer digitisation using desktop GIS",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_threshold",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C093"
      ],
      "related_evidence": [
        "E044",
        "E046"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E046",
      "evidence_text": "Most efficient approach for datasets up to at least 60,000 features",
      "evidence_type": "quantitative_threshold",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "It remains the most efficient approach for datasets up to at least 60,000 features, above which automated approaches like ML should be considered",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "threshold_estimate",
        "sample_representativeness": "comparative_estimate"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "at least"
      },
      "supports_claims": [
        "C093"
      ],
      "related_evidence": [
        "E044",
        "E045"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    }
  ],
  "claims": [
    {
      "claim_id": "C001",
      "claim_text": "The crowdsourcing approach successfully digitised a large number of burial mound features",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "FAIMS Mobile was used to digitise 10,827 mound features from Soviet military topographic maps",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E001"
      ],
      "supports_claims": [
        "C011"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "10,827 features"
        ],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C002",
      "claim_text": "The digitisation effort required a moderate amount of person-hours",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "This digitisation required 241 person-hours",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E002"
      ],
      "supports_claims": [
        "C003",
        "C012"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "241 person-hours"
        ],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C003",
      "claim_text": "Novice volunteers contributed the majority of person-hours",
      "claim_type": "outcome",
      "claim_role": "supporting",
      "claim_status": "explicit",
      "verbatim_quote": "57 from staff; 184 from novice volunteers",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E003"
      ],
      "supports_claims": [
        "C017"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "57 staff hours",
          "184 volunteer hours"
        ],
        "comparative_framing": "volunteers > staff"
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C004",
      "claim_text": "The digitisation achieved high accuracy",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "with an error rate under 6%",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E004",
        "E005"
      ],
      "supports_claims": [
        "C011"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "<6% error rate"
        ],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C011",
      "claim_text": "The approach achieved unanticipated success despite minimal resourcing",
      "claim_type": "synthesis",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "The value of this approach lies in the unanticipated success of a minimally resourced digitisation effort",
      "location": {
        "section": "Introduction",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "synthesize",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E001",
        "E004"
      ],
      "supports_claims": [],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C012",
      "claim_text": "The approach required modest resource investment compared to alternatives",
      "claim_type": "comparison",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "minimally resourced digitisation effort",
      "location": {
        "section": "Introduction",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "compare",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E002"
      ],
      "supports_claims": [
        "C011"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": "minimal vs typical"
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C017",
      "claim_text": "Novice undergraduate volunteers effectively contributed to digitisation",
      "claim_type": "outcome",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "Undergraduates in the associated field school digitised data from maps",
      "location": {
        "section": "Introduction",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E003"
      ],
      "supports_claims": [
        "C016"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C016",
      "claim_text": "The system was accessible to novice volunteers with no GIS experience",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "streamlined, collaborative system for crowdsourcing map digitisation by volunteers with no prior GIS experience",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [],
      "supports_claims": [
        "C010"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C010",
      "claim_text": "Systems designed for field data collection on mobile devices can be profitably customised as participatory geospatial data systems for novice volunteers",
      "claim_type": "generalization",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "it indicates that systems designed for field data collection, running on mobile devices, can be profitably customised to serve as participatory geospatial data systems accessible to novice volunteers",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "generalize",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E001",
        "E002",
        "E003",
        "E004"
      ],
      "supports_claims": [],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C040",
      "claim_text": "Error pattern was predictable and easily correctable",
      "claim_type": "outcome",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "the pattern of errors - mostly false negatives and double-marked features, mostly from contiguous map sections - made them relatively easy to identify and correct",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 4
      },
      "primary_function": "interpret_finding",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E005",
        "E006"
      ],
      "supports_claims": [
        "C004"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C041",
      "claim_text": "Errors were predominantly false negatives and double-markings rather than false positives",
      "claim_type": "outcome",
      "claim_role": "supporting",
      "claim_status": "explicit",
      "verbatim_quote": "mostly false negatives and double-marked features",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 4
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E006"
      ],
      "supports_claims": [
        "C040"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C050",
      "claim_text": "Crowdsourcing approach became more cost-effective than desktop GIS alternatives at moderate dataset sizes",
      "claim_type": "comparison",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 \u2013 60,000 records",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2. Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "primary_function": "generalize",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E015",
        "E016"
      ],
      "supports_claims": [],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "10,000-60,000 records optimal"
        ],
        "comparative_framing": "crowdsourcing vs desktop GIS vs ML"
      },
      "alternatives_acknowledged": [],
      "qualifications": [
        "conservative estimate"
      ],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C051",
      "claim_text": "Crowdsourced digitisation effort proved unexpectedly successful",
      "claim_type": "evaluation",
      "claim_role": "core",
      "verbatim_quote": "Our crowdsourced digitisation effort involving novice volunteers using an adapted mobile application for data capture proved unexpectedly successful",
      "location": {
        "section": "Discussion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "proved",
        "confidence_marker": "unexpectedly",
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E001",
        "E002",
        "E003",
        "E004"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C052",
        "C057",
        "C061"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C052",
      "claim_text": "Payoff threshold suggests digitisation by project staff suitable only for smaller datasets",
      "claim_type": "methodological_recommendation",
      "claim_role": "intermediate",
      "verbatim_quote": "Such a payoff threshold suggests that digitisation by project staff will be suitable only for smaller datasets",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "suggests",
        "confidence_marker": null,
        "scope_limitation": "payoff threshold analysis"
      },
      "supported_by_evidence": [
        "E018"
      ],
      "supports_claims": [
        "C051"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [
        "explicit definition of 'smaller datasets'"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C053",
      "claim_text": "Project could not afford to dedicate 3.5-4.5 weeks of staff time to digitisation",
      "claim_type": "contextual_constraint",
      "claim_role": "supporting",
      "verbatim_quote": "We could not have afforded to dedicate 3.5 \u2013 4.5 weeks of staff time to digitisation",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "could not have afforded",
        "confidence_marker": null,
        "scope_limitation": "project context"
      },
      "supported_by_evidence": [
        "E018"
      ],
      "supports_claims": [
        "C052"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C054",
      "claim_text": "There are four principal approaches to digitising historical maps",
      "claim_type": "classification",
      "claim_role": "intermediate",
      "verbatim_quote": "Digitisation projects will likely choose between one of four principal approaches to digitising historical maps",
      "location": {
        "section": "Discussion",
        "subsection": "4.1 Choosing an approach",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "will likely",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C052",
        "C055",
        "C067"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C055",
      "claim_text": "Desktop GIS digitisation using novice volunteers almost competitive with mobile application at highest rate",
      "claim_type": "comparative_effectiveness",
      "claim_role": "supporting",
      "verbatim_quote": "At the highest rate, desktop GIS digitisation using novice volunteers is almost competitive with the mobile application approach we used",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "is almost competitive",
        "confidence_marker": "At the highest rate",
        "scope_limitation": "optimal conditions"
      },
      "supported_by_evidence": [
        "E020",
        "E022"
      ],
      "supports_claims": [
        "C054"
      ],
      "supported_by_claims": [
        "C056"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C056",
      "claim_text": "Project unable to retain enough volunteers in 2010 to complete desktop GIS work",
      "claim_type": "contextual_constraint",
      "claim_role": "supporting",
      "verbatim_quote": "Scaling to this dataset size, however, assumes that enough volunteers could be retained to complete the work - something we were unable to do in 2010",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "were unable to do",
        "confidence_marker": null,
        "scope_limitation": "2010 experience"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C055"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [
        "reasons for volunteer attrition"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C057",
      "claim_text": "190 features per staff-hour figure understates value realised from volunteer digitisation",
      "claim_type": "evaluation",
      "claim_role": "intermediate",
      "verbatim_quote": "This figure, however, understates the value our project realised from volunteer digitisation",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "understates",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E022"
      ],
      "supports_claims": [
        "C051"
      ],
      "supported_by_claims": [
        "C058",
        "C060",
        "C062",
        "C063"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C058",
      "claim_text": "Customisation of systems like FAIMS Mobile can be outsourced more easily than other project activities",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "customisation of systems like FAIMS Mobile can be outsourced more easily than other project activities",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "can be outsourced more easily",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E023",
        "E024"
      ],
      "supports_claims": [
        "C057"
      ],
      "supported_by_claims": [
        "C059"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C059",
      "claim_text": "21 internal staff hours represent exceptional efficiency compared to alternatives",
      "claim_type": "comparative_effectiveness",
      "claim_role": "supporting",
      "verbatim_quote": "Those 21 internal staff hours represent a digitisation rate of over 500 features per staff-hour. Twenty-one hours would have yielded just 1,260 \u2013 1,575 features if staff had digitised them directly, or 2,730 \u2013 3,780 had we supervised students using desktop GIS",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 5
      },
      "claim_strength": {
        "qualifier": "represent",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E023"
      ],
      "supports_claims": [
        "C058"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C060",
      "claim_text": "Staff time during field season was scarce and valuable",
      "claim_type": "contextual_constraint",
      "claim_role": "supporting",
      "verbatim_quote": "given competing responsibilities, staff time during the field season was scarce and valuable",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 6
      },
      "claim_strength": {
        "qualifier": "was",
        "confidence_marker": null,
        "scope_limitation": "field season context"
      },
      "supported_by_evidence": [
        "E027"
      ],
      "supports_claims": [
        "C057"
      ],
      "supported_by_claims": [
        "C061"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C061",
      "claim_text": "In-field support time was minimal while achieving high productivity",
      "claim_type": "evaluation",
      "claim_role": "supporting",
      "verbatim_quote": "Across two seasons, in-field support for volunteers was only 7 h, representing about 1,550 features per in-field staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 6
      },
      "claim_strength": {
        "qualifier": "was only",
        "confidence_marker": null,
        "scope_limitation": "two field seasons"
      },
      "supported_by_evidence": [
        "E027"
      ],
      "supports_claims": [
        "C051",
        "C060"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C062",
      "claim_text": "Marginal cost for each additional feature digitised is low",
      "claim_type": "economic_efficiency",
      "claim_role": "supporting",
      "verbatim_quote": "the marginal cost for each additional feature digitised is low",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "claim_strength": {
        "qualifier": "is low",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E030"
      ],
      "supports_claims": [
        "C057"
      ],
      "supported_by_claims": [
        "C063"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C063",
      "claim_text": "Scalability of crowdsourcing approach makes it more attractive for expanding projects",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "The scalability of our crowdsourcing approach makes it more attractive if a project may expand over time to include more volunteers, more redeployments, or more maps",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "claim_strength": {
        "qualifier": "makes it more attractive",
        "confidence_marker": null,
        "scope_limitation": "expansion scenarios"
      },
      "supported_by_evidence": [
        "E032",
        "E033"
      ],
      "supports_claims": [
        "C057",
        "C062"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C064",
      "claim_text": "Qualitative factors argue for implementing crowdsourcing approach using mobile application",
      "claim_type": "methodological_recommendation",
      "claim_role": "intermediate",
      "verbatim_quote": "qualitative factors also argue for implementing a crowdsourcing approach using a mobile application",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 8
      },
      "claim_strength": {
        "qualifier": "argue for",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E018",
        "E020",
        "E022",
        "E023",
        "E027"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C065"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C065",
      "claim_text": "Desktop GIS caused continual staff stress and distraction from need for troubleshooting, while mobile approach nearly eliminated interventions and better utilised time, attention, and motivation of both staff and participants",
      "claim_type": "comparative_effectiveness",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "the need for staff to be continually available to troubleshoot problems with desktop GIS, lest digitisation stall, provided a continual source of stress and distraction. The switch to a lightweight GIS running on mobile devices nearly eliminated the need for staff interventions and improved volunteer satisfaction. [...] this approach better utilised the time, attention, and motivation of both staff and participants",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 8
      },
      "primary_function": "compare",
      "claim_nature": "interpretation",
      "supported_by_evidence": [],
      "supports_claims": [
        "C064"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": "desktop GIS vs mobile approach qualitative factors"
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_C065",
          "P1_C066"
        ],
        "consolidation_type": "narrative_consolidation",
        "information_preserved": "complete",
        "granularity_available": "Desktop GIS problems (stress, distraction, troubleshooting) and mobile benefits (reduced interventions, improved satisfaction, better utilization) both preserved",
        "rationale": "Problem-solution narrative assessed together as qualitative comparison supporting C064 recommendation"
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C067",
      "claim_text": "ML approaches worthwhile for large-scale projects with consistent symbology",
      "claim_type": "methodological_recommendation",
      "claim_role": "intermediate",
      "verbatim_quote": "suggests that ML approaches are worthwhile for large-scale projects that benefit from the consistent symbology and style (as found in British and Ottoman imperial maps)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "suggests",
        "confidence_marker": null,
        "scope_limitation": "consistent symbology and style"
      },
      "supported_by_evidence": [
        "E034",
        "E035",
        "E036"
      ],
      "supports_claims": [
        "C054"
      ],
      "supported_by_claims": [
        "C068"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C068",
      "claim_text": "ML papers rarely quantify time-on-task making it difficult to assess when investment yields savings",
      "claim_type": "methodological_gap",
      "claim_role": "supporting",
      "verbatim_quote": "Unfortunately, ML papers rarely quantify time-on-task, making it difficult to assess how large a dataset needs to be before the investment in ML approaches yields time savings",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "rarely quantify",
        "confidence_marker": "Unfortunately",
        "scope_limitation": "ML literature"
      },
      "supported_by_evidence": [
        "E034",
        "E037"
      ],
      "supports_claims": [
        "C067"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C069",
      "claim_text": "Project achieved 44.9 features per person-hour including all time investments",
      "claim_type": "quantitative_outcome",
      "claim_role": "supporting",
      "verbatim_quote": "These 241 h produced a dataset of 10,827 features, a rate of 44.9 features/person-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "produced",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E038",
        "E039"
      ],
      "supports_claims": [
        "C070"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C070",
      "claim_text": "Crowdsourcing approach most suitable for datasets numbering 10,000-60,000 records",
      "claim_type": "methodological_recommendation",
      "claim_role": "core",
      "verbatim_quote": "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 \u2013 60,000 records, assuming similar feature characteristics and data collection requirements",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "is most suitable",
        "confidence_marker": "perhaps",
        "scope_limitation": "similar feature characteristics and data collection requirements"
      },
      "supported_by_evidence": [
        "E040"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C069",
        "C071",
        "C072"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C071",
      "claim_text": "Below 10,000 records, desktop GIS approaches should be considered",
      "claim_type": "methodological_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "Below 10,000 records, approaches using desktop GIS should be considered",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "should be considered",
        "confidence_marker": null,
        "scope_limitation": "dataset size <10,000"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C070"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C072",
      "claim_text": "Above 60,000 records, ML approaches should be contemplated with requisite expertise",
      "claim_type": "methodological_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "Above 60,000 records, ML approaches should be contemplated, but only if a project has access to the requisite expertise",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "should be contemplated",
        "confidence_marker": null,
        "scope_limitation": "requires expertise access"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C070"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C073",
      "claim_text": "Projects where staff time is at premium may find value for datasets below 1,000 records",
      "claim_type": "methodological_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "Projects where staff time is at a premium, or that operate alongside fieldwork where staff have many competing demands, may find it valuable for smaller datasets (even those below 1,000 records)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "may find it valuable",
        "confidence_marker": null,
        "scope_limitation": "staff time premium or fieldwork contexts"
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C074",
      "claim_text": "ML and crowdsourcing approaches are complementary, not exclusive",
      "claim_type": "methodological_synthesis",
      "claim_role": "intermediate",
      "verbatim_quote": "The approaches are not exclusive, therefore, but complementary",
      "location": {
        "section": "Discussion",
        "subsection": "4.2 Combining crowdsourcing and ML approaches",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "are",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C075",
        "C076"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C075",
      "claim_text": "Dataset large enough for ML likely needs training dataset large enough to warrant crowdsourcing",
      "claim_type": "practical_implication",
      "claim_role": "supporting",
      "verbatim_quote": "A dataset big enough to justify ML will likely need a training dataset big enough to warrant crowdsourcing, especially if the features or background are variable",
      "location": {
        "section": "Discussion",
        "subsection": "4.2 Combining crowdsourcing and ML approaches",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "will likely need",
        "confidence_marker": null,
        "scope_limitation": "variable features or background"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C074"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C076",
      "claim_text": "Crowdsourcing platform can be used for ML error-checking datasets",
      "claim_type": "practical_implication",
      "claim_role": "supporting",
      "verbatim_quote": "Once the crowdsourcing platform has been built, moreover, it can be used to produce additional datasets for error-checking to confirm the accuracy of the ML results",
      "location": {
        "section": "Discussion",
        "subsection": "4.2 Combining crowdsourcing and ML approaches",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "can be used",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C074"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C077",
      "claim_text": "Typical archaeology/history projects may not be able to incorporate ML successfully",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "a typical project in history or archaeology - often small, under-resourced, and pursuing several research activities - may not be able to dedicate the personnel, infrastructure, or attention needed to incorporate ML successfully",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "may not be able",
        "confidence_marker": null,
        "scope_limitation": "typical small under-resourced projects"
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C078"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C078",
      "claim_text": "Such projects could deploy collaborative geospatial system for crowdsourcing",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "but could deploy a collaborative geospatial system for crowdsourcing map digitisation",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "could deploy",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C077"
      ],
      "supported_by_claims": [
        "C079"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C079",
      "claim_text": "Deploying collaborative geospatial system requires higher up-front investment but is feasible",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "Deploying such a system requires a higher up-front investment in time and expertise than use of desktop GIS approaches, but it is feasible",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "is feasible",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C078"
      ],
      "supported_by_claims": [
        "C080"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C080",
      "claim_text": "Software Carpentry level skills sufficient to customise and operate FAIMS Mobile",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "A project with a digital humanist or similar technologist with skills at the level of core Software Carpentry lessons can customise and operate a generalised platform such as FAIMS Mobile to implement an effective crowdsourcing system",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "can customise and operate",
        "confidence_marker": null,
        "scope_limitation": "with appropriate skills"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C079"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C081",
      "claim_text": "Technical barriers to deploying such systems will likely decline in future",
      "claim_type": "projection",
      "claim_role": "supporting",
      "verbatim_quote": "in future the technical barriers to deploying such systems will likely decline",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "will likely decline",
        "confidence_marker": null,
        "scope_limitation": "future projection"
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C082",
      "claim_text": "Projects need to weigh trade-offs between different digitisation approaches",
      "claim_type": "methodological_recommendation",
      "claim_role": "core",
      "verbatim_quote": "Projects need to weigh the trade-offs between different approaches",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "need to weigh",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C083",
        "C084",
        "C085"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C083",
      "claim_text": "Desktop GIS complexity limits scale of novice digitisation",
      "claim_type": "limitation",
      "claim_role": "supporting",
      "verbatim_quote": "The complexity of full-featured, desktop GIS makes it difficult for novices to use, limiting the scale of digitisation",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "makes it difficult",
        "confidence_marker": null,
        "scope_limitation": "novice users"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C082"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C084",
      "claim_text": "ML approaches require significant resources and expertise to avoid failures",
      "claim_type": "limitation",
      "claim_role": "supporting",
      "verbatim_quote": "ML approaches require significant resources to implement and expertise to avoid failures arising from training bias or other pitfalls",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "require",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C082"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C085",
      "claim_text": "Purpose-built lightweight collaborative systems fill the gap between desktop GIS and ML",
      "claim_type": "synthesis",
      "claim_role": "core",
      "verbatim_quote": "Purpose-built, lightweight, collaborative geospatial data recording systems fill the gap between",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "fill the gap",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C082"
      ],
      "supported_by_claims": [
        "C086"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C086",
      "claim_text": "FAIMS Mobile facilitated multiple capabilities with high-quality results and few compromises",
      "claim_type": "evaluation",
      "claim_role": "core",
      "verbatim_quote": "Our use of FAIMS Mobile facilitated offline, multi-user map-feature digitisation, minimised post-processing, supported multiple data types, and produced high-quality data aligning with our methods and aims - with few compromises, work-arounds, or other technological distractions",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "facilitated",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C085"
      ],
      "supported_by_claims": [
        "C087",
        "C088",
        "C089",
        "C090"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C087",
      "claim_text": "Deployment facilitated rapid digitisation with modest resources",
      "claim_type": "evaluation",
      "claim_role": "supporting",
      "verbatim_quote": "The deployment of the Map Digitisation FAIMS Mobile customisation facilitated the rapid digitisation (57 staff-hours; 184 volunteer-hours; total) of 10,827 features found in Soviet topographic maps",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "facilitated",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E001",
        "E002",
        "E003"
      ],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C088",
      "claim_text": "System required modest hardware and minimal supervision while supporting offline operation",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "It required only modest hardware and minimal supervision, but supported offline operation, including in-field setup, data collection, synchronisation across multiple devices, and data backup and export",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "required only",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C089",
      "claim_text": "Data was available daily and FAIR-compliant dataset ready with minimal processing",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "All collected data was available daily for review, and a comprehensive, FAIR-compliant dataset was ready for analysis with less than 2 h of processing after collection",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "was available",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E043"
      ],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C090",
      "claim_text": "Overall quality of dataset was high",
      "claim_type": "evaluation",
      "claim_role": "core",
      "verbatim_quote": "Overall quality of this dataset was high",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "was high",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E041",
        "E042"
      ],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [
        "C091",
        "C092"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C091",
      "claim_text": "Data omissions were correctable and subsequently minimised through validation",
      "claim_type": "quality_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "Some 2% of records had recoverable data omissions which were corrected during post-processing and subsequently minimised through the addition of validation",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "were corrected",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E041"
      ],
      "supports_claims": [
        "C090"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C092",
      "claim_text": "Errors were predictable and easily mitigatable",
      "claim_type": "quality_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "errors were predictable and would be easily mitigated by redundant digitisation by volunteers or volunteer peer review",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "were predictable",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E042"
      ],
      "supports_claims": [
        "C090"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C093",
      "claim_text": "Crowdsourcing approach becomes worthwhile at specific payoff thresholds based on staff time assumptions, and is most efficient for datasets up to 60,000 features, above which machine learning should be considered",
      "claim_type": "methodological_recommendation",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "a crowdsourcing approach like ours will become worthwhile after a specific ' payoff threshold ', determined by assumptions about available staff time, project objectives, and dataset requirements. [...] our approach is probably most efficient for up to about 60,000 features, above which ML should be seriously considered",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "primary_function": "recommend",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E045",
        "E044",
        "E046"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C073"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "60,000 features upper bound"
        ],
        "comparative_framing": "payoff threshold analysis"
      },
      "alternatives_acknowledged": [
        "machine learning for > 60,000 features"
      ],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_C093",
          "P1_C094"
        ],
        "consolidation_type": "synthesis",
        "information_preserved": "complete",
        "granularity_available": "Payoff threshold concept and 60,000-feature upper bound both preserved",
        "rationale": "Both claims address optimal dataset size range for crowdsourcing approach; consolidation provides complete recommendation with lower and upper bounds"
      }
    },
    {
      "claim_id": "C095",
      "claim_text": "Approach can produce training datasets for ML and serve as quality assurance tool",
      "claim_type": "practical_implication",
      "claim_role": "intermediate",
      "verbatim_quote": "Our approach can also be used to produce the training datasets needed for ML, and as a tool for quality assurance for ML outputs",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "can be used",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C096",
      "claim_text": "Approach is readily transferable to other mobile GIS systems and map corpora",
      "claim_type": "generalizability",
      "claim_role": "intermediate",
      "verbatim_quote": "This approach is readily transferable to other mobile GIS systems and map corpora",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "is readily transferable",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C097"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C097",
      "claim_text": "Experience provides only single data point for assessing applicability of approaches",
      "claim_type": "limitation",
      "claim_role": "supporting",
      "verbatim_quote": "but our experience provides only a single data point for assessing the applicability of various digitisation approaches to historical maps",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "provides only",
        "confidence_marker": null,
        "scope_limitation": "single data point"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C096"
      ],
      "supported_by_claims": [
        "C098"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C098",
      "claim_text": "More projects need to track and publish comprehensive digitisation metrics",
      "claim_type": "research_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "More projects - whether they use manual or automated approaches - need to track and publish the expert and volunteer time required for setup, training, support, and quality assurance related to map digitisation, as well as digitisation speed, error rates and types, the characteristics of the features being digitised, and the complexity of information extracted",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "need to",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C097"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    }
  ],
  "implicit_arguments": [
    {
      "implicit_argument_id": "IA001",
      "implicit_argument_text": "Quality control mechanisms were in place to achieve the stated error rate",
      "argument_type": "logical_implication",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "with an error rate under 6%",
        "The resulting dataset was consistent, well-documented"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Achieving a low error rate (<6%) with novice volunteers implies systematic quality control procedures were implemented. Consistency and documentation quality further suggest structured verification processes. The authors don't explicitly describe QA mechanisms in abstract, but these outcomes would be impossible without them.",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "supports_claim": "C004",
      "implicit_metadata": {
        "inferential_basis": "procedural_requirement",
        "confidence": "high",
        "gaps_in_stated_reasoning": [
          "Quality control procedures not explicitly described in abstract",
          "Verification methods not specified"
        ],
        "alternative_interpretations": []
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA002",
      "implicit_argument_text": "The customisation of FAIMS Mobile was technically feasible without prohibitive expertise",
      "argument_type": "unstated_assumption",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "a customisation of the Field Acquired Information Management Systems (FAIMS) Mobile platform",
        "it requires less technical expertise, time, and resourcing to undertake"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Introduction",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Authors claim the approach requires 'less technical expertise' than ML while simultaneously reporting successful 'customisation' of FAIMS Mobile. This implies platform customisation was achievable without high technical barriers. The assumption is that customisation was straightforward enough to still qualify as 'low expertise' approach.",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "supports_claim": "C010",
      "implicit_metadata": {
        "inferential_basis": "feasibility_assumption",
        "confidence": "moderate",
        "gaps_in_stated_reasoning": [
          "Technical requirements for customisation not specified",
          "Expertise needed for platform adaptation not detailed"
        ],
        "alternative_interpretations": [
          "Platform may have required significant technical expertise that authors minimize",
          "Customisation may have been simpler than typical mobile platform adaptation"
        ]
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA003",
      "implicit_argument_text": "The Soviet military maps were sufficiently accurate and reliable as source documents",
      "argument_type": "disciplinary_assumption",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "digitise 10,827 mound features from Soviet military topographic maps",
        "creating large, high-quality geospatial datasets from historical maps"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Title",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Authors claim to produce 'high-quality geospatial datasets' from Soviet military maps without discussing map accuracy, georeferencing precision, or systematic distortions. This implies an unstated assumption that these maps are reliable source documents. In archaeological GIS, source map quality fundamentally constrains output quality.",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "supports_claim": "C004",
      "implicit_metadata": {
        "inferential_basis": "source_reliability_assumption",
        "confidence": "moderate",
        "gaps_in_stated_reasoning": [
          "Map accuracy not assessed",
          "Georeferencing error not quantified",
          "Systematic distortions not discussed"
        ],
        "alternative_interpretations": [
          "Maps may have known accuracy limitations not discussed",
          "High-quality claim may refer to digitisation precision rather than absolute accuracy"
        ]
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA004",
      "implicit_argument_text": "The 10,000-60,000 feature range is based on extrapolation from their single case study",
      "argument_type": "bridging_claim",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "FAIMS Mobile was used to digitise 10,827 mound features",
        "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 \u2013 60,000 records"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2",
          "paragraph": 4
        }
      ],
      "inference_reasoning": "Authors generalize from their 10,827-feature case to claim efficiency in 10,000-60,000 range. The bridging logic connecting their single data point to this efficiency range is not explicitly stated. The claim requires assumptions about how resource requirements scale with dataset size, setup costs, and comparison to alternative approaches.",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2",
        "paragraph": 4
      },
      "supports_claim": "C050",
      "implicit_metadata": {
        "inferential_basis": "scaling_model",
        "confidence": "moderate",
        "gaps_in_stated_reasoning": [
          "Scaling assumptions not specified",
          "Efficiency calculations not shown",
          "Comparison methodology not detailed"
        ],
        "alternative_interpretations": [
          "Range may be based on additional unpublished analyses",
          "Conservative estimate may incorporate substantial safety margins"
        ]
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA005",
      "implicit_argument_text": "Staff time efficiency is the primary metric for assessing digitisation approach value",
      "implicit_argument_type": "unstated_assumption",
      "trigger_text": [
        "This discussion, furthermore, focuses on our most limited resource: staff time. As such, the calculations below, which propose dataset size thresholds for various approaches, prioritise staff time required for digitisation",
        "If staff time is the primary limiting resource, our approach becomes worthwhile for datasets no larger than about 4,500 features"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1 Choosing an approach",
          "paragraph": 4
        },
        {
          "section": "Conclusion",
          "subsection": null,
          "paragraph": 3
        }
      ],
      "inference_reasoning": "The entire comparative analysis and threshold recommendations are predicated on staff time being the primary metric of value. This assumption shapes all payoff calculations but is stated as contextual choice rather than universal principle. The implicit argument is that this metric prioritization is appropriate for the field conditions and project constraints, which may not hold for all projects.",
      "implicit_metadata": {
        "inference_basis": "methodological_framing",
        "gaps_in_support": [
          "No consideration of other potential metrics (cost, data quality, volunteer experience)",
          "No discussion of contexts where different metrics might be prioritized"
        ],
        "confidence_in_inference": "high"
      },
      "supports_claims": [
        "C052",
        "C070",
        "C093"
      ],
      "related_implicit_arguments": [
        "IA006"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "implicit_argument_id": "IA006",
      "implicit_argument_text": "Linear scaling of time-to-features ratios is valid for comparing approaches",
      "implicit_argument_type": "unstated_assumption",
      "trigger_text": [
        "At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420 \u2013 4,275 staff-digitised features",
        "At that rate, the 1,300 h it took to deploy the ML approach taken by Can, Gerrits, and Kabadayi would yield about 58,400 records"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 1
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2 Machine learning versus crowdsourcing",
          "paragraph": 3
        }
      ],
      "inference_reasoning": "All comparative calculations assume linear relationships between time investment and features produced, projecting from observed rates to hypothetical larger datasets. This assumes no economies or diseconomies of scale, no learning effects, and consistent feature characteristics across datasets. The validity of these projections depends on this linearity holding at different scales.",
      "implicit_metadata": {
        "inference_basis": "methodological_extrapolation",
        "gaps_in_support": [
          "No empirical validation of linear scaling at different dataset sizes",
          "No discussion of potential non-linear effects (learning curves, fatigue, complexity changes)"
        ],
        "confidence_in_inference": "medium"
      },
      "supports_claims": [
        "C052",
        "C070",
        "C093"
      ],
      "related_implicit_arguments": [
        "IA005"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "implicit_argument_id": "IA007",
      "implicit_argument_text": "Project's feature characteristics and map conditions are representative of broader historical map digitisation",
      "implicit_argument_type": "bridging_claim",
      "trigger_text": [
        "Note that in all following comparisons, we present our experience as a (perhaps idiosyncratic) example. Time per feature included locating it and completing the record, hence the rate reflects the high density and moderate obtrusiveness of archaeological features in Soviet topographic maps",
        "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 \u2013 60,000 records, assuming similar feature characteristics and data collection requirements",
        "This approach is readily transferable to other mobile GIS systems and map corpora, but our experience provides only a single data point"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1 Choosing an approach",
          "paragraph": 4
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2 Machine learning versus crowdsourcing",
          "paragraph": 4
        },
        {
          "section": "Conclusion",
          "subsection": null,
          "paragraph": 4
        }
      ],
      "inference_reasoning": "The authors acknowledge their example may be idiosyncratic and represents a single data point, yet make generalized recommendations about dataset size thresholds and approach suitability. The bridging claim is that despite acknowledging specificity, the project characteristics are representative enough to warrant these generalizations. The tension between acknowledged uniqueness and generalized recommendations reveals this implicit bridging assumption.",
      "implicit_metadata": {
        "inference_basis": "generalization_from_single_case",
        "gaps_in_support": [
          "No empirical comparison with other map types or feature characteristics",
          "Acknowledged as 'single data point' but recommendations presented with confidence"
        ],
        "confidence_in_inference": "medium"
      },
      "supports_claims": [
        "C070",
        "C093",
        "C096"
      ],
      "related_implicit_arguments": [
        "IA005",
        "IA006"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "implicit_argument_id": "IA008",
      "implicit_argument_text": "Volunteer satisfaction and reduced staff stress have measurable project value beyond efficiency metrics",
      "implicit_argument_type": "unstated_assumption",
      "trigger_text": [
        "qualitative factors also argue for implementing a crowdsourcing approach using a mobile application",
        "the need for staff to be continually available to troubleshoot problems with desktop GIS, lest digitisation stall, provided a continual source of stress and distraction",
        "The switch to a lightweight GIS running on mobile devices nearly eliminated the need for staff interventions and improved volunteer satisfaction",
        "this approach better utilised the time, attention, and motivation of both staff and participants"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 8
        },
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 8
        },
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 9
        },
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 9
        }
      ],
      "inference_reasoning": "The discussion of qualitative factors (stress, satisfaction, motivation) is presented as 'also arguing for' the mobile approach, implying these factors have decision-making weight. However, the entire quantitative analysis focuses solely on time efficiency. The implicit argument is that these qualitative benefits have value comparable to or exceeding the quantitative efficiency gains, justifying their inclusion in the recommendation despite not being incorporated into the threshold calculations.",
      "implicit_metadata": {
        "inference_basis": "value_judgment",
        "gaps_in_support": [
          "No quantification of qualitative benefits",
          "No explicit weighting of qualitative vs quantitative factors in recommendations",
          "No discussion of contexts where qualitative factors might be prioritized differently"
        ],
        "confidence_in_inference": "medium-high"
      },
      "supports_claims": [
        "C064",
        "C065",
        "C086"
      ],
      "related_implicit_arguments": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    }
  ],
  "project_metadata": {
    "timeline": {
      "project_start": "2008",
      "project_end": "ongoing through 2022",
      "digitisation_period": "2017-2018",
      "prior_cataloguing": "2008-2016",
      "field_seasons": [
        "2017",
        "2018"
      ],
      "source": {
        "section": "Introduction",
        "subsection": "1.1. The Tundzha Regional Archaeology Project",
        "paragraph": "2-3"
      }
    },
    "location": {
      "country": "Bulgaria",
      "regions": [
        "Yambol region",
        "Kazanlak Valley"
      ],
      "broader_area": "Tundzha River watershed, southeast Bulgaria",
      "map_coverage": "over 20,000 sq km",
      "source": {
        "section": "Introduction / Abstract",
        "subsection": "1.1",
        "paragraph": "1-3"
      }
    },
    "resources": {
      "personnel": {
        "staff_hours": 57,
        "volunteer_hours": 184,
        "total_person_hours": 241,
        "volunteers": "undergraduate students in field school",
        "student_programmer_hours": 36,
        "student_programmer_cost": "ca. AUD $2,000"
      },
      "equipment": {
        "description": "mobile devices",
        "cost_characteristics": "low-cost"
      },
      "software": {
        "platform": "FAIMS Mobile (customised)",
        "license": "open-source"
      },
      "funding": "minimally resourced",
      "source": {
        "section": "Abstract / Discussion",
        "subsection": "4.1.1",
        "paragraph": "1 and 4"
      }
    },
    "track_record": {
      "prior_work": [
        {
          "period": "2008-2016",
          "output": "773 mounds catalogued in Kazanlak Valley",
          "methods": "pedestrian surface survey, manual digitisation of satellite imagery and maps"
        },
        {
          "period": "2008-2016",
          "output": "431 mounds catalogued in Yambol region",
          "methods": "pedestrian surface survey, manual digitisation"
        },
        {
          "period": "2010",
          "output": "Unsuccessful desktop GIS volunteer digitisation trial",
          "outcome": "High volunteer attrition, excessive staff time demands"
        }
      ],
      "project_aims": [
        "reconstructing ancient environment",
        "mapping evolution of habitation",
        "explaining human-environment interactions",
        "producing inventory for research and conservation",
        "analysing threats"
      ],
      "source": {
        "section": "Introduction",
        "subsection": "1.1",
        "paragraph": "2"
      }
    }
  },
  "research_designs": [],
  "methods": [],
  "protocols": [],
  "extraction_notes": {
    "pass": 2,
    "section_extracted": "Complete paper - rationalization of Pass 1 extraction",
    "items_before_rationalization": 114,
    "items_after_rationalization": 103,
    "reduction_percentage": 9.6,
    "consolidations_performed": 8,
    "additions_performed": 0,
    "boundary_corrections": 0,
    "consolidation_details": {
      "evidence_consolidations": [
        "E006+E007+E008 \u2192 E006 (error breakdown profile)",
        "E018+E019 \u2192 E018 (staff digitisation capability)",
        "E020+E021 \u2192 E020 (desktop GIS volunteer capability)",
        "E023+E025+E026 \u2192 E023 (internal staff efficiency)",
        "E027+E028+E029 \u2192 E027 (in-field support efficiency)",
        "E030+E031 \u2192 E030 (support overhead)"
      ],
      "claim_consolidations": [
        "C065+C066 \u2192 C065 (qualitative effectiveness comparison)",
        "C093+C094 \u2192 C093 (dataset size threshold recommendation)"
      ]
    },
    "pass2_strategy": "Focused consolidation on efficiency calculations and comparative evidence where items assessed together; preserved temporal comparisons and items supporting distinct claims",
    "consolidation_patterns_applied": [
      "Compound findings (rate + projection combinations)",
      "Profile consolidation (error type breakdown)",
      "Narrative consolidation (problem-solution pairs)",
      "Synthesis (overlapping recommendations)"
    ],
    "items_kept_separate": [
      "E002/E003 - total vs breakdown may support different claims",
      "E009 - per-student error range distinct from E010 speed-accuracy correlation",
      "C052 vs C070 - same dataset size range but different rhetorical contexts",
      "Temporal comparison evidence preserved throughout"
    ],
    "quality_checks_performed": [
      "All consolidations have complete metadata",
      "Source integrity maintained through verbatim_quote consolidation",
      "No quantitative values lost",
      "Cross-references validated",
      "Acid test applied throughout",
      "Strategic duplication preserved where needed for claim interpretability"
    ]
  }
}