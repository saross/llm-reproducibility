{
  "section": "Sections 4-5 (Discussion + Conclusion)",
  "pages": "8-11",
  "new_evidence_items": [
    {
      "evidence_id": "E079",
      "evidence_text": "Project staff with desktop GIS experience could digitize at sustained rate of 60-75 features per staff-hour",
      "evidence_type": "baseline digitization rate - expert staff",
      "evidence_basis": "author_assertion",
      "supports_claims": ["C046", "C047"],
      "declared_uncertainty": {
        "type": "bounded_range",
        "indicator": "range",
        "quantification": "60-75 features/hour",
        "severity": "minor"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 9
      },
      "verbatim_quote": "After brief workspace setup, project staff with desktop GIS experience could digitise at a sustained rate of 60–75 features per staff-hour.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E080",
      "evidence_text": "57 staff hours could produce 3,420-4,275 features if staff digitized directly using desktop GIS",
      "evidence_type": "calculated threshold - expert digitization",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C046", "C047"],
      "declared_uncertainty": {
        "type": "bounded_range",
        "quantification": "3,420-4,275 features",
        "severity": "minor"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 9
      },
      "verbatim_quote": "At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420–4,275 staff-digitised features (see Table 4).",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E081",
      "evidence_text": "Based on 2010 rate of 130-180 features per staff-hour supervising volunteers with desktop GIS, 57 hours might produce 7,410-10,260 features",
      "evidence_type": "calculated threshold - supervised desktop GIS",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C048"],
      "declared_uncertainty": {
        "type": "bounded_range",
        "quantification": "7,410-10,260 features",
        "severity": "moderate"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 9
      },
      "verbatim_quote": "Had specialist project staff instead trained and supervised volunteers to use desktop GIS for digitisation, based on our 2010 digitisation rate of 130–180 features per staff-hour, 57 h might have produced 7,410–10,260 features.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E082",
      "evidence_text": "57 staff hours produced 10,827 features using crowdsourcing approach, or about 190 features per staff-hour",
      "evidence_type": "crowdsourcing efficiency measurement",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C002", "C049"],
      "declared_uncertainty": {
        "type": "approximation",
        "indicator": "about",
        "severity": "minor"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 9
      },
      "verbatim_quote": "By comparison, the 57 h of staff time required for our digitisation approach using a customisation of FAIMS Mobile produced 10,827 features, or about 190 features per staff-hour.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E083",
      "evidence_text": "Only 21 of 57 staff hours came from project staff, while 36 hours were from outsourced student programmer",
      "evidence_type": "time allocation breakdown",
      "evidence_basis": "statistical_output",
      "supports_claims": ["C032", "C049"],
      "location": {
        "section": "Section 4.1.1",
        "page": 9
      },
      "verbatim_quote": "First, customisation of systems like FAIMS Mobile can be outsourced more easily than other project activities. Only 21 of the 57 h needed to support the system came from project staff, while the other 36 h were completed by a student programmer for a modest cost (ca. AUD $2,000).",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E084",
      "evidence_text": "21 internal staff hours represent digitization rate of over 500 features per staff-hour",
      "evidence_type": "efficiency calculation - internal staff only",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C049"],
      "location": {
        "section": "Section 4.1.1",
        "page": "9-10"
      },
      "verbatim_quote": "Those 21 internal staff hours represent a digitisation rate of over 500 features per staff-hour.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E085",
      "evidence_text": "21 internal staff hours would yield only 1,260-1,575 features if staff digitized directly, or 2,730-3,780 if supervising desktop GIS volunteers",
      "evidence_type": "comparative threshold calculation",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C049"],
      "declared_uncertainty": {
        "type": "bounded_range",
        "severity": "minor"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Twenty-one hours would have yielded just 1,260–1,575 features if staff had digitised them directly, or 2,730–3,780 had we supervised students using desktop GIS.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E086",
      "evidence_text": "In-field support for volunteers was only 7 hours, representing about 1,550 features per in-field staff-hour",
      "evidence_type": "in-field efficiency measurement",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C050"],
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Across two seasons, in-field support for volunteers was only 7 h, representing about 1,550 features per in-field staff-hour.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E087",
      "evidence_text": "7 in-field staff hours would only allow staff to directly digitize 420-525 features, or supervise digitization of 910-1,260 features",
      "evidence_type": "in-field threshold comparison",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C050"],
      "declared_uncertainty": {
        "type": "bounded_range",
        "severity": "minor"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Seven hours would only allow staff to directly digitise 420–525 features, or supervise the digitisation of 910-1,260.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E088",
      "evidence_text": "Marginal cost for each additional feature: 4.3 seconds of staff support (13 hours in-field support and QA ÷ 10,827 features)",
      "evidence_type": "marginal cost calculation",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C051"],
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Third, the marginal cost for each additional feature digitised is low. This figure includes in-field support and quality assurance (13 h), and translates to 4.3 s of staff support per additional feature.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E089",
      "evidence_text": "Preparing and distributing additional maps took only 6 minutes per map (6 hours for 58 maps)",
      "evidence_type": "map preparation efficiency",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C051"],
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Preparing and distributing additional maps took only 6 min per map (6 h for 58 maps).",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E090",
      "evidence_text": "Adding another field season only costs one additional hour of setup time (based on 2018 redeployment)",
      "evidence_type": "redeployment cost",
      "evidence_basis": "direct_measurement",
      "supports_claims": ["C051"],
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Even adding another field season only costs one additional hour of setup time (based on our 2018 redeployment).",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E091",
      "evidence_text": "Urban Occupations Project reported 1,250 hours of manual digitization to create training data for ML road classification",
      "evidence_type": "ML training data requirement - external benchmark",
      "evidence_basis": "author_assertion",
      "supports_claims": ["C052", "C053"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "The ERC-funded Urban Occupations Project (Can, Gerrits, and Kabadayi 2021), however, provides one benchmark for judging when pursuing a ML approach might be worthwhile. This project reported 1,250 h of manual digitisation to create enough training data to classify roads visible in historical maps of the Ottoman Empire.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E092",
      "evidence_text": "Urban Occupations ML expert spent seven days testing and fine-tuning the model after training data creation",
      "evidence_type": "ML development time",
      "evidence_basis": "author_assertion",
      "supports_claims": ["C052"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Using this input, and after additional preprocessing and filtering, an ML expert spent seven days testing and fine tuning the model.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E093",
      "evidence_text": "Urban Occupations ML approach required minimum of about 1,300 hours preparation time",
      "evidence_type": "total ML setup time",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C052", "C053"],
      "declared_uncertainty": {
        "type": "approximation",
        "indicator": "about",
        "severity": "minor"
      },
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "This example, which appears to have required a minimum of about 1,300 h of preparation time alone, suggests that ML approaches are worthwhile for large-scale projects that benefit from the consistent symbology and style (as found in British and Ottoman imperial maps).",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E094",
      "evidence_text": "At crowdsourcing rate of 44.9 features/person-hour, 1,300 hours would yield about 58,400 records",
      "evidence_type": "ML threshold calculation",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C053"],
      "declared_uncertainty": {
        "type": "approximation",
        "indicator": "about",
        "severity": "moderate"
      },
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "We spent 44 staff hours customising and deploying a streamlined geospatial system in FAIMS Mobile, 184 participant-hours digitising features, seven staff-hours directly supporting that digitisation, and six staff hours checking for errors. These 241 h produced a dataset of 10,827 features, a rate of 44.9 features/person-hour. At that rate, the 1,300 h it took to deploy the ML approach taken by Can, Gerrits, and Kabadayi would yield about 58,400 records",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E095",
      "evidence_text": "Crowdsourcing most suitable for datasets of 10,000-60,000 records (rounded estimate)",
      "evidence_type": "efficiency range recommendation",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C004", "C054"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "To summarise in round numbers, a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000–60,000 records, assuming similar feature characteristics and data collection requirements (see Table 5).",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E096",
      "evidence_text": "Below 10,000 records, desktop GIS approaches should be considered",
      "evidence_type": "threshold recommendation",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C054"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Below 10,000 records, approaches using desktop GIS should be considered.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E097",
      "evidence_text": "Conservative payoff threshold: 3,500-4,500 features vs direct staff digitization; 7,500-10,000 vs supervised volunteers with desktop GIS",
      "evidence_type": "threshold calculation - conservative",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C047", "C048"],
      "declared_uncertainty": {
        "type": "bounded_range",
        "severity": "moderate"
      },
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Under the most conservative scenario that considers all invested time, the use of our system pays off between about 3,500–4,500 features versus direct digitisation by staff using desktop GIS, and about 7,500–10,000 features versus support for volunteers using desktop GIS.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E098",
      "evidence_text": "Projects where staff time is at premium may find mobile approach valuable for datasets below 1,000 records",
      "evidence_type": "qualified threshold recommendation",
      "evidence_basis": "author_assertion",
      "supports_claims": ["C005", "C055"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Projects where staff time is at a premium, or that operate alongside fieldwork where staff have many competing demands, may find it valuable for smaller datasets (even those below 1,000 records).",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E099",
      "evidence_text": "Above 60,000 records, ML approaches should be contemplated, but only if project has access to requisite expertise",
      "evidence_type": "threshold recommendation - ML",
      "evidence_basis": "derived_calculation",
      "supports_claims": ["C053", "C054"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Above 60,000 records, ML approaches should be contemplated, but only if a project has access to the requisite expertise.",
      "extraction_confidence": "medium"
    },
    {
      "evidence_id": "E100",
      "evidence_text": "A typical small project in history or archaeology may not be able to dedicate personnel, infrastructure, or attention needed for ML but could deploy collaborative geospatial system",
      "evidence_type": "feasibility assessment",
      "evidence_basis": "author_assertion",
      "supports_claims": ["C056"],
      "location": {
        "section": "Section 4.3",
        "page": 10
      },
      "verbatim_quote": "Today, a typical project in history or archaeology - often small, under-resourced, and pursuing several research activities - may not be able to dedicate the personnel, infrastructure, or attention needed to incorporate ML successfully, but could deploy a collaborative geospatial system for crowdsourcing map digitisation.",
      "extraction_confidence": "high"
    },
    {
      "evidence_id": "E101",
      "evidence_text": "A project with digital humanist or similar technologist with Software Carpentry-level skills can customize and operate a system like FAIMS Mobile",
      "evidence_type": "technical skill requirement",
      "evidence_basis": "author_assertion",
      "supports_claims": ["C056"],
      "location": {
        "section": "Section 4.3",
        "page": 10
      },
      "verbatim_quote": "A project with a digital humanist or similar technologist with skills at the level of core Software Carpentry lessons (TheCarpentries, 2023) can customise and operate a generalised platform such as FAIMS Mobile to implement an effective crowdsourcing system.",
      "extraction_confidence": "high"
    }
  ],
  "new_claims": [
    {
      "claim_id": "C046",
      "claim_text": "Direct digitization by expert staff using desktop GIS would require 57 staff hours to produce 3,420-4,275 features, making it suitable only for smaller datasets",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E079", "E080"],
      "supports_claims": ["C047", "C054"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "efficiency threshold",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 9
      },
      "verbatim_quote": "At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420–4,275 staff-digitised features (see Table 4). Such a payoff threshold suggests that digitisation by project staff will be suitable only for smaller datasets.",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C047",
      "claim_text": "The crowdsourcing approach becomes worthwhile at around 3,500-4,500 features compared to direct staff digitization using desktop GIS",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E080", "E082", "E097"],
      "supports_claims": ["C004", "C054"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "payoff threshold",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.1, 4.1.2",
        "page": "9-10"
      },
      "verbatim_quote": "Under the most conservative scenario that considers all invested time, the use of our system pays off between about 3,500–4,500 features versus direct digitisation by staff using desktop GIS",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C048",
      "claim_text": "The crowdsourcing approach becomes worthwhile at around 7,500-10,000 features compared to supervising volunteers using desktop GIS",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E081", "E082", "E097"],
      "supports_claims": ["C004", "C054"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "payoff threshold",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.1, 4.1.2",
        "page": "9-10"
      },
      "verbatim_quote": "Under the most conservative scenario that considers all invested time, the use of our system pays off between about...7,500–10,000 features versus support for volunteers using desktop GIS.",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C049",
      "claim_text": "When accounting only for internal staff time (21 hours), the approach is much more efficient (>500 features per staff-hour) than alternatives",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E083", "E084", "E085"],
      "supports_claims": ["C002", "C032"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "efficiency ratio",
        "comparison_type": "ratio"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": "9-10"
      },
      "verbatim_quote": "Only 21 of the 57 h needed to support the system came from project staff, while the other 36 h were completed by a student programmer for a modest cost (ca. AUD $2,000). Those 21 internal staff hours represent a digitisation rate of over 500 features per staff-hour.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C050",
      "claim_text": "In-field staff time was minimal (7 hours) and highly efficient (~1,550 features per hour), making the approach particularly valuable during fieldwork when staff time is scarce",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E053", "E086", "E087"],
      "supports_claims": ["C002", "C033", "C055"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "in-field efficiency",
        "comparison_type": "ratio"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Second, given competing responsibilities, staff time during the field season was scarce and valuable. We could afford to invest in customisation and setup beforehand, and error checking after, if it reduced staff obligations during fieldwork. Across two seasons, in-field support for volunteers was only 7 h, representing about 1,550 features per in-field staff-hour.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C051",
      "claim_text": "The marginal cost per additional feature is low (4.3 seconds staff time), making the approach increasingly valuable as dataset size grows",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E088", "E089", "E090"],
      "supports_claims": ["C002"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "marginal cost",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.1",
        "page": 10
      },
      "verbatim_quote": "Third, the marginal cost for each additional feature digitised is low. This figure includes in-field support and quality assurance (13 h), and translates to 4.3 s of staff support per additional feature. Thus, the larger the dataset, the more value is extracted from the setup and deployment time.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C052",
      "claim_text": "Machine learning approaches require minimum ~1,300 hours investment for training data creation and model development",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E091", "E092", "E093"],
      "supports_claims": ["C008", "C053"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "ML setup time",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "This example, which appears to have required a minimum of about 1,300 h of preparation time alone, suggests that ML approaches are worthwhile for large-scale projects that benefit from the consistent symbology and style (as found in British and Ottoman imperial maps).",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C053",
      "claim_text": "Machine learning becomes worthwhile above ~60,000 records, making crowdsourcing most suitable for the 10,000-60,000 record range",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E091", "E093", "E094", "E099"],
      "supports_claims": ["C004", "C008", "C054"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "ML threshold",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Above 60,000 records, ML approaches should be contemplated, but only if a project has access to the requisite expertise.",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C054",
      "claim_text": "Conservative efficiency estimates suggest: crowdsourcing is most suitable for 10,000-60,000 features; desktop GIS below 10,000; ML above 60,000",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E095", "E096", "E097", "E099"],
      "supports_claims": ["C004", "C007"],
      "quantitative_details": {
        "involves_quantification": true,
        "metric": "optimal range by approach",
        "comparison_type": "absolute"
      },
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "To summarise in round numbers, a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000–60,000 records, assuming similar feature characteristics and data collection requirements (see Table 5). Below 10,000 records, approaches using desktop GIS should be considered. Above 60,000 records, ML approaches should be contemplated",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C055",
      "claim_text": "For projects where staff time is at premium or operating during fieldwork, the mobile crowdsourcing approach may be valuable even for datasets below 1,000 records",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "comparative_assessment",
      "claim_nature": "evaluative",
      "supported_by": ["E098"],
      "supports_claims": ["C005", "C050"],
      "location": {
        "section": "Section 4.1.2",
        "page": 10
      },
      "verbatim_quote": "Projects where staff time is at a premium, or that operate alongside fieldwork where staff have many competing demands, may find it valuable for smaller datasets (even those below 1,000 records).",
      "extraction_confidence": "medium"
    },
    {
      "claim_id": "C056",
      "claim_text": "Typical small HASS projects can feasibly deploy collaborative geospatial systems with Software Carpentry-level technical skills, but may not have resources for ML",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "evaluative",
      "claim_nature": "evaluative",
      "supported_by": ["E100", "E101"],
      "supports_claims": ["C008", "C010"],
      "location": {
        "section": "Section 4.3",
        "page": 10
      },
      "verbatim_quote": "Today, a typical project in history or archaeology - often small, under-resourced, and pursuing several research activities - may not be able to dedicate the personnel, infrastructure, or attention needed to incorporate ML successfully, but could deploy a collaborative geospatial system for crowdsourcing map digitisation. A project with a digital humanist or similar technologist with skills at the level of core Software Carpentry lessons (TheCarpentries, 2023) can customise and operate a generalised platform such as FAIMS Mobile to implement an effective crowdsourcing system.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C057",
      "claim_text": "Crowdsourcing and ML approaches are complementary rather than exclusive: crowdsourcing can produce training datasets and error-checking datasets for ML",
      "claim_type": "interpretation",
      "claim_role": "intermediate",
      "primary_function": "methodological_justification",
      "claim_nature": "evaluative",
      "supports_claims": ["C008", "C009"],
      "location": {
        "section": "Section 4.2",
        "page": 10
      },
      "verbatim_quote": "Finally, since training a model requires a manually produced dataset and a degree of manual error-checking, a combination of ML and crowdsourcing approaches might serve even large-scale projects. A dataset big enough to justify ML will likely need a training dataset big enough to warrant crowdsourcing, especially if the features or background are variable. Once the crowdsourcing platform has been built, moreover, it can be used to produce additional datasets for error-checking to confirm the accuracy of the ML results. The approaches are not exclusive, therefore, but complementary.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C058",
      "claim_text": "The volunteer-based crowdsourcing effort proved unexpectedly successful for a minimally resourced, secondary activity producing >10,000 high-quality features",
      "claim_type": "interpretation",
      "claim_role": "core",
      "primary_function": "evaluative",
      "claim_nature": "evaluative",
      "supported_by": ["E001", "E002", "E005", "E058", "E061"],
      "location": {
        "section": "Section 4",
        "page": 9
      },
      "verbatim_quote": "Our crowdsourced digitisation effort involving novice volunteers using an adapted mobile application for data capture proved unexpectedly successful. It was only an auxiliary activity undertaken on a time-available basis, intentionally secondary to pedestrian survey. While volunteer-based geographic digitisation projects often enjoy high productivity at low-cost (Goodchild, 2007; Simon et al., 2015), they may face data-quality problems and require continuous access to a network (Pődör, 2015; Budig et al., 2016; Lin et al., 2014). Our approach was done under field conditions, with inexpensive equipment and limited internet connectivity, yet produced a large (>10,000 features), high-quality (<6% error rate) dataset while placing reasonable demands on both volunteers and staff compared to other approaches.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C059",
      "claim_text": "More projects need to track and publish time data (expert, volunteer, training, support, QA) plus digitization characteristics to refine and generalize efficiency recommendations",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "future_direction",
      "claim_nature": "evaluative",
      "location": {
        "section": "Section 5 (Conclusion)",
        "page": 11
      },
      "verbatim_quote": "More projects - whether they use manual or automated approaches - need to track and publish the expert and volunteer time required for setup, training, support, and quality assurance related to map digitisation, as well as digitisation speed, error rates and types, the characteristics of the features being digitised, and the complexity of information extracted. More such data could refine and generalise the recommendations proposed here.",
      "extraction_confidence": "high"
    },
    {
      "claim_id": "C060",
      "claim_text": "The approach is readily transferable to other mobile GIS systems and map corpora",
      "claim_type": "interpretation",
      "claim_role": "supporting",
      "primary_function": "methodological_justification",
      "claim_nature": "evaluative",
      "supports_claims": ["C006", "C011"],
      "location": {
        "section": "Section 5 (Conclusion)",
        "page": 11
      },
      "verbatim_quote": "This approach is readily transferable to other mobile GIS systems and map corpora, but our experience provides only a single data point for assessing the applicability of various digitisation approaches to historical maps.",
      "extraction_confidence": "high"
    }
  ]
}
