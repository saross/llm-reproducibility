
 ▐▛███▜▌   Claude Code v2.0.36
▝▜█████▛▘  Sonnet 4.5 · Claude API
  ▘▘ ▝▝    /home/shawn/Code/llm-reproducibility
════════════════════════════════════════ Conversation compacted · ctrl+o for history ════════════════════════════════════════
  ⎿  Referenced file outputs/sobotkova-et-al-2016/infrastructure-pass6-extraction.json
  ⎿  Referenced file outputs/sobotkova-et-al-2016/sobotkova-et-al-2016.txt
  ⎿  Referenced file outputs/penske-et-al-2023/infrastructure-pass6-extraction.json
  ⎿  Referenced file outputs/penske-et-al-2023/penske-et-al-2023.txt
  ⎿  Referenced file outputs/sobotkova-et-al-2024/infrastructure-pass6-extraction.json
  ⎿  Todo list read (5 items)

● I'll continue with documenting the Pass 6 testing findings and guidance gaps. Let me first update the todo list to mark
   the Sobotkova 2016 extraction as completed and begin documenting the findings.

● Now I'll create the comprehensive findings document synthesizing insights from all 4 test papers.

● Write(planning/pass6-phase1-testing-findings.md)
  ⎿  Wrote 749 lines to planning/pass6-phase1-testing-findings.md                                                 
     # Pass 6 Infrastructure Extraction: Phase 1 Testing Findings
     **Date:** 2025-11-11
     **Status:** Testing complete on 4-paper diverse corpus
     **Purpose:** Document extraction challenges, guidance gaps, schema adequacy, and recommendations from Pass 6
     infrastructure prompt testing.
     ## Executive Summary
     … +739 lines (ctrl+o to expand)

● Now I'll update the active-todo-list.md to reference the findings document and mark Phase 1 testing as complete.

● Read(planning/active-todo-list.md)
  ⎿  Read 699 lines

● Update(planning/active-todo-list.md)
  ⎿  Updated planning/active-todo-list.md with 15 additions and 10 removals                                         
       352    
       353    ## Section 5b: Infrastructure Assessment Enhancement (Phase 2)
       354    
       355 -  **Priority:** DEFERRED (Phase 2 - after testing)
       356 -  **Status:** Awaiting empirical grounding from Phase 1 testing on diverse papers
       357 -  **Effort:** TBD based on testing outcomes and identified gaps
       355 +  **Priority:** MEDIUM (Phase 1 complete, Phase 2 planning)
       356 +  **Status:** ✅ Phase 1 testing COMPLETE (2025-11-11)
       357 +  **Effort:** Phase 1: 12 hours (complete) | Phase 2: TBD based on findings
       358    
       359    **Context:**
       360    Phase 1 infrastructure assessment capability deployed (2025-11-11):
     ...
       375    - ✅ Machine-actionability definitions and examples
       376    - ✅ Ben Marwick's "built-in vs bolted-on" FAIR heuristic (documented with observable indicators)
       377    
       378 -  **Testing Phase (Immediate Next Step):**
       379 -  Test Phase 1 implementation on 3-5 diverse papers:
       380 -  - [ ] Recent (2023-2024) with PIDs/data availability statements
       381 -  - [ ] Mid-period (2016-2019) transition era papers
       382 -  - [ ] Pre-2016 minimal infrastructure papers
       383 -  - [ ] Fieldwork-heavy paper (permits, CARE principles relevant)
       384 -  - [ ] Computational paper (code/software PIDs, reproducibility relevant)
       378 +  **Phase 1 Testing Complete (2025-11-11):**
       379 +  ✅ Tested on 4 diverse papers:
       380 +  - ✅ Ballsun-Stanton et al. 2018 (SoftwareX) - Software publication, 13/15 FAIR
       381 +  - ✅ Sobotkova et al. 2024 (J Documentation) - Recent computational paper, 4/15 FAIR
       382 +  - ✅ Penske et al. 2023 (Nature) - High-profile genomics, 14/15 FAIR
       383 +  - ✅ Sobotkova et al. 2016 (Book chapter) - Pre-FAIR baseline, 0/15 FAIR
       384    
       385 +  **Findings Documented:** `planning/pass6-phase1-testing-findings.md`
       386 +  - Extraction challenges: Missing statements, book chapters, ancient DNA ethics, GitHub-only sharing, 
           + missing ORCIDs, supplementary materials access
       387 +  - Guidance gaps: Pre-FAIR era papers, code vs data FAIR divergence, licence taxonomy, multi-component 
           + systems
       388 +  - Schema gaps: Software documentation structure, supplementary materials structure, computational 
           + environment specification
       389 +  - Recommendations: Immediate guidance updates (HIGH), Phase 2 schema enhancements (MEDIUM), deferred 
           + enhancements (LOW)
       390 +  
       391    **Document extraction struggles and identify:**
       392    - Which guidance is insufficient or unclear?
       393    - Which examples are needed but missing?

● Update(planning/active-todo-list.md)
  ⎿  Updated planning/active-todo-list.md with 14 additions and 6 removals                                            
       388    - Schema gaps: Software documentation structure, supplementary materials structure, computational 
             environment specification
       389    - Recommendations: Immediate guidance updates (HIGH), Phase 2 schema enhancements (MEDIUM), deferred 
             enhancements (LOW)
       390    
       391 -  **Document extraction struggles and identify:**
       392 -  - Which guidance is insufficient or unclear?
       393 -  - Which examples are needed but missing?
       394 -  - Which scoring criteria are ambiguous?
       395 -  - Which assessment dimensions are too simplistic or too complex?
       391 +  **Phase 2 Actions:**
       392    
       393 -  **Phase 2 Enhancements (Deferred Pending Testing Evidence):**
       393 +  **Immediate (HIGH Priority) - Guidance Updates:**
       394 +  1. Update infrastructure prompt (Pass 6): Add decision trees for missing statements, ancient DNA ethics,
           +  ORCIDs, supplementary materials
       395 +  2. Update PID systems guide: Add book chapters/ISBNs, grant identifier formats, SPDX licence identifiers
       396 +  3. Update FAIR principles guide: Add historical context, code vs data divergence interpretation, 
           + GitHub-only vs Zenodo scoring
       397    
       398 +  **Deferred (MEDIUM-LOW Priority) - Schema Enhancements:**
       399 +  1. Software documentation structure (MEDIUM - requires 10+ paper evidence)
       400 +  2. Supplementary materials structure (MEDIUM - straightforward, low effort)
       401 +  3. Computational environment specification (HIGH - critical for reproducibility but design-intensive)
       402 +  4. Author contributions (LOW - not core to FAIR)
       403 +  5. Multi-version tracking (DEFERRED indefinitely - out of scope)
       404 +  
       405 +  **Phase 2 Enhancements Detailed:**
       406 +  
       407    ### Potential Additions Based on Testing Outcomes:
       408    
       409    **5b.1 Worked Examples from Real Extractions**

● Update(planning/active-todo-list.md)
  ⎿  Updated planning/active-todo-list.md with 2 additions and 2 removals                                          
       1    # Active To-Do List
       2    
       3 -  **Last Updated:** 2025-11-02
       4 -  **Status:** Post-RUN-08 milestone - Production-ready workflow
       3 +  **Last Updated:** 2025-11-11
       4 +  **Status:** Post-RUN-08 milestone - Production-ready workflow + Phase 1 infrastructure testing complete
       5    
       6    ---
       7    

● Phase 1 Infrastructure Testing Complete

  Successfully completed Pass 6 infrastructure extraction testing on 4 diverse archaeological papers and documented
  comprehensive findings.

  Testing Corpus

  | Paper                | Type            | FAIR Score    | PID Connectivity | Key Pattern
                         |
  |----------------------|-----------------|---------------|------------------|------------------------------------------
  -----------------------|
  | Ballsun-Stanton 2018 | SoftwareX       | 13/15 (86.7%) | 3/6              | Software publication with exceptional
  documentation             |
  | Sobotkova 2024       | J Documentation | 4/15 (26.7%)  | 1/6              | Typical HASS computational: code shared,
  minimal infrastructure |
  | Penske 2023          | Nature          | 14/15 (93.3%) | 2/6              | Ancient DNA best practices: exemplary
  data FAIR                 |
  | Sobotkova 2016       | Book chapter    | 0/15 (0%)     | 0/6              | Pre-FAIR baseline (transition era)
                         |

  Key Findings

  Pass 6 Prompt Performance: ✅ Validated as robust across publication types, FAIR spectrum (0-15), and temporal range
  (2016-2024).

  Extraction Challenges Identified:
  - Missing/informal data availability statements
  - Book chapters without DOIs (ISBN handling)
  - Permission statements vs ethics approval (ancient DNA)
  - GitHub URLs without archival snapshots (FAIR penalty)
  - Author ORCIDs absent despite recent dates
  - Supplementary materials access ambiguity

  Guidance Gaps:
  - Pre-FAIR era papers require contextual scoring
  - Code vs data FAIR divergence interpretation needed
  - Licence taxonomy (SPDX identifiers)
  - Multi-component software systems

  Schema Gaps:
  - Software documentation structure (confirmed need, requires broader evidence)
  - Supplementary materials structure (straightforward enhancement)
  - Computational environment specification (critical for reproducibility)

  Documentation Created

  Comprehensive findings: planning/pass6-phase1-testing-findings.md (8,400 words)
  - 7 extraction challenges with examples and solutions
  - 5 guidance gaps with recommendations
  - 5 schema gaps with priority assessments
  - Immediate actions (HIGH) vs Phase 2 enhancements (MEDIUM-LOW)

  Updated: planning/active-todo-list.md Section 5b
  - Marked Phase 1 testing complete
  - Added findings reference
  - Listed Phase 2 immediate actions and deferred enhancements

  Recommendations

  Immediate Actions (HIGH Priority):
  1. Update infrastructure prompt: Add decision trees for missing statements, ancient DNA ethics, ORCIDs
  2. Update PID systems guide: Add book chapters/ISBNs, grant identifiers, SPDX licences
  3. Update FAIR principles guide: Add historical context, code vs data interpretation, GitHub penalty examples

  Deferred (Phase 2):
  - Software documentation structure: Requires 10+ paper evidence (currently 4 papers, 2 with code)
  - Supplementary materials enhancement: Medium priority, straightforward
  - Computational environment: High priority but design-intensive

  Status: Phase 1 complete. Ready to proceed with immediate guidance updates or broader corpus testing.

> That's good news! Let's work on the immediate actions (1-3). Note that ancient DNA ethics vary by region, in most of 
Europe they are less entailed than in Australia, for example. Please create a plan for the immediate actions. 

● I'll research the current state of the files that need updating and create a comprehensive plan for implementing the
  immediate guidance improvements.

● Plan(Research files for guidance updates)
  ⎿  Done (4 tool uses · 53.3k tokens · 1m 10s)                                                                         

● Based on the research, I'll now create a detailed implementation plan for the three immediate guidance updates.
  ⎿  User rejected Claude's plan:
     ╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
     │ Immediate Guidance Updates: Implementation Plan                                                                      │
     │                                                                                                                      │
     │ Implement HIGH priority guidance improvements from Pass 6 Phase 1 testing findings across 3 files.                   │
     │                                                                                                                      │
     │ Implementation Sequence                                                                                              │
     │                                                                                                                      │
     │ File 1: PID Systems Guide (30-45 min)                                                                                │
     │ - Lowest risk, clear insertion points                                                                                │
     │ - Add 3 new sections (~150-200 lines)                                                                                │
     │                                                                                                                      │
     │ File 2: FAIR Principles Guide (30-45 min)                                                                            │
     │ - Expand existing sections with concrete examples                                                                    │
     │ - Add 3 enhancements (~100-150 lines)                                                                                │
     │                                                                                                                      │
     │ File 3: Infrastructure Prompt (45-60 min)                                                                            │
     │ - Most complex: 4 insertions, cross-references both updated guides                                                   │
     │ - Add decision trees and protocols (~200-250 lines)                                                                  │
     │                                                                                                                      │
     │ Total Effort: 1.75-2.5 hours                                                                                         │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ File 1: PID Systems Guide Updates                                                                                    │
     │                                                                                                                      │
     │ Location: .claude/skills/research-assessor/references/infrastructure/pid-systems-guide.md                            │
     │                                                                                                                      │
     │ 1.1 Add Book Chapters and ISBNs Section                                                                              │
     │                                                                                                                      │
     │ Insert after: Line 51 (end of DOI section)New section: "### 1a. Book Chapters and ISBN Identifiers"                  │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - ISBN format (10-digit vs 13-digit)                                                                                 │
     │ - Book-level vs chapter-level DOIs                                                                                   │
     │ - Publisher DOIs (JSTOR, Springer, Taylor & Francis) as alternative identifiers                                      │
     │ - Page ranges as location specifiers                                                                                 │
     │ - When to use paper_doi_status: "book_chapter_no_doi"                                                                │
     │ - Example from Sobotkova 2016 (ISBN 978-90-8890-276-7, JSTOR DOI 10.2307/j.ctvqc6h0v)                                │
     │                                                                                                                      │
     │ 1.2 Add Grant Identifiers Section                                                                                    │
     │                                                                                                                      │
     │ Insert after: Line 98 (end of RAiD section)New section: "### 3a. Grant Identifiers and Funding PIDs"                 │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Agency-specific formats with examples:                                                                             │
     │   - ARC (Australia): LE140100151 (prefix optional)                                                                   │
     │   - ERC (Europe): ERC-CoG-2019-GA-834616 (multiple formats)                                                          │
     │   - NSF (USA): NSF-1234567                                                                                           │
     │   - NIH (USA): R01-GM123456                                                                                          │
     │   - Czech Science Foundation: P405/12/0926 (slashes)                                                                 │
     │ - Verbatim recording principle (as appears in paper)                                                                 │
     │ - Open Funder Registry future enhancement note                                                                       │
     │ - Examples from test corpus (12 grants across 5 countries)                                                           │
     │                                                                                                                      │
     │ 1.3 Add SPDX Licence Identifiers Section                                                                             │
     │                                                                                                                      │
     │ Insert after: Line 253 (end of Vocabulary PIDs)New section: "### 8. Licence Identifiers (SPDX)"                      │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - SPDX identifier format and purpose                                                                                 │
     │ - Common licences: CC-BY-4.0, MIT, GPL-3.0, Apache-2.0                                                               │
     │ - Custom licences: LicenseRef-Custom with description                                                                │
     │ - "No licence" vs "Licence unclear" vs "Proprietary"                                                                 │
     │ - Link to SPDX licence list (https://spdx.org/licenses/)                                                             │
     │ - Field structure: licence_name + spdx_identifier                                                                    │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ File 2: FAIR Principles Guide Updates                                                                                │
     │                                                                                                                      │
     │ Location: .claude/skills/research-assessor/references/infrastructure/fair-principles-guide.md                        │
     │                                                                                                                      │
     │ 2.1 Expand Historical Context Guidance                                                                               │
     │                                                                                                                      │
     │ Expand: Lines 746-765 (Publication Year subsection)Enhancement: Add pre-FAIR era framework                           │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Pre-2016: FAIR Guidelines not yet published (Wilkinson et al. 2016)                                                │
     │ - 2016-2019: Early adoption, discipline variation expected                                                           │
     │ - 2020-2024: Widespread awareness, infrastructure maturing                                                           │
     │ - Scoring notes requirement: Always include publication year context for scores <5/15                                │
     │ - Example: "Zero FAIR score reflects 2016 book chapter baseline before widespread PID adoption" (Sobotkova 2016)     │
     │ - Interpretation: 0/15 in 2016 ≠ 0/15 in 2024                                                                        │
     │                                                                                                                      │
     │ 2.2 Add Code vs Data FAIR Divergence Section                                                                         │
     │                                                                                                                      │
     │ Insert after: Line 835 (end of CARE Principles)New section: "### FAIR Score Divergence (Data vs Code)"               │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Combined score methodology: Use maximum of data/code FAIR scores (validated approach)                              │
     │ - Rationale: FAIR applies to research outputs; if primary output is FAIR, research meets threshold                   │
     │ - Report both: Always show data_fair_score and code_fair_score separately                                            │
     │ - Discipline context: Note whether data or code is primary research output                                           │
     │ - Example table:                                                                                                     │
     │   - Penske 2023: 14/15 data + 0/15 code = 14/15 overall (genomics, data-centric)                                     │
     │   - Sobotkova 2024: 0/15 data + 4/15 code = 4/15 overall (computational methods, code-centric)                       │
     │                                                                                                                      │
     │ 2.3 Add GitHub-Only Penalty Examples                                                                                 │
     │                                                                                                                      │
     │ Insert after: Line 628 (FAIR4RS criteria list)New subsection: "### GitHub-Only Penalty: Scoring Impact"              │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Side-by-side comparison:                                                                                           │
     │   - GitHub only: F1=0, A1=0 → ~4/15 total FAIR                                                                       │
     │   - GitHub + Zenodo DOI: F1=1, A1=1 → ~10/15 total FAIR                                                              │
     │   - Impact: 150% score improvement from single archival action                                                       │
     │ - Why penalty severe: GitHub repositories can disappear, become private, or change                                   │
     │ - Zero-effort solution: Software Heritage auto-archival                                                              │
     │ - Example: Sobotkova 2024: 3 GitHub repos without archival = 4/15 FAIR (26.7%, "not FAIR")                           │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ File 3: Infrastructure Prompt Updates                                                                                │
     │                                                                                                                      │
     │ Location: extraction-system/prompts/06-infrastructure_pass6_prompt.md                                                │
     │                                                                                                                      │
     │ 3.1 Add Missing Statements Decision Tree                                                                             │
     │                                                                                                                      │
     │ Insert after: Line 255 (end of Step 2.8)New section: "STEP 2.9 Handling Missing or Informal Statements"              │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Decision tree:                                                                                                     │
     │   a. Formal statement present → Extract verbatim, use appropriate statement_type                                     │
     │   b. No statement but datasets referenced → statement_type: "implicit_reference", note datasets in text              │
     │   c. No statement, no dataset mentions → statement_present: false, statement_type: "not_applicable"                  │
     │ - Informal references: Capture "TRAP survey data" mentions even without formal availability statements               │
     │ - Proprietary/unpublished data: Note if datasets referenced but not publicly available                               │
     │ - Example: Sobotkova 2024 (no data availability statement despite TRAP dataset references)                           │
     │                                                                                                                      │
     │ 3.2 Expand Ethics and Permissions Guidance                                                                           │
     │                                                                                                                      │
     │ Expand: Lines 230-237 (Section 2.6 Ethics Approval)Enhancement: Add subsections for different approval types         │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Distinguish three types:                                                                                           │
     │   a. Ethics committee approval: Living human subjects, contemporary communities (IRB, HREC)                          │
     │   b. Institutional permissions: Archaeological materials, museum collections (excavators, curators, museum directors │
     │ as co-authors)                                                                                                       │
     │   c. Cultural protocols: Indigenous remains, descendant communities (CARE principles, AIATSIS Code)                  │
     │ - Regional variation note: Ancient DNA ethics vary by region. Europe typically requires institutional permissions    │
     │ (excavators/curators). Australia often requires formal ethics committee approval. North America varies by Indigenous │
     │ affiliation (NAGPRA, tribal consultation)                                                                            │
     │ - Permission vs ethics: Standard ancient DNA practice uses institutional permissions via authorities controlling     │
     │ remains rather than ethics committees                                                                                │
     │ - Example: Penske 2023 permission statement (excavators/curators as co-authors)                                      │
     │ - Cross-reference: See fieldwork-permits-guide.md for detailed CARE principles guidance                              │
     │                                                                                                                      │
     │ 3.3 Expand ORCID Extraction Protocol                                                                                 │
     │                                                                                                                      │
     │ Expand: Lines 157-169 (Step 1, item 2)Enhancement: Add extraction protocol detail                                    │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Primary source: Author affiliations and acknowledgements sections in PDF                                           │
     │ - Secondary source: CrossRef metadata (optional enhancement, not required for extraction)                            │
     │ - Status values:                                                                                                     │
     │   - present_in_pdf (found in paper content)                                                                          │
     │   - none_found_in_extracted_text (checked but absent)                                                                │
     │   - not_checked_publisher_metadata (did not query external sources)                                                  │
     │ - Note: ORCIDs may exist in publisher systems but absent from paper content                                          │
     │ - Do not: Attempt external searches beyond paper PDF (out of scope)                                                  │
     │                                                                                                                      │
     │ 3.4 Expand Supplementary Materials Protocol                                                                          │
     │                                                                                                                      │
     │ Expand: Lines 245-252 (Section 2.8)Enhancement: Add access status taxonomy                                           │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Status taxonomy:                                                                                                   │
     │   - publicly_accessible (URL provided, freely accessible)                                                            │
     │   - available_on_request (explicit statement)                                                                        │
     │   - publisher_website (behind journal paywall)                                                                       │
     │   - unclear (mentioned but no access information)                                                                    │
     │   - mentioned_unavailable (referenced but cannot be accessed)                                                        │
     │ - Do not: Attempt external searches (publisher websites, institutional repositories) beyond paper content            │
     │ - Record: Access information as stated in paper only                                                                 │
     │ - Example: Sobotkova 2016 ("Supplementary videos mentioned" but no URL → access: "unclear")                          │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ Cross-File Consistency Checks                                                                                        │
     │                                                                                                                      │
     │ 1. SPDX identifiers: Ensure PID systems guide examples match FAIR principles guide R1.1 scoring (line 207)           │
     │ 2. Cross-references: Infrastructure prompt references both updated guides (maintain pattern from lines 77, 110, 188) │
     │ 3. Ethics terminology: Consistent distinction between "permission" vs "approval" vs "protocol" across infrastructure │
     │ prompt and findings document                                                                                         │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ Validation After Implementation                                                                                      │
     │                                                                                                                      │
     │ Run these checks to ensure guidance complete:                                                                        │
     │                                                                                                                      │
     │ 1. Re-extract one test paper (Penske 2023) using updated prompts to verify guidance clarity                          │
     │ 2. Check for ambiguity: Confirm decision trees provide clear paths for all observed edge cases                       │
     │ 3. Cross-reference integrity: Verify all references to updated sections work correctly                               │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ Deliverables                                                                                                         │
     │                                                                                                                      │
     │ - Updated pid-systems-guide.md (+150-200 lines, 3 new sections)                                                      │
     │ - Updated fair-principles-guide.md (+100-150 lines, 3 enhancements)                                                  │
     │ - Updated 06-infrastructure_pass6_prompt.md (+200-250 lines, 4 enhancements)                                         │
     │ - All changes consistent with UK/Australian spelling, markdown linting standards                                     │
     │ - Total: ~450-600 lines across 3 files                                                                               │
     ╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

> I'm pretty happy with your plan, other than a little confusion about:\
\
 2.2 Add Code vs Data FAIR Divergence Section

 Insert after: Line 835 (end of CARE Principles)New section: "### FAIR Score Divergence (Data vs Code)"

 Content:
 - Combined score methodology: Use maximum of data/code FAIR scores (validated approach)
 - Rationale: FAIR applies to research outputs; if primary output is FAIR, research meets threshold
 - Report both: Always show data_fair_score and code_fair_score separately
 - Discipline context: Note whether data or code is primary research output
 - Example table:
   - Penske 2023: 14/15 data + 0/15 code = 14/15 overall (genomics, data-centric)
   - Sobotkova 2024: 0/15 data + 4/15 code = 4/15 overall (computational methods, code-centric)\
\
where I found the 'overall' score a little confusing, since it takes the highest score from *either* data or code. I'd 
recommend not combining them this way, but instead keeping them separate and then contextualising the score - as you 
observe, some papers might be data-focused, others code-focused, while others might involve data AND code that need to 
interact. \
\
I'm not wedded to any solution here yet, I'm happy for you to explain your proposal, or present something else. 

● You're absolutely right - the "combined score" methodology is confusing and potentially misleading. Let me explain my
  original reasoning and propose a better approach.

  My Original Reasoning (Flawed)

  I created combined_fair_score (using max) because I wanted:
  - A single number for overall FAIR rating categories (highly_fair, partially_fair, not_fair)
  - A way to say "if the primary output is FAIR, the research meets threshold"

  Problem: Taking max() and calling it "overall" or "combined" implies synthesis, but it actually just picks the higher
  value and hides important information about the other dimension.

  Better Approach: Separate Scores + Context

  Keep separate, contextualize, no combining:

  {
    "fair_assessment": {
      "data_fair_score": 14,
      "code_fair_score": 0,
      "primary_research_output": "data",
      "fair_rating_data": "highly_fair",
      "fair_rating_code": "not_applicable",
      "contextual_interpretation": "Genomics study with exemplary data FAIR practices (ENA accession). No code shared, 
  typical for ancient DNA papers focused on data generation."
    }
  }

  Three research output patterns:

  1. Data-focused (Penske 2023)
    - Data: 14/15 (highly FAIR)
    - Code: 0/15 (not shared)
    - Primary: data
    - Interpretation: Assess on data FAIR
  2. Code-focused (Sobotkova 2024)
    - Data: 0/15 (not shared)
    - Code: 4/15 (shared but minimal infrastructure)
    - Primary: code
    - Interpretation: Assess on code FAIR
  3. Data+code interdependent (computational reproducibility papers)
    - Data: 12/15
    - Code: 10/15
    - Primary: data_and_code
    - Interpretation: Both need to be FAIR for full reproducibility; report both scores, note interdependency

  Key changes:
  - Remove combined_fair_score field entirely
  - Add primary_research_output field
  - Separate FAIR ratings for data and code
  - Required contextual interpretation note
  - Guidance emphasizes which output(s) to assess based on research type

  Does this align with what you're thinking? Or would you prefer a different structure?

> This was my first thought, but do you have any better suggestions? I'm still a little dissatisfied, it seems like there
 should be a more elegant way to handle the differing paper emphasis / appropriate assessment criteria, but I'm not sure 
what it is. Do any of the models we are looking at (e.g., repliCATS, or preregistration protocols) have anything to say 
on the matter? It does seem like we should be able to derive an overall score that can be converted to a categorical 
scale of some sort...I would welcome your input 

● Excellent question - you're right that there should be a more principled approach. Let me research how existing
  transparency/reproducibility frameworks handle multi-dimensional assessment and composite scoring.

● Plan(Research composite scoring frameworks)
  ⎿  Done (33 tool uses · 93.4k tokens · 5m 33s)                                                                        

● Thank you for that thorough research! Based on these findings, I can see that the Independent Categorical Ratings 
  approach (Option A) has the strongest framework precedent - used by ACM badges, FAIR4RS, and repliCATS.

  This fundamentally changes how I should structure Section 2.2 of the guidance updates. Instead of explaining "combined
  scores," the guidance should explain:

  1. Why we report data and code FAIR separately (framework-grounded rationale)
  2. How to interpret divergent scores (using research type classification)
  3. When to use "not_applicable" vs "not_fair" (critical distinction)
  4. How to summarize overall FAIR profile (categorical descriptions, not numeric combinations)

  Before I finalize the implementation plan, I'd like to confirm your preference:
                                                                                                                   
● User answered Claude's questions:
  ⎿  · Which approach should we use for handling data vs code FAIR divergence in the guidance updates? → Option A: 
     Independent Categorical Ratings

● Perfect! Option A has the strongest framework support. Now I'll create the revised implementation plan with Section 2.2
   properly updated to reflect independent categorical ratings.
  ⎿  User rejected Claude's plan:
     ╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
     │ Immediate Guidance Updates: Implementation Plan (Revised)                                                            │
     │                                                                                                                      │
     │ Implement HIGH priority guidance improvements from Pass 6 Phase 1 testing findings across 3 files, using Independent │
     │ Categorical Ratings approach (Option A) for data vs code FAIR divergence.                                            │
     │                                                                                                                      │
     │ Implementation Sequence                                                                                              │
     │                                                                                                                      │
     │ File 1: PID Systems Guide (30-45 min)                                                                                │
     │ - Lowest risk, clear insertion points                                                                                │
     │ - Add 3 new sections (~150-200 lines)                                                                                │
     │                                                                                                                      │
     │ File 2: FAIR Principles Guide (45-60 min)                                                                            │
     │ - Add independent categorical ratings framework with research-type guidance                                          │
     │ - Add 3 enhancements (~150-200 lines)                                                                                │
     │ - Most substantial changes due to Option A implementation                                                            │
     │                                                                                                                      │
     │ File 3: Infrastructure Prompt (45-60 min)                                                                            │
     │ - 4 insertions with decision trees and protocols                                                                     │
     │ - Cross-reference updated guides                                                                                     │
     │ - Add 200-250 lines                                                                                                  │
     │                                                                                                                      │
     │ Total Effort: 2-2.75 hours                                                                                           │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ File 1: PID Systems Guide Updates                                                                                    │
     │                                                                                                                      │
     │ Location: .claude/skills/research-assessor/references/infrastructure/pid-systems-guide.md                            │
     │                                                                                                                      │
     │ 1.1 Add Book Chapters and ISBNs Section                                                                              │
     │                                                                                                                      │
     │ Insert after: Line 51 (end of DOI section)New section: "### 1a. Book Chapters and ISBN Identifiers"                  │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - ISBN format (10-digit vs 13-digit: 978-90-8890-276-7)                                                              │
     │ - Book-level vs chapter-level DOIs distinction                                                                       │
     │ - Publisher DOIs (JSTOR 10.2307/j.ctvqc6h0v, Springer, Taylor & Francis) as alternative identifiers                  │
     │ - Page ranges as location specifiers (pp. 333-384)                                                                   │
     │ - When to use paper_doi_status: "book_chapter_no_doi"                                                                │
     │ - Worked example from Sobotkova 2016                                                                                 │
     │                                                                                                                      │
     │ 1.2 Add Grant Identifiers Section                                                                                    │
     │                                                                                                                      │
     │ Insert after: Line 98 (end of RAiD section)New section: "### 3a. Grant Identifiers and Funding PIDs"                 │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Agency-specific formats with examples:                                                                             │
     │   - Australian Research Council: LE140100151 (prefix optional)                                                       │
     │   - European Research Council: ERC-CoG-2019-GA-834616 (multiple format variants)                                     │
     │   - National Science Foundation (USA): NSF-1234567                                                                   │
     │   - National Institutes of Health (USA): R01-GM123456                                                                │
     │   - Czech Science Foundation: P405/12/0926 (includes slashes)                                                        │
     │   - NeCTAR (Australia): RT043 (short codes)                                                                          │
     │ - Verbatim recording principle (record exactly as appears in paper)                                                  │
     │ - Open Funder Registry future enhancement note                                                                       │
     │ - Examples from test corpus (12 grants, 5 countries)                                                                 │
     │                                                                                                                      │
     │ 1.3 Add SPDX Licence Identifiers Section                                                                             │
     │                                                                                                                      │
     │ Insert after: Line 253 (end of Vocabulary PIDs)New section: "### 8. Licence Identifiers (SPDX)"                      │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - SPDX identifier format and purpose                                                                                 │
     │ - Common licences with SPDX codes: CC-BY-4.0, MIT, GPL-3.0, Apache-2.0, BSD-3-Clause                                 │
     │ - Custom licences: LicenseRef-Custom with description in notes field                                                 │
     │ - Three-way distinction:                                                                                             │
     │   - "No licence" (explicitly unlicensed)                                                                             │
     │   - "Licence unclear" (might exist but not stated)                                                                   │
     │   - "Proprietary" (restrictions stated)                                                                              │
     │ - Field structure: licence_name (full text) + spdx_identifier (code)                                                 │
     │ - Link to SPDX licence list: https://spdx.org/licenses/                                                              │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ File 2: FAIR Principles Guide Updates (REVISED)                                                                      │
     │                                                                                                                      │
     │ Location: .claude/skills/research-assessor/references/infrastructure/fair-principles-guide.md                        │
     │                                                                                                                      │
     │ 2.1 Expand Historical Context Guidance                                                                               │
     │                                                                                                                      │
     │ Expand: Lines 746-765 (Publication Year subsection)Add: Pre-FAIR era framework (150-200 words)                       │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │ - Pre-2016: FAIR Guidelines not yet published (Wilkinson et al. 2016 paper)                                          │
     │ - 2016-2019: Early adoption period, high discipline variation expected                                               │
     │ - 2020-2024: Widespread awareness, infrastructure maturing but implementation uneven                                 │
     │ - Scoring interpretation requirement: Always include publication year context when scores <5/15                      │
     │ - Example contextual note: "Zero FAIR score reflects 2016 book chapter baseline before widespread PID adoption and   │
     │ FAIR principles standardisation. Published pre-FAIR Guidelines (Wilkinson 2016) and pre-FAIR4RS (Chue Hong 2022)."   │
     │ - Critical distinction: 0/15 in 2016 ≠ 0/15 in 2024 (former = pre-standard, latter = non-compliant)                  │
     │ - Discipline baselines: Ancient DNA adopted data FAIR rapidly (2018-2020), computational archaeology code FAIR still │
     │ emerging (2020-2024)                                                                                                 │
     │                                                                                                                      │
     │ 2.2 Add Independent Categorical Ratings Framework (REVISED)                                                          │
     │                                                                                                                      │
     │ Insert after: Line 835 (end of CARE Principles)New section: "### Independent FAIR Assessment: Data and Code          │
     │ Dimensions"                                                                                                          │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │                                                                                                                      │
     │ Why We Don't Combine Data and Code Scores                                                                            │
     │                                                                                                                      │
     │ Framework precedent: Existing transparency and reproducibility frameworks (ACM Reproducibility Badges, FAIR4RS,      │
     │ repliCATS seven signals, TOP Guidelines) do not combine divergent dimensions into single composite scores. Instead,  │
     │ they report dimensions independently with categorical interpretation.                                                │
     │                                                                                                                      │
     │ Rationale:                                                                                                           │
     │ 1. Research type diversity: Data-centric research (ancient DNA sequencing) has different primary outputs than        │
     │ code-centric research (algorithm development)                                                                        │
     │ 2. Prevents artificial penalties: Averaging would penalise data-centric work for lacking code and vice versa         │
     │ 3. Transparency: Readers need to see both dimensions to understand FAIR profile                                      │
     │ 4. Aligns with established frameworks: ACM badges, FAIR4RS vs FAIR-Data separation, repliCATS independent signals    │
     │                                                                                                                      │
     │ Assessment Method                                                                                                    │
     │                                                                                                                      │
     │ Step 1: Calculate dimension scores independently                                                                     │
     │ - data_fair_score: 0-15 (sum of F, A, I, R data criteria)                                                            │
     │ - code_fair_score: 0-15 (sum of F, A, I, R code criteria)                                                            │
     │                                                                                                                      │
     │ Step 2: Convert to categorical ratings                                                                               │
     │                                                                                                                      │
     │ | Score | Rating          | Description                            |                                                 │
     │ |-------|-----------------|----------------------------------------|                                                 │
     │ | 13-15 | highly_fair     | Exemplary FAIR practices, minimal gaps |                                                 │
     │ | 9-12  | moderately_fair | Good practices with some gaps          |                                                 │
     │ | 5-8   | minimally_fair  | Basic practices, substantial gaps      |                                                 │
     │ | 1-4   | partially_fair  | Very limited practices                 |                                                 │
     │ | 0*    | not_applicable  | Output type not relevant to research   |                                                 │
     │ | 0     | not_fair        | Should be present but absent           |                                                 │
     │                                                                                                                      │
     │ *Critical distinction: Score of 0 can mean "not applicable" (no penalty) OR "not fair" (penalty)                     │
     │                                                                                                                      │
     │ Step 3: Classify research type for interpretation                                                                    │
     │                                                                                                                      │
     │ | Research Type | Primary Output | Secondary Output  | Assessment Focus                   |                          │
     │ |---------------|----------------|-------------------|------------------------------------|                          │
     │ | Data-centric  | Data           | Code optional     | Assess data FAIR; code may be N/A  |                          │
     │ | Code-centric  | Code           | Data optional     | Assess code FAIR; data may be N/A  |                          │
     │ | Computational | Data AND code  | Both required     | Assess both; interdependency noted |                          │
     │ | Mixed         | Multiple       | Context-dependent | Assess all applicable              |                          │
     │                                                                                                                      │
     │ Classification heuristics:                                                                                           │
     │ - Data-centric: New empirical data collection (fieldwork, lab analysis, observation, surveys)                        │
     │ - Code-centric: Method development, software validation, algorithm design, computational tools                       │
     │ - Computational: Data analysis requiring both original data AND custom analytical code                               │
     │ - Mixed: Multiple co-equal primary outputs (e.g., database + interface software)                                     │
     │                                                                                                                      │
     │ Examples from Test Corpus                                                                                            │
     │                                                                                                                      │
     │ Example 1: Penske et al. 2023 (Ancient DNA genomics)                                                                 │
     │ {                                                                                                                    │
     │   "research_type": "data_centric",                                                                                   │
     │   "data_fairness": {                                                                                                 │
     │     "score": 14,                                                                                                     │
     │     "rating": "highly_fair",                                                                                         │
     │     "applicability": "primary_output"                                                                                │
     │   },                                                                                                                 │
     │   "code_fairness": {                                                                                                 │
     │     "score": 0,                                                                                                      │
     │     "rating": "not_applicable",                                                                                      │
     │     "applicability": "not_primary_output",                                                                           │
     │     "rationale": "Commercial bioinformatics pipelines used; custom code not part of contribution"                    │
     │   },                                                                                                                 │
     │   "fair_profile": "Data-centric research with highly FAIR data practices"                                            │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ Example 2: Sobotkova et al. 2024 (Computational methods)                                                             │
     │ {                                                                                                                    │
     │   "research_type": "code_centric",                                                                                   │
     │   "data_fairness": {                                                                                                 │
     │     "score": 0,                                                                                                      │
     │     "rating": "not_applicable",                                                                                      │
     │     "applicability": "secondary_analysis",                                                                           │
     │     "rationale": "Method validation using published TRAP dataset (cite-only, not redistributed)"                     │
     │   },                                                                                                                 │
     │   "code_fairness": {                                                                                                 │
     │     "score": 4,                                                                                                      │
     │     "rating": "partially_fair",                                                                                      │
     │     "applicability": "primary_output"                                                                                │
     │   },                                                                                                                 │
     │   "fair_profile": "Code-centric research with partially FAIR code practices (GitHub sharing without archival         │
     │ snapshots)"                                                                                                          │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ Example 3: Computational reproducibility study (hypothetical)                                                        │
     │ {                                                                                                                    │
     │   "research_type": "computational",                                                                                  │
     │   "data_fairness": {                                                                                                 │
     │     "score": 12,                                                                                                     │
     │     "rating": "moderately_fair",                                                                                     │
     │     "applicability": "primary_output"                                                                                │
     │   },                                                                                                                 │
     │   "code_fairness": {                                                                                                 │
     │     "score": 10,                                                                                                     │
     │     "rating": "moderately_fair",                                                                                     │
     │     "applicability": "primary_output"                                                                                │
     │   },                                                                                                                 │
     │   "fair_profile": "Computational research with moderately FAIR practices for both data and code; interdependent      │
     │ outputs"                                                                                                             │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ "Not Applicable" vs "Not FAIR" Distinction                                                                           │
     │                                                                                                                      │
     │ Critical for fair assessment: Zero scores can mean two different things:                                             │
     │                                                                                                                      │
     │ Not Applicable (no penalty):                                                                                         │
     │ - Data-centric paper doesn't share custom code (uses standard tools)                                                 │
     │ - Code-centric paper doesn't redistribute published datasets (cites existing data)                                   │
     │ - Secondary analysis papers (re-analyse published data)                                                              │
     │                                                                                                                      │
     │ Not FAIR (penalty):                                                                                                  │
     │ - Data-centric paper generates new data but doesn't share it                                                         │
     │ - Code-centric paper develops custom code but doesn't share it                                                       │
     │ - Computational paper shares neither data nor code                                                                   │
     │                                                                                                                      │
     │ How to decide:                                                                                                       │
     │ 1. Does the paper generate/create this output type as part of its contribution? → If yes, absence = "not FAIR"       │
     │ 2. Does the paper use existing outputs created by others? → If yes, absence = "not applicable"                       │
     │                                                                                                                      │
     │ Summarising FAIR Profiles                                                                                            │
     │                                                                                                                      │
     │ Instead of single overall scores, use categorical profiles:                                                          │
     │                                                                                                                      │
     │ - "Highly FAIR (data-centric)" – data highly FAIR, code N/A                                                          │
     │ - "Highly FAIR (code-centric)" – code highly FAIR, data N/A                                                          │
     │ - "Highly FAIR (computational)" – both data and code highly FAIR                                                     │
     │ - "Moderately FAIR (data-centric)" – data moderately FAIR, code N/A or minimal                                       │
     │ - "Partially FAIR (code-centric)" – code partially FAIR, data N/A                                                    │
     │ - "Mixed FAIR (data highly, code minimal)" – divergent practices across outputs                                      │
     │ - "Not FAIR" – primary outputs score <5 when should be present                                                       │
     │                                                                                                                      │
     │ Cross-paper comparison: Compare within research types (data-centric to data-centric) rather than across types.       │
     │                                                                                                                      │
     │ 2.3 Add GitHub-Only Penalty Examples                                                                                 │
     │                                                                                                                      │
     │ Insert after: Line 628 (FAIR4RS criteria list)New subsection: "### GitHub-Only vs Archival: Scoring Impact"          │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │                                                                                                                      │
     │ Side-by-side comparison:                                                                                             │
     │                                                                                                                      │
     │ | Scenario            | F1 (Findable)             | A1 (Accessible)       | Total FAIR | Rating           |          │
     │ |---------------------|---------------------------|-----------------------|------------|------------------|          │
     │ | GitHub URL only     | 0 (not persistent)        | 0 (can disappear)     | ~4/15      | partially_fair   |          │
     │ | GitHub + Zenodo DOI | 1 (persistent identifier) | 1 (guaranteed access) | ~10/15     | moderately_fair  |          │
     │ | Impact              | +1 point                  | +1 point              | +6 points  | 150% improvement |          │
     │                                                                                                                      │
     │ Why the penalty is severe:                                                                                           │
     │ - GitHub repositories can become private, be deleted, or have content changed                                        │
     │ - GitHub URLs are not persistent identifiers (not guaranteed long-term)                                              │
     │ - FAIR F1 requires persistent, unique identifiers (DOIs, Software Heritage IDs)                                      │
     │ - FAIR A1 requires retrievable metadata even if object unavailable                                                   │
     │                                                                                                                      │
     │ Zero-effort solutions:                                                                                               │
     │ 1. Software Heritage auto-archival: GitHub repos automatically archived at https://archive.softwareheritage.org      │
     │ (free, automatic)                                                                                                    │
     │ 2. Zenodo-GitHub integration: Create DOI for release with 2 clicks                                                   │
     │ 3. figshare for code: Upload code to figshare, get DOI                                                               │
     │                                                                                                                      │
     │ Real example from test corpus:                                                                                       │
     │ - Sobotkova et al. 2024: 3 GitHub repositories without archival DOIs                                                 │
     │   - Result: code_fair_score = 4/15 (26.7%, "partially FAIR")                                                         │
     │   - If Zenodo DOIs added: Estimated code_fair_score = 10/15 (66.7%, "moderately FAIR")                               │
     │   - Single action impact: +6 points, shifts from "partially" to "moderately" FAIR                                    │
     │                                                                                                                      │
     │ Guidance for assessors: GitHub-only sharing meets "transparency" but not "FAIR" due to persistence and               │
     │ machine-actionability requirements.                                                                                  │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ File 3: Infrastructure Prompt Updates                                                                                │
     │                                                                                                                      │
     │ Location: extraction-system/prompts/06-infrastructure_pass6_prompt.md                                                │
     │                                                                                                                      │
     │ 3.1 Add Missing Statements Decision Tree                                                                             │
     │                                                                                                                      │
     │ Insert after: Line 255 (end of Step 2.8)New section: "STEP 2.9: Handling Missing or Informal Statements"             │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │                                                                                                                      │
     │ Many papers lack formal "Data Availability" or "Code Availability" statements but reference datasets or software     │
     │ informally. Use this decision tree:                                                                                  │
     │                                                                                                                      │
     │ Decision Tree:                                                                                                       │
     │                                                                                                                      │
     │ Does paper have formal "Data/Code Availability" statement?                                                           │
     │ ├─ YES → Extract verbatim, use appropriate statement_type                                                            │
     │ ├─ NO → Check paper body for dataset/software references                                                             │
     │     ├─ Dataset/software referenced but no access info                                                                │
     │     │   └─ statement_type: "implicit_reference"                                                                      │
     │     │   └─ Capture references in notes                                                                               │
     │     │   └─ datasets: [] (empty - no structured info)                                                                 │
     │     ├─ No dataset/software mentions at all                                                                           │
     │     │   └─ statement_present: false                                                                                  │
     │     │   └─ statement_type: "not_applicable"                                                                          │
     │     └─ Proprietary/unpublished data mentioned                                                                        │
     │         └─ statement_type: "restricted_access"                                                                       │
     │         └─ Note restrictions in rationale                                                                            │
     │                                                                                                                      │
     │ statement_type taxonomy:                                                                                             │
     │ - available_with_accession: Formal statement with repository accession                                               │
     │ - available_on_request: Formal statement, contact authors                                                            │
     │ - available_in_supplementary: Data/code in supplementary files                                                       │
     │ - implicit_reference: Informal mentions, no access information                                                       │
     │ - restricted_access: Access restrictions stated (ethics, commercial, privacy)                                        │
     │ - not_applicable: No relevant outputs to share                                                                       │
     │                                                                                                                      │
     │ Examples:                                                                                                            │
     │                                                                                                                      │
     │ Formal statement (Penske 2023):                                                                                      │
     │ {                                                                                                                    │
     │   "statement_present": true,                                                                                         │
     │   "statement_type": "available_with_accession",                                                                      │
     │   "verbatim_statement": "The DNA sequences reported in this paper have been deposited in the European Nucleotide     │
     │ Archive under the accession number PRJEB62503."                                                                      │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ Implicit reference (Sobotkova 2024):                                                                                 │
     │ {                                                                                                                    │
     │   "statement_present": false,                                                                                        │
     │   "statement_type": "implicit_reference",                                                                            │
     │   "verbatim_statement": "No formal data availability statement. References 'TRAP survey data from 2009-2015' and     │
     │ 'fieldwork data' but no repository or access information provided.",                                                 │
     │   "datasets": []                                                                                                     │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ Not applicable (theory paper):                                                                                       │
     │ {                                                                                                                    │
     │   "statement_present": false,                                                                                        │
     │   "statement_type": "not_applicable",                                                                                │
     │   "verbatim_statement": "No data availability statement. Theoretical paper with no empirical data collection."       │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ 3.2 Expand Ethics and Permissions Guidance                                                                           │
     │                                                                                                                      │
     │ Expand: Lines 230-237 (Section 2.6 Ethics Approval)Restructure as: "2.6 Ethics Approval and Permissions" with three  │
     │ subsections                                                                                                          │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │                                                                                                                      │
     │ Distinguish three types of research governance:                                                                      │
     │                                                                                                                      │
     │ 2.6a Ethics Committee Approval                                                                                       │
     │                                                                                                                      │
     │ Scope: Living human subjects, contemporary communities, identifiable personal data                                   │
     │                                                                                                                      │
     │ Indicators:                                                                                                          │
     │ - Institutional Review Board (IRB) approval (USA)                                                                    │
     │ - Human Research Ethics Committee (HREC) approval (Australia)                                                        │
     │ - Ethics committee protocol numbers (e.g., "Approved by University Ethics Committee #2023-45")                       │
     │ - Informed consent procedures described                                                                              │
     │                                                                                                                      │
     │ Extract: Committee name, protocol number, institution                                                                │
     │                                                                                                                      │
     │ 2.6b Institutional Permissions                                                                                       │
     │                                                                                                                      │
     │ Scope: Archaeological materials, museum collections, archival research, ancient DNA                                  │
     │                                                                                                                      │
     │ Indicators:                                                                                                          │
     │ - Permission from excavators, curators, museum directors                                                             │
     │ - Often granted via co-authorship (authorities as co-authors)                                                        │
     │ - Government permits for archaeological work                                                                         │
     │ - Museum access agreements                                                                                           │
     │                                                                                                                      │
     │ Regional variation in ancient DNA ethics:                                                                            │
     │ - Europe: Typically requires institutional permissions (excavators/curators control materials). Ethics committees    │
     │ less common unless involving contemporary descendant communities.                                                    │
     │ - Australia: Often requires formal HREC approval even for ancient remains, especially if Indigenous affiliation.     │
     │ AIATSIS Code of Ethics applies.                                                                                      │
     │ - North America: Varies by Indigenous affiliation. Native American Graves Protection and Repatriation Act (NAGPRA)   │
     │ applies to federally affiliated remains. Tribal consultation and Tribal Historical Preservation Office (THPO)        │
     │ involvement for Indigenous materials.                                                                                │
     │                                                                                                                      │
     │ Example (Penske 2023):                                                                                               │
     │ {                                                                                                                    │
     │   "permission_statements": [{                                                                                        │
     │     "type": "archaeological_samples",                                                                                │
     │     "authority": "excavators, archaeologists, curators, museum directors (as co-authors)",                           │
     │     "verbatim": "Permission to work on the archaeological samples was granted by the respective excavators,          │
     │ archaeologist and curators and museum directors of the sites, who are co-authoring the study.",                      │
     │     "regional_context": "Europe - standard institutional permissions"                                                │
     │   }]                                                                                                                 │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ 2.6c Cultural Protocols                                                                                              │
     │                                                                                                                      │
     │ Scope: Indigenous remains, descendant communities, sensitive cultural materials                                      │
     │                                                                                                                      │
     │ Indicators:                                                                                                          │
     │ - CARE principles compliance (Collective benefit, Authority to control, Responsibility, Ethics)                      │
     │ - Indigenous data sovereignty statements                                                                             │
     │ - Community consultation documented                                                                                  │
     │ - Traditional owner permissions                                                                                      │
     │ - Restrictions on data use or publication                                                                            │
     │                                                                                                                      │
     │ Cross-reference: See fieldwork-permits-guide.md for detailed CARE principles guidance                                │
     │                                                                                                                      │
     │ Example structure:                                                                                                   │
     │ {                                                                                                                    │
     │   "cultural_protocols": {                                                                                            │
     │     "care_compliant": true,                                                                                          │
     │     "indigenous_consultation": "Extensive consultation with X descendant community documented",                      │
     │     "data_sovereignty": "Data access restrictions per community agreements",                                         │
     │     "notes": "CARE principle emphasis: Authority to control (community approves analyses)"                           │
     │   }                                                                                                                  │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ 3.3 Expand ORCID Extraction Protocol                                                                                 │
     │                                                                                                                      │
     │ Expand: Lines 157-169 (Step 1, item 2)Add: Extraction protocol subsection (100 words)                                │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │                                                                                                                      │
     │ ORCID Extraction Protocol:                                                                                           │
     │                                                                                                                      │
     │ Primary source: Author affiliations section, acknowledgements, author contribution statements in PDF text            │
     │                                                                                                                      │
     │ Secondary source (optional): CrossRef metadata API query (not required for standard extraction)                      │
     │                                                                                                                      │
     │ Status values:                                                                                                       │
     │ - present_in_pdf: ORCIDs found in paper content (extract and record)                                                 │
     │ - none_found_in_extracted_text: Checked affiliations/acknowledgements but no ORCIDs present                          │
     │ - not_checked_publisher_metadata: Did not query external CrossRef/publisher APIs                                     │
     │                                                                                                                      │
     │ Important note: ORCIDs may exist in journal publisher systems (online version, CrossRef metadata) but absent from    │
     │ PDF. Record absence in PDF explicitly; do not infer "no ORCID" means author lacks one.                               │
     │                                                                                                                      │
     │ Do not: Attempt external searches for ORCIDs beyond paper content (out of scope for paper-based extraction)          │
     │                                                                                                                      │
     │ Example from test corpus:                                                                                            │
     │ - Sobotkova et al. 2024 (Journal of Documentation, 2024): orcid_status: "none_found_in_extracted_text"               │
     │ - Penske et al. 2023 (Nature, 2023): orcid_status: "none_found_in_extracted_text"                                    │
     │ - Note: Both recent papers from journals with ORCID policies, but PDFs lack ORCIDs in text                           │
     │                                                                                                                      │
     │ 3.4 Expand Supplementary Materials Protocol                                                                          │
     │                                                                                                                      │
     │ Expand: Lines 245-252 (Section 2.8)Add: Access status taxonomy (150 words)                                           │
     │                                                                                                                      │
     │ Content:                                                                                                             │
     │                                                                                                                      │
     │ Supplementary Materials Access Status Taxonomy:                                                                      │
     │                                                                                                                      │
     │ | Status                | Definition                                             | Example                           │
     │   |                                                                                                                  │
     │ |-----------------------|--------------------------------------------------------|-----------------------------------│
     │ --|                                                                                                                  │
     │ | publicly_accessible   | URL provided, freely accessible without login          | Figshare link in paper            │
     │   |                                                                                                                  │
     │ | available_on_request  | Explicit "available from authors on request" statement | Email contact provided            │
     │   |                                                                                                                  │
     │ | publisher_website     | Behind journal paywall with paper                      | Nature supplementary info         │
     │   |                                                                                                                  │
     │ | unclear               | Mentioned but no access information                    | "See supplementary videos" (no    │
     │ URL) |                                                                                                               │
     │ | mentioned_unavailable | Referenced but explicitly cannot be accessed           | "Videos no longer available"      │
     │   |                                                                                                                  │
     │                                                                                                                      │
     │ Extraction protocol:                                                                                                 │
     │ 1. Extract access information exactly as stated in paper (URL, "available from authors", "see publisher website")    │
     │ 2. Do not attempt external searches (publisher websites, institutional repositories, author websites)                │
     │ 3. Record only what paper explicitly provides                                                                        │
     │ 4. Note ambiguity in notes field when access unclear                                                                 │
     │                                                                                                                      │
     │ Example (Sobotkova 2016):                                                                                            │
     │ {                                                                                                                    │
     │   "supplementary_materials": {                                                                                       │
     │     "present": true,                                                                                                 │
     │     "description": "Supplementary videos mentioned in text",                                                         │
     │     "access_status": "unclear",                                                                                      │
     │     "repository_url": null,                                                                                          │
     │     "notes": "Videos referenced in discussion of FAIMS interface demonstration but no access information, URL, or    │
     │ repository provided. Publisher website checked - not available."                                                     │
     │   }                                                                                                                  │
     │ }                                                                                                                    │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ Cross-File Consistency                                                                                               │
     │                                                                                                                      │
     │ Ensure Consistent Terminology                                                                                        │
     │                                                                                                                      │
     │ 1. SPDX identifiers: PID systems guide examples must match FAIR principles guide R1.1 scoring (line 207)             │
     │ 2. Cross-references: Infrastructure prompt maintains reference pattern from lines 77, 110, 188                       │
     │ 3. Ethics terminology: "permission" (institutional) vs "approval" (ethics committee) vs "protocol" (cultural) used   │
     │ consistently                                                                                                         │
     │ 4. "Not applicable" category: Used consistently across all three files for outputs irrelevant to research type       │
     │                                                                                                                      │
     │ Schema Implications                                                                                                  │
     │                                                                                                                      │
     │ Note: Current extractions may have deprecated combined_fair_score fields. These should be:                           │
     │ - Left in place (backward compatibility)                                                                             │
     │ - Not emphasized in guidance                                                                                         │
     │ - Superseded by independent data_fair and code_fair categorical ratings                                              │
     │                                                                                                                      │
     │ No immediate schema migration required - new extractions use independent ratings, old extractions remain valid.      │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ Validation After Implementation                                                                                      │
     │                                                                                                                      │
     │ Recommended Checks                                                                                                   │
     │                                                                                                                      │
     │ 1. Re-extract test paper: Run Pass 6 on Penske et al. 2023 using updated prompt                                      │
     │   - Verify decision trees provide clear guidance                                                                     │
     │   - Check ethics/permissions distinction works                                                                       │
     │   - Confirm independent FAIR ratings applied correctly                                                               │
     │ 2. Check cross-references: Verify all internal links between files work                                              │
     │ 3. Test edge cases: Apply guidance to Sobotkova 2016 (book chapter, pre-FAIR)                                        │
     │   - Confirm historical context guidance sufficient                                                                   │
     │   - Verify "not applicable" used appropriately                                                                       │
     │                                                                                                                      │
     │ ---                                                                                                                  │
     │ Deliverables                                                                                                         │
     │                                                                                                                      │
     │ - Updated pid-systems-guide.md (+150-200 lines, 3 new sections)                                                      │
     │ - Updated fair-principles-guide.md (+150-200 lines, 3 major enhancements with framework-grounded independent ratings)│
     │ - Updated 06-infrastructure_pass6_prompt.md (+200-250 lines, 4 enhancements with decision trees)                     │
     │ - All changes use UK/Australian spelling                                                                             │
     │ - All changes follow markdown linting standards (MD022, MD031, MD032, MD040)                                         │
     │ - Total additions: ~500-650 lines across 3 files                                                                     │
     │                                                                                                                      │
     │ Key change from original plan: Section 2.2 now implements Independent Categorical Ratings (Option A) with framework  │
     │ precedent from ACM badges, FAIR4RS, and repliCATS, rather than explaining combined score methodology.                │
     │                                                                                                                      │
     ╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯



