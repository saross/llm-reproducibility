# Adversarial Review Framework

## Overview

The adversarial review is the key differentiator of this reproduction system. It is
conducted in Session R-C, which starts with a fresh context window — no memory of how
the reproduction was built, debugged, or executed. The reviewer sees only the artefacts
the reproduction produced. This prevents confirmation bias and mirrors independent peer
review: the reviewer's job is to be sceptical, actively searching for reasons the
reproduction might be flawed, incomplete, or misleadingly presented.

## Purpose and Independence

Adversarial review in a fresh context matters for several interconnected reasons.

**Confirmation bias is inherent in self-assessment.** The person (or session) that
performed the reproduction naturally wants it to succeed. Hours of debugging, iterating
on Dockerfiles, and coaxing scripts into running create psychological investment in a
positive outcome. This investment subtly distorts judgment — "close enough" feels
generous after a hard-won build, and ambiguous results get interpreted favourably.

**Fresh eyes catch errors that familiarity obscures.** When you have watched a value
evolve across six build iterations, you stop questioning whether the final value is
correct. A reviewer encountering the artefacts for the first time has no such
familiarity and will notice inconsistencies, gaps, and unsupported claims that the
reproducer has learned to overlook.

**Session R-C reads artefacts only.** The reviewer examines the Dockerfile, log.md,
environment.md, comparison-report.md, and all generated outputs. It has no memory of
build iterations, debugging steps, workarounds attempted, or judgment calls made during
reproduction. Every claim in the comparison report must be supported by evidence
visible in the artefacts themselves.

**This mirrors hostile peer review.** A reviewer at a journal or conference evaluates a
paper's reproducibility claims based solely on what is documented. They do not have
access to the authors' lab notebooks or debugging logs. The adversarial review session
applies exactly this standard: if it is not in the artefacts, it does not count.

## Dimension 1: Provenance Audit

**Question:** Did the reproduced values genuinely come from the Docker run?

This dimension verifies that the results presented in the comparison report were
actually produced by the containerised reproduction, not manually inserted, copied from
the published paper, or generated outside the Docker environment.

### Checks

- **Timestamp consistency**: Verify that run.log timestamps are consistent with the
  claimed execution timeline. A two-hour analysis should not have a run.log spanning
  three seconds.
- **Build log presence**: The Docker build log must be present and show a successful
  build. Missing or truncated build logs are a red flag.
- **Output file timestamps**: Output files should be timestamped after the Docker run
  began, not before. Files predating the run suggest they were not generated by it.
- **No manual insertion**: Look for evidence that values were manually inserted or
  copy-pasted from the published paper rather than extracted from genuine outputs.
- **Plausible file sizes**: Output file sizes should be plausible for their claimed
  content. A "complete analysis output" that is 47 bytes is suspicious.

### Red Flags

- Values in the comparison report that do not appear in any output file.
- "Exact match" claims for inherently stochastic analyses (e.g., Markov chain Monte
  Carlo (MCMC) sampling, bootstrapped confidence intervals) where exact replication is
  statistically implausible without fixed seeds.

### Verdict: PASS / CONCERN

## Dimension 2: Quantitative Claims Audit

**Question:** Are the numerical comparisons in the comparison report accurate?

This dimension re-examines every numerical claim, checking for transcription errors,
precision masking, and unreported values.

### For Each "EXACT MATCH" Claim

- Verify the published value was correctly transcribed from the paper (check the
  original table or figure).
- Verify the reproduced value was correctly extracted from the output file (check the
  raw output, not just the comparison report's citation of it).
- Check for transcription errors: transposed digits, wrong row or column, decimal
  point shifts.

### For Each "WITHIN HPD" Claim

- Verify the Highest Posterior Density (HPD) interval was correctly extracted from the
  paper.
- Verify the reproduced value genuinely falls within the stated interval.
- Check interval bounds: inclusive versus exclusive, one-sided versus two-sided. A
  value at the exact boundary warrants scrutiny.

### Check for Unreported Values

- Were any values from the paper skipped in the comparison?
- Were any analyses omitted without justification?
- Does the total count of compared values match expectations? (For example, if the
  paper reports 54 values across its tables, the comparison should account for all 54.)

### Check for Precision Masking

- Were values rounded to hide discrepancies? A paper reporting 0.0347 and a
  reproduction yielding 0.0391 might both round to 0.04.
- Are comparison precisions consistent with how the paper reports them? Do not reduce
  precision below the paper's own reporting standard.

### Verdict: PASS / CONCERN

## Dimension 3: Scope Completeness Audit

**Question:** What was NOT reproduced?

A reproduction that only tackles the easy parts and quietly omits the rest can appear
successful while leaving the most important claims unverified. This dimension
systematically checks for gaps.

### Enumeration Checks

- **Tables**: Enumerate all tables in the paper. Were all accounted for in the
  comparison (either reproduced or explicitly scoped out with justification)?
- **Figures**: Enumerate all figures. Were all regenerated, or explicitly documented as
  out of scope?
- **Statistical tests**: Enumerate all statistical tests or analyses. Were all
  reproduced?

### Omission Justification Review

- Are scope limitations genuinely justified? Acceptable reasons include proprietary
  software requirements, inaccessible datasets, or analyses requiring hardware not
  available in a Docker container.
- Could omitted analyses have been reproduced with available tools? An analysis omitted
  because "it uses R" when R is freely available is not justified.
- Are there "easy" analyses that were skipped without explanation?

### Selective Reproduction Check

- Were only the "easy" or "favourable" results reproduced?
- Were analyses with expected discrepancies (e.g., stochastic results, platform-
  dependent floating-point arithmetic) avoided rather than attempted and documented?

### Verdict: PASS / CONCERN

## Dimension 4: Confirmation Bias Check

**Question:** Were ambiguous results interpreted too favourably?

Even with genuine artefacts, accurate transcription, and complete scope, the final
verdict can be distorted by optimistic interpretation. This dimension scrutinises every
judgment call in the comparison report.

### Identify All Judgment Calls

Look for language indicating subjective assessment:

- **"Close enough"** — What criterion makes it close enough? Is there a defined
  tolerance, or is the assessor simply satisfied?
- **"Minor discrepancy"** — Is the discrepancy genuinely minor? Would it change a
  reader's interpretation of the paper's conclusions?
- **"Expected variation"** — Is the variation genuinely expected for this type of
  analysis? Is there a citation or statistical argument supporting this expectation?

### Re-Evaluate Each Judgment

- Could a sceptical reader interpret the same result as a failure?
- Is the verdict justified by the evidence presented, or by the desire for a positive
  outcome?
- Would a different (but equally reasonable) tolerance threshold change the verdict?

### Check Verdict Justification

- Does the final reproduction verdict follow logically from the evidence presented?
- Would the same evidence, viewed without investment in the reproduction's success,
  support a different verdict?

### Verdict: PASS / CONCERN

## Dimension 5: Methodological Soundness Audit

**Question:** Was the reproduction methodology itself sound?

Even if results match, a methodologically unsound reproduction proves nothing. This
dimension examines the integrity of the reproduction process.

### Docker Isolation

- Was Docker genuinely used for computational isolation? (A Dockerfile may be present
  but the analysis could have been run on the host system.)
- Were there host-system dependencies smuggled in via volume mounts? (For example,
  mounting a local library directory into the container.)
- Does the Dockerfile build from a well-defined base image, or does it rely on
  unversioned or mutable tags?

### Script Modifications

- Were wrapper script modifications genuinely non-algorithmic? The reproduction system
  permits wrapper scripts that handle file paths, output formatting, and execution
  orchestration, but not scripts that alter analytical logic.
- Review the wrapper script line by line: does it change any parameters, thresholds,
  model specifications, or data transformations?
- Document all modifications and assess their potential impact on results.

### Reference Verification

- Was the correct version of the paper used for comparison? (Published version, not an
  earlier preprint or draft that may report different values.)
- Were supplementary materials from the correct version? Supplementary tables
  sometimes change between preprint and publication.

### Pre-Computed Versus Fresh Results

- If pre-computed results from the original authors were used, were they independently
  verified against the paper's reported values?
- If fresh results were generated, were they compared against both the paper's reported
  values and any available pre-computed outputs?

### Data Provenance

- Was the correct dataset used? (Correct version, correct subset, correct
  preprocessing.)
- Were data transformations consistent with the paper's description?
- If data was downloaded from a repository, was the correct version or commit used?

### Verdict: PASS / CONCERN

## Overall Assessment

The overall assessment synthesises the five per-dimension verdicts into a single
reproduction confidence level.

- **CONFIRMED**: All 5 dimensions receive a PASS verdict. The reproduction is robust
  and trustworthy. An independent reviewer would have no material concerns about the
  validity of the reproduction or the accuracy of its claims.

- **QUALIFIED**: 1-2 dimensions have a CONCERN verdict. The reproduction is valid but
  carries caveats that must be documented. The concerns do not invalidate the core
  findings but represent areas where the reproduction's evidence is incomplete or where
  reasonable reviewers might disagree about interpretation.

- **CHALLENGED**: 3 or more dimensions have a CONCERN verdict. The reproduction may
  not be trustworthy. Material revision is recommended before the reproduction can be
  considered reliable evidence about the paper's computational reproducibility.

## Report Structure

The adversarial review report should follow this structure to ensure consistency and
completeness across all reviews.

```markdown
# Adversarial Review Report

## Paper: {title}

## Date: {date}

## Reviewer context: Fresh session (no reproduction context)

### Artefacts Reviewed

- List of files examined

### Dimension 1: Provenance Audit

Verdict: PASS / CONCERN
{analysis}

### Dimension 2: Quantitative Claims Audit

Verdict: PASS / CONCERN
{analysis}

### Dimension 3: Scope Completeness Audit

Verdict: PASS / CONCERN
{analysis}

### Dimension 4: Confirmation Bias Check

Verdict: PASS / CONCERN
{analysis}

### Dimension 5: Methodological Soundness Audit

Verdict: PASS / CONCERN
{analysis}

### Overall Assessment

Verdict: CONFIRMED / QUALIFIED / CHALLENGED
{synthesis}

### Recommendations

- {any recommended changes or clarifications}
```
