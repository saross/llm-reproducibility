# The Seven repliCATS Credibility Signals: HASS-Adapted Definitions

**Purpose:** Define credibility assessment signals for HASS research with approach-specific scoring anchors

**Date:** 2025-11-17

**Version:** 2.0 (with approach-specific anchors)

---

## Overview

The repliCATS project (Collaborative Assessment for Trustworthy Science) developed a structured framework for assessing research credibility through seven distinct signals. These signals are adapted here for Humanities, Arts, and Social Sciences (HASS) research, with concrete scoring anchors that vary by research approach (deductive/inductive/abductive).

**Key Innovation:** Different research approaches require different credibility criteria. A score of 75 on Transparency means different things for deductive research (data + code sharing) versus inductive research (workflow transparency). Use approach-specific anchors to avoid false equivalence.

---

## Signal 1: Comprehensibility (Clarity of Claims & Argument)

### Definition

Are claims clear, explicit, and well-structured? Can readers understand what is being claimed and on what basis?

### HASS-Specific Considerations

- Are central claims **explicit** and **bounded** (not vague or unlimited)?
- Are key domain terms defined? (e.g., "site," "landscape unit," "phase")
- Is the logical structure of the argument traceable?
- Can claims be evaluated independently?

### Assessment Questions

- Are claims stated explicitly or left implicit?
- Are scope boundaries clear (who/what/where/when)?
- Are technical terms defined in context?
- Can the logical structure be traced from evidence to claim?

### Red Flags and Green Flags

**Red Flags:**

- Vague, unbounded claims ("the landscape was intensively used")
- Undefined technical terminology
- Implicit assumptions not stated
- Logical leaps without explanation

**Green Flags:**

- Explicit, bounded claims with clear scope ("Between 1200-900 BCE, settlements in the Vardar valley concentrated within 2km of the river")
- Key terms defined
- Transparent reasoning structure
- Claims traceable to evidence

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Comprehensibility**

- Hypotheses explicitly stated and clearly bounded
- All key terms operationally defined
- Logical structure of hypothesis testing transparent
- Claims unambiguous and testable
- Reasoning from test results to conclusions clear

**60-79: Good Comprehensibility**

- Hypotheses stated (some definitional clarity may be lacking)
- Most key terms defined
- Logical structure mostly clear
- Claims understandable and evaluable
- Reasoning generally traceable

**40-59: Moderate Comprehensibility**

- Hypotheses present but may lack precision
- Some key terms defined, others implicit
- Logical structure partially obscured
- Claims understandable with effort
- Some reasoning gaps present

**20-39: Low Comprehensibility**

- Vague or implicit hypotheses
- Key terms largely undefined
- Logical structure unclear
- Claims ambiguous
- Reasoning difficult to follow

**0-19: Minimal Comprehensibility**

- No clear hypotheses
- Terms undefined
- Logical structure absent
- Claims incomprehensible or meaningless
- Reasoning not traceable

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Comprehensibility**

- Research questions and goals explicit
- Pattern descriptions clear and well-bounded
- Key terms defined
- Logical progression from observations to patterns transparent
- Scope of pattern claims clear
- *Note: Emphasis on pattern clarity rather than hypothesis precision*

**60-79: Good Comprehensibility**

- Research goals stated
- Pattern descriptions mostly clear
- Most key terms defined
- Logical progression traceable
- Scope generally clear

**40-59: Moderate Comprehensibility**

- Research goals present (may lack specificity)
- Pattern descriptions understandable (some vagueness)
- Some terms defined
- Logical progression partially clear
- Scope mentioned

**20-39: Low Comprehensibility**

- Vague research goals
- Pattern descriptions ambiguous
- Terms largely undefined
- Logical progression unclear
- Scope unclear

**0-19: Minimal Comprehensibility**

- No clear research goals
- Pattern descriptions incomprehensible
- Terms undefined
- No logical structure
- No scope boundaries

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Comprehensibility**

- Explanatory claims explicitly stated and bounded
- Theoretical framework clear
- Alternative explanations articulated
- Inference logic transparent
- Key concepts well-defined
- Scope and limitations of inference clear

**60-79: Good Comprehensibility**

- Explanatory claims stated
- Framework mostly clear
- Some alternatives mentioned
- Inference logic traceable
- Key concepts defined

**40-59: Moderate Comprehensibility**

- Explanatory claims present (may lack precision)
- Framework partially clear
- Alternatives implicit or limited
- Inference logic partially obscured
- Some concepts defined

**20-39: Low Comprehensibility**

- Vague explanatory claims
- Framework unclear
- No alternatives
- Inference logic opaque
- Concepts undefined

**0-19: Minimal Comprehensibility**

- No clear explanatory claims
- No framework
- Single interpretation with no justification
- Inference logic absent
- Concepts undefined

### Scoring Guidance

1. Identify research approach from classification
2. Use appropriate anchor set
3. Assess clarity of claims and argument structure against anchors
4. Assign score within appropriate band
5. Justify by referencing specific anchor criteria

---

## Signal 2: Transparency (Research Design & Documentation)

### Definition

Are research methods and procedures well-documented and reproducible? Can others understand and critically evaluate the research design?

### HASS-Specific Considerations

- Is research clearly **exploratory** (hypothesis-generating) vs **confirmatory** (hypothesis-testing)?
- Are sampling strategies, survey coverage, excavation strategies documented?
- Are analytical choices **explained** (not just implemented)?
- Are field forms, context sheets, and raw data accessible via repositories?
- Are there links to repository artefacts?

### Assessment Questions

- Is the research design explicitly stated?
- Are sampling decisions explained and justified?
- Can field/lab procedures be traced from documentation?
- Are analytical choices made explicit?
- Is sufficient detail provided for critical evaluation?
- Are data and materials accessible?

### Red Flags and Green Flags

**Red Flags:**

- Survey results with no coverage or intensity explanation
- Methods described but no data access
- Analytical choices not justified
- No repository links or access information
- Undocumented subjective decisions

**Green Flags:**

- Explicit design statement (exploratory/confirmatory)
- Documented sampling rationale
- Publicly accessible datasets with metadata
- Transparent analytical workflow
- Repository links that resolve

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Transparency**

- Pre-registered study design and analysis plan (or convincing explanation for absence)
- Comprehensive methods documentation with protocols
- Data and code publicly available with persistent identifiers
- All research materials accessible
- Explicit limitations and assumptions stated

**60-79: Good Transparency**

- Clear research design and hypothesis specification
- Detailed methods documentation
- Data availability clearly stated (even if embargoed)
- Code or analysis workflow documented
- Major limitations acknowledged

**40-59: Moderate Transparency**

- Research design stated (may lack detail)
- Methods described (gaps present)
- Data availability mentioned (may be vague)
- Some protocol documentation
- Limitations present (may be minimal)

**20-39: Low Transparency**

- Implicit research design
- Incomplete methods (hard to assess procedures)
- Data availability unclear or unstated
- Minimal protocol documentation
- Limitations absent or superficial

**0-19: Minimal Transparency**

- No clear research design
- Vague or absent methods
- No data/code sharing information
- No protocol documentation
- No acknowledgement of limitations

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Transparency**

- Clear documentation of exploratory goals and research questions
- Comprehensive data collection and sampling procedures
- Analysis workflow documented (how patterns identified)
- Data archived with documentation
- Explicit scope constraints and interpretation limitations
- *Note: Pre-registration not expected; emphasis on workflow transparency*

**60-79: Good Transparency**

- Research goals clearly stated
- Data collection procedures documented
- Analysis approach described
- Data accessible (or access path clear)
- Limitations acknowledged

**40-59: Moderate Transparency**

- Research goals present (may lack specificity)
- Data collection described (gaps present)
- Analysis approach mentioned (may be vague)
- Data availability partially addressed
- Some limitations noted

**20-39: Low Transparency**

- Vague research goals
- Incomplete data collection documentation
- Analysis approach unclear
- Data availability not addressed
- Minimal limitations

**0-19: Minimal Transparency**

- No clear research goals
- Data collection not documented
- Analysis process opaque
- No data sharing
- No limitations

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Transparency**

- Theoretical framework explicitly stated
- Alternative explanations considered and documented
- Evidence selection criteria transparent
- Reasoning process traceable
- Data/sources accessible
- Scope and limitations of inference clearly bounded

**60-79: Good Transparency**

- Framework stated
- Some alternative explanations considered
- Evidence criteria mentioned
- Reasoning documented
- Sources accessible

**40-59: Moderate Transparency**

- Framework implicit or partial
- Limited consideration of alternatives
- Evidence criteria unclear
- Reasoning partially documented
- Some source access

**20-39: Low Transparency**

- Vague framework
- No alternative explanations
- Evidence selection opaque
- Reasoning not documented
- Source access unclear

**0-19: Minimal Transparency**

- No theoretical framework
- Single interpretation, no alternatives
- Evidence selection arbitrary
- Reasoning not traceable
- No source access

---

## Signal 3: Plausibility (Fit with Broader Knowledge)

### Definition

Does the claim align with established prior evidence and theory? Are interpretations consistent with domain knowledge?

### HASS-Specific Considerations

- Do interpretations respect chronology, geomorphology, and regional comparanda?
- Are claims consistent with established domain knowledge?
- Are anomalies acknowledged and explained (not ignored)?
- Do interpretations require implausible auxiliary assumptions?

### Assessment Questions

- Do chronological claims align with established typologies?
- Are interpretations consistent with regional patterns?
- Are anomalies explicitly addressed?
- Does the interpretation require accepting claims that contradict established knowledge?
- Are novel claims appropriately justified?

### Red Flags and Green Flags

**Red Flags:**

- Dating inconsistent with typology, no acknowledgement
- Interpretations contradicting regional patterns without discussion
- Implausible auxiliary assumptions required
- Novel claims without sufficient justification

**Green Flags:**

- Interpretations aligned with chronological frameworks
- Regional patterns respected
- Anomalies explicitly discussed
- Novel claims well-justified with evidence

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Plausibility**

- Hypotheses grounded in established theory
- Predictions consistent with domain knowledge
- Anomalous results acknowledged and explained
- Theoretical framework coherent and well-established
- No implausible auxiliary assumptions required

**60-79: Good Plausibility**

- Hypotheses theoretically motivated
- Generally consistent with domain knowledge
- Major anomalies addressed
- Framework coherent
- Minimal implausible assumptions

**40-59: Moderate Plausibility**

- Hypotheses have theoretical basis (may be tenuous)
- Partially consistent with domain knowledge
- Some anomalies acknowledged
- Framework present but may have gaps
- Some questionable assumptions

**20-39: Low Plausibility**

- Weak theoretical grounding
- Inconsistencies with domain knowledge not addressed
- Anomalies ignored
- Framework unclear or incoherent
- Multiple questionable assumptions

**0-19: Minimal Plausibility**

- No theoretical grounding
- Contradicts established knowledge
- Anomalies ignored or denied
- No coherent framework
- Requires implausible assumptions

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Plausibility**

- Observed patterns consistent with known regional/chronological frameworks
- Typologies and classifications follow established conventions
- Anomalies acknowledged and contextualised
- Pattern interpretations grounded in comparative data
- No implausible claims required

**60-79: Good Plausibility**

- Patterns generally consistent with frameworks
- Classifications reasonable
- Major anomalies addressed
- Interpretations have comparative basis
- Minimal questionable claims

**40-59: Moderate Plausibility**

- Patterns partially consistent (some tensions)
- Classifications defensible
- Some anomalies acknowledged
- Some comparative grounding
- Some questionable elements

**20-39: Low Plausibility**

- Patterns inconsistent with frameworks
- Classifications questionable
- Anomalies not addressed
- Limited comparative grounding
- Multiple questionable claims

**0-19: Minimal Plausibility**

- Patterns contradict established knowledge
- Classifications arbitrary
- Anomalies ignored
- No comparative basis
- Requires implausible interpretations

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Plausibility**

- Proposed explanation coherent with domain knowledge
- Alternative explanations properly evaluated
- Inference grounded in established theory
- Explanatory framework robust
- No ad hoc assumptions required

**60-79: Good Plausibility**

- Explanation generally coherent
- Alternatives considered
- Theoretical grounding present
- Framework defensible
- Minimal ad hoc assumptions

**40-59: Moderate Plausibility**

- Explanation partially coherent
- Limited consideration of alternatives
- Some theoretical grounding
- Framework present but may have gaps
- Some ad hoc assumptions

**20-39: Low Plausibility**

- Explanation weakly coherent
- Alternatives not considered
- Limited theoretical grounding
- Framework unclear
- Multiple ad hoc assumptions

**0-19: Minimal Plausibility**

- Explanation incoherent
- No alternative explanations
- No theoretical grounding
- No framework
- Requires implausible assumptions

---

## Signal 4: Validity (Evidential Adequacy)

### Definition

Are methods appropriate for the research question and claims adequately supported by evidence?

### HASS-Specific Considerations

- Are data **sufficient** and **representative** for the claims made?
- Are **alternative interpretations** explicitly considered?
- Is there over-generalisation from limited data?
- Do authors acknowledge sampling limitations?
- Are rival hypotheses addressed?

### Assessment Questions

- Is evidence sufficient for the claims being made?
- Are sampling limitations acknowledged?
- Are alternative explanations considered?
- Is the scope of claims appropriately matched to the scope of evidence?
- Are there acknowledged gaps in the evidence base?

### Red Flags and Green Flags

**Red Flags:**

- Landscape-scale claims from minimal test pits
- No acknowledgement of sampling limitations
- Alternative interpretations not discussed
- Over-generalisation from limited evidence
- Evidence-claim mismatch

**Green Flags:**

- Sufficient, representative data
- Alternative interpretations considered
- Limitations clearly stated
- Claims appropriately scoped to evidence
- Gaps acknowledged

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Validity**

- Evidence directly addresses hypothesis
- Sample size and power adequate
- Methods appropriate for testing predictions
- Alternative hypotheses explicitly tested
- Confounds controlled or acknowledged
- Limitations and threats to validity stated

**60-79: Good Validity**

- Evidence addresses hypothesis
- Sample adequate
- Methods appropriate
- Some alternative hypotheses considered
- Major confounds addressed
- Limitations acknowledged

**40-59: Moderate Validity**

- Evidence partially addresses hypothesis
- Sample present but may be limited
- Methods defensible
- Limited consideration of alternatives
- Some confounds addressed
- Some limitations noted

**20-39: Low Validity**

- Evidence weakly supports hypothesis
- Sample insufficient
- Methods questionable
- Alternatives not considered
- Confounds not addressed
- Limitations minimal

**0-19: Minimal Validity**

- Evidence doesn't address hypothesis
- Sample inadequate
- Methods inappropriate
- No alternatives
- Confounds ignored
- No limitations

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Validity**

- Data sufficient and representative for pattern claims
- Sampling strategy systematic and appropriate
- Coverage adequate for generalisations made
- Alternative pattern interpretations considered
- Sampling limitations explicitly acknowledged
- Claims scoped to evidence

**60-79: Good Validity**

- Data sufficient for main patterns
- Sampling systematic
- Coverage adequate
- Some alternatives considered
- Limitations acknowledged
- Claims generally scoped appropriately

**40-59: Moderate Validity**

- Data present but may be limited
- Sampling partially systematic
- Coverage partial
- Limited alternatives
- Some limitations noted
- Some claims may exceed evidence

**20-39: Low Validity**

- Data insufficient for patterns claimed
- Sampling unsystematic or unclear
- Coverage inadequate
- No alternatives
- Limitations minimal
- Claims exceed evidence

**0-19: Minimal Validity**

- Data grossly insufficient
- No systematic sampling
- Minimal coverage
- No alternatives
- No limitations
- Major evidence-claim mismatch

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Validity**

- Evidence supports proposed explanation
- Alternative explanations rigorously evaluated
- Inference well-grounded in available evidence
- Rival explanations ruled out or ranked
- Evidence gaps acknowledged
- Scope of inference matched to evidence

**60-79: Good Validity**

- Evidence supports explanation
- Alternatives considered
- Inference grounded in evidence
- Some rivals addressed
- Gaps acknowledged
- Inference scope appropriate

**40-59: Moderate Validity**

- Evidence partially supports explanation
- Limited alternatives considered
- Inference has some grounding
- Few rivals addressed
- Some gaps noted
- Inference scope may exceed evidence

**20-39: Low Validity**

- Evidence weakly supports explanation
- Alternatives not seriously considered
- Inference poorly grounded
- Rivals ignored
- Gaps not acknowledged
- Inference exceeds evidence

**0-19: Minimal Validity**

- Evidence doesn't support explanation
- No alternatives
- Inference ungrounded
- Rivals not addressed
- Gaps ignored
- Major inference-evidence mismatch

---

## Signal 5: Robustness (Sensitivity to Analytical Choices)

### Definition

Would results hold under different reasonable analytical approaches? Are conclusions sensitive to specific methodological choices?

### HASS-Specific Considerations

- Would main inferences survive reasonable alternative analytical processing?
- Are there sensitivity analyses showing results aren't artefacts of specific choices?
- How dependent are results on hand-curated or subjective steps?
- Are key analytical decisions justified?
- Would different reasonable choices lead to different conclusions?

### Assessment Questions

- Are sensitivity analyses performed?
- How dependent are results on specific analytical choices?
- Are alternative analytical approaches explored?
- Could different reasonable choices lead to different conclusions?
- Are manual curation steps documented and justified?

### Red Flags and Green Flags

**Red Flags:**

- Single analytical approach, no alternatives tested
- Results heavily dependent on subjective choices
- No sensitivity testing
- Manual curation undocumented
- Analytical choices unjustified

**Green Flags:**

- Multiple analytical approaches tested
- Sensitivity analyses performed
- Results robust across variations
- Manual steps documented
- Analytical choices justified

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Robustness**

- Sensitivity analyses performed (varying parameters, methods)
- Results robust across reasonable analytical choices
- Alternative statistical approaches tested
- Assumptions validated or tested
- Dependencies on choices clearly documented
- Robustness checks reported

**60-79: Good Robustness**

- Some sensitivity analysis
- Results appear robust (limited testing)
- Some alternative approaches tested
- Assumptions mostly tested
- Dependencies documented
- Some robustness evidence

**40-59: Moderate Robustness**

- Limited sensitivity analysis
- Robustness unclear
- Few alternatives tested
- Assumptions stated but not tested
- Some dependencies noted
- Minimal robustness evidence

**20-39: Low Robustness**

- No sensitivity analysis
- Robustness unknown
- Single approach only
- Assumptions untested
- Dependencies not documented
- No robustness evidence

**0-19: Minimal Robustness**

- No analysis of sensitivity
- Results likely fragile
- Arbitrary analytical choices
- Assumptions violated
- Critical dependencies hidden
- Evidence of non-robustness

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Robustness**

- Pattern identification tested with multiple methods
- Results consistent across reasonable classification schemes
- Inter-observer reliability assessed (if subjective coding)
- Alternative analytical frameworks explored
- Sensitivity to sampling documented
- Convergent evidence from multiple indicators

**60-79: Good Robustness**

- Some methodological triangulation
- Main patterns robust to reasonable variations
- Some reliability assessment
- Some alternatives explored
- Sampling sensitivity considered
- Some convergent evidence

**40-59: Moderate Robustness**

- Limited triangulation
- Pattern robustness unclear
- Minimal reliability assessment
- Few alternatives
- Sampling sensitivity mentioned
- Limited convergent evidence

**20-39: Low Robustness**

- No triangulation
- Single method only
- No reliability assessment
- No alternatives
- Sampling sensitivity ignored
- No convergent evidence

**0-19: Minimal Robustness**

- Arbitrary methods
- Results likely artefactual
- No reliability consideration
- Method-dependent conclusions
- Sampling biases ignored
- Contradictory evidence ignored

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Robustness**

- Alternative explanations rigorously tested
- Inference robust across reasonable interpretive frameworks
- Evidence triangulation from multiple sources
- Theoretical assumptions tested or varied
- Inference stability assessed
- Sensitivity to framework choices documented

**60-79: Good Robustness**

- Alternatives considered
- Inference appears robust
- Some triangulation
- Assumptions considered
- Some stability evidence
- Framework sensitivity noted

**40-59: Moderate Robustness**

- Limited alternatives
- Robustness unclear
- Minimal triangulation
- Assumptions stated
- Stability unclear
- Framework sensitivity minimal

**20-39: Low Robustness**

- No alternatives
- Single framework only
- No triangulation
- Assumptions untested
- Stability unknown
- Framework dependency hidden

**0-19: Minimal Robustness**

- No alternative explanations
- Framework-dependent conclusions
- Evidence cherry-picking
- Assumptions violated
- Inference unstable
- Critical framework dependencies ignored

---

## Signal 6: Replicability (Analytic Reproducibility)

### Definition

Can others reproduce the **analytical outputs** given the same inputs? (Note: For HASS, this means analytic reproducibility, NOT field replication of non-repeatable phenomena)

### HASS-Specific Considerations

**Critical adaptation for HASS:** Replicability = Analytic Reproducibility, NOT field replication

- Can others reproduce the **analytical outputs** given the same inputs?
- Are data and code available and complete?
- Are computational workflows documented?
- Can analytical steps be traced and rerun?

**Important:** This does NOT mean "can you re-excavate the site" (impossible) â€” it means "given the excavation data, can you reproduce the analysis?"

### Assessment Questions

- Are raw data publicly accessible (or appropriately restricted with justification)?
- Is analysis code available and documented?
- Can analytical workflows be traced step-by-step?
- Are data and code complete (not just fragments)?
- If access is restricted, is governance framework explained?

### Red Flags and Green Flags

**Red Flags:**

- Statistical analysis with no code or raw data
- Data "available on request" (rarely fulfilled)
- Code provided but undocumented
- Incomplete data or code fragments
- Undocumented restrictions

**Green Flags:**

- Complete data + documented code in stable repository
- Clear workflow documentation
- Persistent identifiers (DOIs)
- FAIR/CARE compliance where appropriate
- Justified, transparent restrictions with governance

### Special Considerations

- **Indigenous/Community Data:** System does NOT penalise appropriate restrictions if aligned with CARE principles
- **Sensitive Archaeological Data:** Legitimate restrictions (site protection) should be explained, not hidden

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Replicability**

- Complete raw data publicly available with persistent identifiers
- All analysis code shared with documentation
- Computational environment specified (versions, dependencies)
- Workflow fully documented and executable
- Outputs completely reproducible
- FAIR principles met

**60-79: Good Replicability**

- Data available (may have minor gaps)
- Code shared with basic documentation
- Workflow documented
- Most outputs reproducible
- FAIR principles mostly met

**40-59: Moderate Replicability**

- Data partially available
- Some code shared (may be incomplete)
- Workflow partially documented
- Some outputs reproducible
- Partial FAIR compliance

**20-39: Low Replicability**

- Minimal data sharing
- Fragmentary code or no code
- Workflow poorly documented
- Outputs difficult to reproduce
- Limited FAIR compliance

**0-19: Minimal Replicability**

- No data sharing
- No code
- No workflow documentation
- Outputs not reproducible
- No FAIR compliance

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Replicability**

- Data archived with comprehensive documentation
- Analysis workflow documented (even if not fully automated)
- Classification schemes and coding procedures explicit
- Raw observations accessible
- Metadata complete
- FAIR/CARE principles met (where appropriate)
- *Note: Full code automation not required; workflow transparency is key*

**60-79: Good Replicability**

- Data archived with documentation
- Workflow documented
- Procedures explicit
- Observations accessible
- Metadata adequate
- FAIR/CARE principles mostly met

**40-59: Moderate Replicability**

- Data partially archived
- Workflow partially documented
- Procedures stated
- Some observations accessible
- Metadata present
- Partial FAIR/CARE compliance

**20-39: Low Replicability**

- Minimal data archiving
- Workflow poorly documented
- Procedures vague
- Limited observation access
- Metadata minimal
- Limited FAIR/CARE compliance

**0-19: Minimal Replicability**

- No data archiving
- No workflow documentation
- Procedures opaque
- No observation access
- No metadata
- No FAIR/CARE compliance

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Replicability**

- Evidence sources fully documented and accessible
- Reasoning process explicitly traceable
- Theoretical framework clearly specified
- Alternative interpretations documented
- Interpretive decisions explained
- Source materials archived
- *Note: "Replication" means others can trace reasoning, not necessarily reach same conclusion*

**60-79: Good Replicability**

- Sources documented and mostly accessible
- Reasoning process traceable
- Framework specified
- Interpretations documented
- Decisions explained
- Materials mostly archived

**40-59: Moderate Replicability**

- Sources partially documented
- Reasoning partially traceable
- Framework present
- Some interpretations documented
- Some decisions explained
- Partial archiving

**20-39: Low Replicability**

- Minimal source documentation
- Reasoning difficult to trace
- Framework vague
- Interpretations not documented
- Decisions unexplained
- Minimal archiving

**0-19: Minimal Replicability**

- No source documentation
- Reasoning not traceable
- No framework
- Interpretations opaque
- Decisions hidden
- No archiving

---

## Signal 7: Generalisability (Scope & Limitations)

### Definition

Can findings transfer to other contexts? Are claims carefully constrained by place, time, and context? Are limitations explicitly acknowledged?

### HASS-Specific Considerations

- Are claims carefully **constrained** by place, time, and context?
- Do authors **explicitly articulate limits** to generalisation?
- Are scope boundaries appropriate for the evidence?
- Are limitations acknowledged rather than ignored?
- Is over-generalisation avoided?

### Assessment Questions

- Are claims appropriately bounded by context (geographic, temporal, cultural)?
- Are limitations explicitly stated?
- Is the scope of generalisation matched to the scope of evidence?
- Are extrapolations appropriately qualified?
- Are there unjustified leaps from local to regional/universal claims?

### Red Flags and Green Flags

**Red Flags:**

- Study of one valley making unqualified claims about "Bronze Age settlement patterns"
- Regional patterns with unclear temporal/cultural boundaries
- Extrapolations without qualification
- Limitations absent or superficial
- Unjustified universal claims

**Green Flags:**

- Claims carefully bounded by geography, chronology, context
- Explicit limitation statements
- Appropriate constraint on generalisations
- Qualified extrapolations
- Scope matched to evidence

### Approach-Specific Scoring Anchors (0-100 Scale)

#### For Deductive Research (Hypothesis-Testing)

**80-100: Excellent Generalisability**

- Scope of hypothesis clearly bounded
- Limitations to external validity explicitly stated
- Population, context, and temporal bounds clear
- Extrapolations appropriately qualified
- Threats to generalisation discussed
- Transfer conditions specified

**60-79: Good Generalisability**

- Hypothesis scope stated
- Main limitations acknowledged
- Bounds mostly clear
- Extrapolations qualified
- Some generalisability discussion
- Transfer considerations present

**40-59: Moderate Generalisability**

- Scope present but may be vague
- Some limitations stated
- Bounds partially clear
- Some qualification of claims
- Limited generalisability discussion
- Minimal transfer consideration

**20-39: Low Generalisability**

- Vague scope
- Minimal limitations
- Bounds unclear
- Unqualified claims
- No generalisability discussion
- No transfer consideration

**0-19: Minimal Generalisability**

- No scope boundaries
- No limitations
- Unbounded claims
- Universal claims from limited context
- No generalisability consideration
- Unjustified extrapolations

#### For Inductive Research (Exploratory, Pattern-Finding)

**80-100: Excellent Generalisability**

- Pattern claims explicitly bounded (geographic, temporal, cultural)
- Sampling limitations thoroughly discussed
- Scope appropriately matched to coverage
- Extrapolations carefully qualified
- Regional/contextual constraints clear
- Limitation statements prominent
- *Note: Emphasis on appropriate scope constraint, not necessarily broad applicability*

**60-79: Good Generalisability**

- Pattern claims bounded
- Sampling limitations acknowledged
- Scope generally matched to coverage
- Extrapolations qualified
- Constraints stated
- Limitations present

**40-59: Moderate Generalisability**

- Claims partially bounded
- Some sampling limitations noted
- Scope partially matched
- Some qualification
- Some constraints stated
- Some limitations

**20-39: Low Generalisability**

- Claims weakly bounded
- Sampling limitations minimal
- Scope exceeds coverage
- Limited qualification
- Vague constraints
- Minimal limitations

**0-19: Minimal Generalisability**

- Claims unbounded
- No sampling limitations
- Scope greatly exceeds coverage
- No qualification
- No constraints
- No limitations

#### For Abductive Research (Inference to Best Explanation)

**80-100: Excellent Generalisability**

- Explanatory scope clearly bounded
- Contextual constraints explicit
- Transfer conditions specified
- Limitations of inference acknowledged
- Domain of applicability clear
- Alternative contexts discussed

**60-79: Good Generalisability**

- Explanation scope stated
- Constraints present
- Transfer considerations mentioned
- Inference limitations acknowledged
- Applicability domain stated
- Some context discussion

**40-59: Moderate Generalisability**

- Scope partially bounded
- Some constraints
- Limited transfer discussion
- Some inference limitations
- Domain partially clear
- Minimal context discussion

**20-39: Low Generalisability**

- Vague scope
- Minimal constraints
- No transfer discussion
- Inference limitations minimal
- Domain unclear
- No context discussion

**0-19: Minimal Generalisability**

- Unbounded explanatory claims
- No constraints
- Universal claims from single context
- No inference limitations
- No domain specification
- Context ignored

---

## Using Approach-Specific Anchors

### Scoring Procedure

1. **Identify research approach** from classification.json (deductive/inductive/abductive)
2. **Select appropriate anchor set** for that approach
3. **Assess paper against anchor descriptions** (not universal criteria)
4. **Assign score within appropriate band** (0-19, 20-39, 40-59, 60-79, 80-100)
5. **Justify score by referencing specific anchor criteria** met or missed

### Example Justification Template

> "[Signal Name] scored [X] ([Band Description] for [approach] research). This paper [meets criteria A, B, C required for band], but [lacks criterion D which would move to higher band]. [Approach-specific note explaining why certain criteria apply or don't apply]."

**Example:**

> "Transparency scored 75 (Good Transparency band for inductive research). This paper clearly documents exploratory goals and data collection procedures (required for 60-79), has archived data (60-79), but lacks detailed analysis workflow documentation (would move to 80-100). Pre-registration not expected for inductive work, so absence does not lower score."

### Critical Note: Cross-Approach Comparison

**A score of 75 means different things for different approaches:**

- 75 for deductive research implies data + code sharing, hypothesis specification
- 75 for inductive research implies workflow transparency, sampling documentation
- 75 for abductive research implies framework clarity, alternative explanations

**Do not compare scores across approaches numerically.** Instead, compare against anchor descriptions. The anchors define what constitutes "good" or "excellent" work within each research tradition.

---

## Related References

- `approach-taxonomy.md` - Research approach definitions (deductive/inductive/abductive)
- `assessment-frameworks.md` - Signal emphasis by research approach
- `track-a-quality-criteria.md` - Quality gating for credibility assessment
