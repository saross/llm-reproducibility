{
  "schema_version": "2.5",
  "extraction_timestamp": "2025-10-23T20:50:39.700385+00:00",
  "extractor": "Claude Sonnet 4.5",
  "paper_metadata": {
    "title": "Creating large, high-quality geospatial datasets from historical maps using novice volunteers",
    "authors": [
      "Adela Sobotkova",
      "Shawn A. Ross",
      "Christian Nassif-Haynes",
      "Brian Ballsun-Stanton"
    ],
    "year": 2023,
    "doi": "10.1016/j.apgeog.2023.102967",
    "journal": "Applied Geography"
  },
  "evidence": [
    {
      "evidence_id": "E001",
      "evidence_text": "FAIMS Mobile was used to digitise 10,827 mound features",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "FAIMS Mobile was used to digitise 10,827 mound features from Soviet military topographic maps",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_count",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C001",
        "C002",
        "C011"
      ],
      "related_evidence": [
        "E002",
        "E003",
        "E004"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E002",
      "evidence_text": "Digitisation required 241 person-hours total",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "This digitisation required 241 person-hours",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C002",
        "C003",
        "C012"
      ],
      "related_evidence": [
        "E001",
        "E003"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E003",
      "evidence_text": "57 person-hours from staff, 184 from novice volunteers",
      "evidence_type": "quantitative_resource_breakdown",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "57 from staff; 184 from novice volunteers",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C003",
        "C017"
      ],
      "related_evidence": [
        "E002"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E004",
      "evidence_text": "Error rate under 6%",
      "evidence_type": "quantitative_quality",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "with an error rate under 6%",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "bounded_range",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "type": "bounded_range",
        "indicator": "under",
        "quantification": "<6%",
        "author_explanation": null,
        "severity": "minor"
      },
      "supports_claims": [
        "C004",
        "C011"
      ],
      "related_evidence": [
        "E001"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E005",
      "evidence_text": "Quality assurance check covering 7% of digitised features found 49 errors from 834 features, yielding 5.87% error rate",
      "evidence_type": "quantitative_quality",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "found 49 errors from a true count of 834 features, a 5.87% error rate",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "sample_check"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C004",
        "C040"
      ],
      "related_evidence": [
        "E004"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E006",
      "evidence_text": "Error breakdown from QA check: 42 false negatives (missed symbols), 6 double-marked features, 1 classification error, and no false positives",
      "evidence_type": "error_breakdown",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Forty-two of these errors were false negatives (symbols missed by students). Six were double-marked (Student C digitised a section of a map twice). Students made only one classification error (a similar symbol mistaken for a benchmark), and no outright false positives",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "sample_check"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C040",
        "C041"
      ],
      "related_evidence": [
        "E005"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E006",
          "P1_E007",
          "P1_E008"
        ],
        "consolidation_type": "profile_consolidation",
        "information_preserved": "complete",
        "granularity_available": "Individual error types available in source: false negatives (42), double-marks (6), classification errors (1), false positives (0)",
        "rationale": "All error types from same QA check form complete error profile assessed together"
      }
    },
    {
      "evidence_id": "E009",
      "evidence_text": "Individual student error rates ranged from 1.3% to 10.6%",
      "evidence_type": "quantitative_quality_range",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Students' individual error rates ranged from 1.3% to 10.6%",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "per_student_breakdown"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E005"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E010",
      "evidence_text": "Two fastest digitisers (44-45s per feature) had lowest error rates (1.3% and 2.9%)",
      "evidence_type": "comparative_performance",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Note that the two fastest digitisers (Students A and B; 44 and 45 s per feature respectively) also had the lowest error rates (1.3 and 2.9%)",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "per_student_breakdown"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E009"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E011",
      "evidence_text": "Two slowest digitisers (61-73s per feature) had highest error rates (10.6% and 7.4%)",
      "evidence_type": "comparative_performance",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "while the two slowest (Students C and D; 61 and 73 s) had the highest error rates (10.6 and 7.4%)",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "per_student_breakdown"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E010"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E012",
      "evidence_text": "35 of 49 false negatives resulted from one student failing to digitise three contiguous map sections",
      "evidence_type": "error_pattern",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "35 of the 49 false negatives were the result of Student C failing to digitise three contiguous sections of an assigned map",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "error_pattern_analysis"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C041"
      ],
      "related_evidence": [
        "E006"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E013",
      "evidence_text": "Excluding Student C outlier would reduce cumulative error rate from 5.9% to 2.8%",
      "evidence_type": "adjusted_error_calculation",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "excluding Student C would have cut the cumulative error rate in half to 2.8%",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "adjusted_calculation"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [
        "E012"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E014",
      "evidence_text": "Recoverable data omissions totaled 223 records (2.06%)",
      "evidence_type": "data_quality_issue",
      "evidence_basis": "quality_assurance_check",
      "verbatim_quote": "Recoverable data omissions across both years totaled 223 (2.06% of records)",
      "location": {
        "section": "Results",
        "subsection": "3.5.1. Recoverable data omissions and incomplete records",
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [],
      "related_evidence": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E015",
      "evidence_text": "Project staff could digitise at 60-75 features per staff-hour using desktop GIS",
      "evidence_type": "comparative_performance_benchmark",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "project staff with desktop GIS experience could digitise at a sustained rate of 60 – 75 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "bounded_range",
        "sample_representativeness": "staff_performance"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C050",
        "C051"
      ],
      "related_evidence": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E016",
      "evidence_text": "57 hours of staff time for crowdsourcing system yielded 10,827 features (190 features per staff-hour)",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_calculation",
      "verbatim_quote": "the 57 h of staff time required for our digitisation approach using a customisation of FAIMS Mobile produced 10,827 features, or about 190 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "calculated",
        "sample_representativeness": "complete_project"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C052",
        "C053"
      ],
      "related_evidence": [
        "E003"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E017",
      "evidence_text": "21 internal staff hours represents over 500 features per staff-hour",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_calculation",
      "verbatim_quote": "Those 21 internal staff hours represent a digitisation rate of over 500 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1. Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "bounded_range",
        "sample_representativeness": "internal_staff_subset"
      },
      "declared_uncertainty": {
        "type": "bounded_range",
        "indicator": "over",
        "quantification": ">500",
        "author_explanation": null,
        "severity": "minor"
      },
      "supports_claims": [
        "C054"
      ],
      "related_evidence": [
        "E016"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E018",
      "evidence_text": "Project staff digitise at sustained rate of 60-75 features per staff-hour; at this rate, 57h of staff time could produce 3,420-4,275 staff-digitised features",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Project staff can digitise mounds at a sustained rate of 60 – 75 features per staff-hour. At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420 – 4,275 staff-digitised features",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "range_estimate",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": "60-75 features per staff-hour",
        "hedging_language": null
      },
      "supports_claims": [
        "C052",
        "C051",
        "C064",
        "C053"
      ],
      "related_evidence": [
        "E020"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E018",
          "P1_E019"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "Rate (60-75/h) and projection (3,420-4,275 features) both preserved",
        "rationale": "Rate and projection assessed together as staff capability; both needed for comparative analysis"
      }
    },
    {
      "evidence_id": "E020",
      "evidence_text": "2010 digitisation rate with volunteers using desktop GIS: 130-180 features per staff-hour of support; 57h with this approach might have produced 7,410-10,260 features",
      "evidence_type": "comparative_performance_benchmark",
      "evidence_basis": "prior_project_data",
      "verbatim_quote": "In 2010, we attempted to use desktop GIS to crowdsource digitisation but found volunteers required extensive and continuous staff support, resulting in a de facto production rate of 130 – 180 features per staff-hour (of desktop GIS support and troubleshooting). This compares to 190 features per staff-hour for the FAIMS Mobile approach and suggests that using volunteers with desktop GIS might have produced 7,410 – 10,260 features for 57 h, yet at a much higher administrative cost",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "range_estimate",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": "130-180 features per staff-hour",
        "hedging_language": null
      },
      "supports_claims": [
        "C056",
        "C064",
        "C055"
      ],
      "related_evidence": [
        "E018",
        "E022"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E020",
          "P1_E021"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "Rate (130-180/h) and projection (7,410-10,260 features) both preserved",
        "rationale": "Rate and projection assessed together as desktop GIS approach capability"
      }
    },
    {
      "evidence_id": "E022",
      "evidence_text": "FAIMS Mobile approach: 190 features per staff-hour",
      "evidence_type": "quantitative_performance",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "the 57 h of staff time required for our digitisation approach using a customisation of FAIMS Mobile produced 10,827 features, or about 190 features per staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "approximate_calculation",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C057",
        "C064"
      ],
      "related_evidence": [
        "E001",
        "E002"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E023",
      "evidence_text": "Of 57h total, only 21h came from internal project staff (36h from student programmer); 21 internal staff hours represent over 500 features per staff-hour, and would yield 1,260-1,575 features if staff digitised directly, or 2,730-3,780 if supervising desktop GIS volunteers",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Only 21 of those 57 h came from the project's in ternal archaeologists, because a programming student donated 36 h to app configuration and customisation (at a cost of ca. AUD $2,000). This 21 h of internal staff time was sufficient to crowdsource production of 10,827 features, a rate of over 500 features per staff-hour. By way of comparison, dedicating those 21 h to staff digitisation would have yielded 1,260 – 1,575 features, while using them to supervise a desktop-GIS crowdsourcing approach would have produced 2,730 – 3,780 features",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C058",
        "C059",
        "C064"
      ],
      "related_evidence": [
        "E022",
        "E024"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E023",
          "P1_E025",
          "P1_E026"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "21h internal staff, 500+ features/h rate, comparative projections all preserved",
        "rationale": "Internal staff time breakdown with all comparative projections assessed together"
      }
    },
    {
      "evidence_id": "E024",
      "evidence_text": "Student programmer cost: approximately AUD $2,000",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "completed by a student programmer for a modest cost (ca. AUD $2,000)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "approximate_value",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "ca."
      },
      "supports_claims": [
        "C058"
      ],
      "related_evidence": [
        "E023"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E027",
      "evidence_text": "In-field support for volunteers was only 7h across two seasons (about 1,550 features per in-field staff-hour); 7h would allow staff to directly digitise 420-525 features, or supervise desktop GIS digitisation of 910-1,260 features",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "It is also worth considering how the project used its most-precious resource: staff time in the field, which totalled just 7 h across two field seasons. This time was spent supporting volunteers (troubleshooting), demonstrating the system, and locating maps that corresponded to digitisation targets. This in-field supervision represented about 1,550 features per in-field staff-hour. If dedicated to direct staff digitisation, those 7 h would have yielded about 420 – 525 features, or 910 – 1,260 features if used to support desktop GIS digitisation by students",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 6
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C064",
        "C060",
        "C061"
      ],
      "related_evidence": [
        "E023",
        "E030"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E027",
          "P1_E028",
          "P1_E029"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "7h total, 1,550 features/h rate, comparative projections all preserved",
        "rationale": "In-field support time with all comparative projections assessed together"
      }
    },
    {
      "evidence_id": "E030",
      "evidence_text": "In-field support and quality assurance totalled 13h; marginal cost per additional feature: 4.3 seconds of staff support",
      "evidence_type": "efficiency_calculation",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Combining the 7 h of in-field support and 6 h of quality assurance, the ' marginal cost ' to produce an additional feature was 4.3 s of staff support (13 h divided by 10,827 features)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C062"
      ],
      "related_evidence": [
        "E027",
        "E032"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_E030",
          "P1_E031"
        ],
        "consolidation_type": "compound_finding",
        "information_preserved": "complete",
        "granularity_available": "13h total (7h field + 6h QA), 4.3s marginal cost per feature",
        "rationale": "Support time total and per-feature metric assessed together as system efficiency"
      }
    },
    {
      "evidence_id": "E032",
      "evidence_text": "Map preparation: 6 minutes per map (6h for 58 maps)",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Preparing and distributing additional maps took only 6 min per map (6 h for 58 maps)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C063"
      ],
      "related_evidence": [
        "E033"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E033",
      "evidence_text": "2018 redeployment required only 1h additional setup time",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_observation",
      "verbatim_quote": "Even adding another field season only costs one additional hour of setup time (based on our 2018 redeployment)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C063"
      ],
      "related_evidence": [
        "E032"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E034",
      "evidence_text": "Urban Occupations Project: 1,250h manual digitisation for training data",
      "evidence_type": "quantitative_benchmark",
      "evidence_basis": "literature_source",
      "verbatim_quote": "This project reported 1,250 h of manual digitisation to create enough training data to classify roads visible in historical maps of the Ottoman Empire",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "secondary_source",
        "measurement_precision": "exact_count",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C067",
        "C068"
      ],
      "related_evidence": [
        "E035",
        "E036"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E035",
      "evidence_text": "ML expert spent 7 days testing and fine-tuning the model",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "literature_source",
      "verbatim_quote": "an ML expert spent seven days testing and fine tuning the model",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "secondary_source",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C067",
        "C068"
      ],
      "related_evidence": [
        "E034",
        "E036"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E036",
      "evidence_text": "ML output: 300,000 km of roads digitised",
      "evidence_type": "quantitative_outcome",
      "evidence_basis": "literature_source",
      "verbatim_quote": "The output was impressive: some 300,000 km of roads were digitised",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "secondary_source",
        "measurement_precision": "approximate_value",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "some"
      },
      "supports_claims": [
        "C067"
      ],
      "related_evidence": [
        "E034",
        "E035"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E037",
      "evidence_text": "ML approach required minimum of about 1,300h preparation time",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "which appears to have required a minimum of about 1,300 h of preparation time alone",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_calculation",
        "sample_representativeness": "external_benchmark"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C068"
      ],
      "related_evidence": [
        "E034",
        "E035"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E038",
      "evidence_text": "Project time breakdown: 44 staff hours customising, 184 participant-hours digitising, 7 staff-hours supporting, 6 staff hours checking = 241h total",
      "evidence_type": "quantitative_resource_breakdown",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "We spent 44 staff hours customising and deploying a streamlined geospatial system in FAIMS Mobile, 184 participant-hours digitising features, seven staff-hours directly supporting that digitisation, and six staff hours checking for errors. These 241 h produced a dataset of 10,827 features",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_measurement",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C069"
      ],
      "related_evidence": [
        "E001",
        "E002",
        "E039"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E039",
      "evidence_text": "241h produced 10,827 features at rate of 44.9 features per person-hour",
      "evidence_type": "quantitative_performance",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "These 241 h produced a dataset of 10,827 features, a rate of 44.9 features/person-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "precise_calculation",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": null,
      "supports_claims": [
        "C069"
      ],
      "related_evidence": [
        "E038"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E040",
      "evidence_text": "At 44.9 features/person-hour rate, 1,300h would yield about 58,400 records",
      "evidence_type": "quantitative_calculation",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "At that rate, the 1,300 h it took to deploy the ML approach taken by Can, Gerrits, and Kabadayi would yield about 58,400 records",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_calculation",
        "sample_representativeness": "comparative_estimate"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C070"
      ],
      "related_evidence": [
        "E037",
        "E039"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E041",
      "evidence_text": "2% of records had recoverable data omissions",
      "evidence_type": "quantitative_error",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "Some 2% of records had recoverable data omissions which were corrected during post-processing",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "exact_percentage",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "Some"
      },
      "supports_claims": [
        "C090",
        "C091"
      ],
      "related_evidence": [
        "E042"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E042",
      "evidence_text": "Accuracy check covering 7% of digitised features indicated error rate under 6%",
      "evidence_type": "quantitative_error",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "An accuracy check by staff covering 7% of digitised features indicated an error rate of under 6%",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "threshold_estimate",
        "sample_representativeness": "sample_based"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "under"
      },
      "supports_claims": [
        "C090",
        "C091",
        "C092"
      ],
      "related_evidence": [
        "E041"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E043",
      "evidence_text": "Data ready for analysis with less than 2h processing after collection",
      "evidence_type": "quantitative_resource",
      "evidence_basis": "direct_measurement",
      "verbatim_quote": "a comprehensive, FAIR-compliant dataset was ready for analysis with less than 2 h of processing after collection",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "evidence_strength": {
        "source_reliability": "primary_data",
        "measurement_precision": "threshold_estimate",
        "sample_representativeness": "complete_dataset"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "less than"
      },
      "supports_claims": [
        "C089"
      ],
      "related_evidence": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E044",
      "evidence_text": "Payoff threshold: 4,500 features vs direct digitisation by staff",
      "evidence_type": "quantitative_threshold",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "our approach becomes worthwhile for datasets no larger than about 4,500 features versus direct digitisation of features by expert staff",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_threshold",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C093"
      ],
      "related_evidence": [
        "E045",
        "E046"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E045",
      "evidence_text": "Payoff threshold: 10,000 features vs volunteer digitisation using desktop GIS",
      "evidence_type": "quantitative_threshold",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "no larger than about 10,000 features versus volunteer digitisation using desktop GIS",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "approximate_threshold",
        "sample_representativeness": "project_specific"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "about"
      },
      "supports_claims": [
        "C093"
      ],
      "related_evidence": [
        "E044",
        "E046"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "evidence_id": "E046",
      "evidence_text": "Most efficient approach for datasets up to at least 60,000 features",
      "evidence_type": "quantitative_threshold",
      "evidence_basis": "derived_calculation",
      "verbatim_quote": "It remains the most efficient approach for datasets up to at least 60,000 features, above which automated approaches like ML should be considered",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "evidence_strength": {
        "source_reliability": "derived_calculation",
        "measurement_precision": "threshold_estimate",
        "sample_representativeness": "comparative_estimate"
      },
      "declared_uncertainty": {
        "explicit_range": null,
        "hedging_language": "at least"
      },
      "supports_claims": [
        "C093"
      ],
      "related_evidence": [
        "E044",
        "E045"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    }
  ],
  "claims": [
    {
      "claim_id": "C001",
      "claim_text": "The crowdsourcing approach successfully digitised a large number of burial mound features",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "FAIMS Mobile was used to digitise 10,827 mound features from Soviet military topographic maps",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E001"
      ],
      "supports_claims": [
        "C011"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "10,827 features"
        ],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C002",
      "claim_text": "The digitisation effort required a moderate amount of person-hours",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "This digitisation required 241 person-hours",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E002"
      ],
      "supports_claims": [
        "C003",
        "C012"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "241 person-hours"
        ],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C003",
      "claim_text": "Novice volunteers contributed the majority of person-hours",
      "claim_type": "outcome",
      "claim_role": "supporting",
      "claim_status": "explicit",
      "verbatim_quote": "57 from staff; 184 from novice volunteers",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E003"
      ],
      "supports_claims": [
        "C017"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "57 staff hours",
          "184 volunteer hours"
        ],
        "comparative_framing": "volunteers > staff"
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C004",
      "claim_text": "The digitisation achieved high accuracy",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "with an error rate under 6%",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E004",
        "E005"
      ],
      "supports_claims": [
        "C011"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "<6% error rate"
        ],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C011",
      "claim_text": "The approach achieved unanticipated success despite minimal resourcing",
      "claim_type": "synthesis",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "The value of this approach lies in the unanticipated success of a minimally resourced digitisation effort",
      "location": {
        "section": "Introduction",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "synthesize",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E001",
        "E004"
      ],
      "supports_claims": [],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C012",
      "claim_text": "The approach required modest resource investment compared to alternatives",
      "claim_type": "comparison",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "minimally resourced digitisation effort",
      "location": {
        "section": "Introduction",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "compare",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E002"
      ],
      "supports_claims": [
        "C011"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": "minimal vs typical"
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C017",
      "claim_text": "Novice undergraduate volunteers effectively contributed to digitisation",
      "claim_type": "outcome",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "Undergraduates in the associated field school digitised data from maps",
      "location": {
        "section": "Introduction",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E003"
      ],
      "supports_claims": [
        "C016"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C016",
      "claim_text": "The system was accessible to novice volunteers with no GIS experience",
      "claim_type": "outcome",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "streamlined, collaborative system for crowdsourcing map digitisation by volunteers with no prior GIS experience",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [],
      "supports_claims": [
        "C010"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C010",
      "claim_text": "Systems designed for field data collection on mobile devices can be profitably customised as participatory geospatial data systems for novice volunteers",
      "claim_type": "generalization",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "it indicates that systems designed for field data collection, running on mobile devices, can be profitably customised to serve as participatory geospatial data systems accessible to novice volunteers",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "primary_function": "generalize",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E001",
        "E002",
        "E003",
        "E004"
      ],
      "supports_claims": [],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C040",
      "claim_text": "Error pattern was predictable and easily correctable",
      "claim_type": "outcome",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "the pattern of errors - mostly false negatives and double-marked features, mostly from contiguous map sections - made them relatively easy to identify and correct",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 4
      },
      "primary_function": "interpret_finding",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E005",
        "E006"
      ],
      "supports_claims": [
        "C004"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C041",
      "claim_text": "Errors were predominantly false negatives and double-markings rather than false positives",
      "claim_type": "outcome",
      "claim_role": "supporting",
      "claim_status": "explicit",
      "verbatim_quote": "mostly false negatives and double-marked features",
      "location": {
        "section": "Results",
        "subsection": "3.5.2. Digitisation errors",
        "paragraph": 4
      },
      "primary_function": "report_finding",
      "claim_nature": "empirical",
      "supported_by_evidence": [
        "E006"
      ],
      "supports_claims": [
        "C040"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": null
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C050",
      "claim_text": "Crowdsourcing approach became more cost-effective than desktop GIS alternatives at moderate dataset sizes",
      "claim_type": "comparison",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 – 60,000 records",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2. Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "primary_function": "generalize",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E015",
        "E016"
      ],
      "supports_claims": [],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "10,000-60,000 records optimal"
        ],
        "comparative_framing": "crowdsourcing vs desktop GIS vs ML"
      },
      "alternatives_acknowledged": [],
      "qualifications": [
        "conservative estimate"
      ],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C051",
      "claim_text": "Crowdsourced digitisation effort proved unexpectedly successful",
      "claim_type": "evaluation",
      "claim_role": "core",
      "verbatim_quote": "Our crowdsourced digitisation effort involving novice volunteers using an adapted mobile application for data capture proved unexpectedly successful",
      "location": {
        "section": "Discussion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "proved",
        "confidence_marker": "unexpectedly",
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E001",
        "E002",
        "E003",
        "E004"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C052",
        "C057",
        "C061"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C052",
      "claim_text": "Payoff threshold suggests digitisation by project staff suitable only for smaller datasets",
      "claim_type": "methodological_recommendation",
      "claim_role": "intermediate",
      "verbatim_quote": "Such a payoff threshold suggests that digitisation by project staff will be suitable only for smaller datasets",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "suggests",
        "confidence_marker": null,
        "scope_limitation": "payoff threshold analysis"
      },
      "supported_by_evidence": [
        "E018"
      ],
      "supports_claims": [
        "C051"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [
        "explicit definition of 'smaller datasets'"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C053",
      "claim_text": "Project could not afford to dedicate 3.5-4.5 weeks of staff time to digitisation",
      "claim_type": "contextual_constraint",
      "claim_role": "supporting",
      "verbatim_quote": "We could not have afforded to dedicate 3.5 – 4.5 weeks of staff time to digitisation",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "could not have afforded",
        "confidence_marker": null,
        "scope_limitation": "project context"
      },
      "supported_by_evidence": [
        "E018"
      ],
      "supports_claims": [
        "C052"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C054",
      "claim_text": "There are four principal approaches to digitising historical maps",
      "claim_type": "classification",
      "claim_role": "intermediate",
      "verbatim_quote": "Digitisation projects will likely choose between one of four principal approaches to digitising historical maps",
      "location": {
        "section": "Discussion",
        "subsection": "4.1 Choosing an approach",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "will likely",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C052",
        "C055",
        "C067"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C055",
      "claim_text": "Desktop GIS digitisation using novice volunteers almost competitive with mobile application at highest rate",
      "claim_type": "comparative_effectiveness",
      "claim_role": "supporting",
      "verbatim_quote": "At the highest rate, desktop GIS digitisation using novice volunteers is almost competitive with the mobile application approach we used",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "is almost competitive",
        "confidence_marker": "At the highest rate",
        "scope_limitation": "optimal conditions"
      },
      "supported_by_evidence": [
        "E020",
        "E022"
      ],
      "supports_claims": [
        "C054"
      ],
      "supported_by_claims": [
        "C056"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C056",
      "claim_text": "Project unable to retain enough volunteers in 2010 to complete desktop GIS work",
      "claim_type": "contextual_constraint",
      "claim_role": "supporting",
      "verbatim_quote": "Scaling to this dataset size, however, assumes that enough volunteers could be retained to complete the work - something we were unable to do in 2010",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "were unable to do",
        "confidence_marker": null,
        "scope_limitation": "2010 experience"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C055"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [
        "reasons for volunteer attrition"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C057",
      "claim_text": "190 features per staff-hour figure understates value realised from volunteer digitisation",
      "claim_type": "evaluation",
      "claim_role": "intermediate",
      "verbatim_quote": "This figure, however, understates the value our project realised from volunteer digitisation",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "understates",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E022"
      ],
      "supports_claims": [
        "C051"
      ],
      "supported_by_claims": [
        "C058",
        "C060",
        "C062",
        "C063"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C058",
      "claim_text": "Customisation of systems like FAIMS Mobile can be outsourced more easily than other project activities",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "customisation of systems like FAIMS Mobile can be outsourced more easily than other project activities",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "can be outsourced more easily",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E023",
        "E024"
      ],
      "supports_claims": [
        "C057"
      ],
      "supported_by_claims": [
        "C059"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C059",
      "claim_text": "21 internal staff hours represent exceptional efficiency compared to alternatives",
      "claim_type": "comparative_effectiveness",
      "claim_role": "supporting",
      "verbatim_quote": "Those 21 internal staff hours represent a digitisation rate of over 500 features per staff-hour. Twenty-one hours would have yielded just 1,260 – 1,575 features if staff had digitised them directly, or 2,730 – 3,780 had we supervised students using desktop GIS",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 5
      },
      "claim_strength": {
        "qualifier": "represent",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E023"
      ],
      "supports_claims": [
        "C058"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C060",
      "claim_text": "Staff time during field season was scarce and valuable",
      "claim_type": "contextual_constraint",
      "claim_role": "supporting",
      "verbatim_quote": "given competing responsibilities, staff time during the field season was scarce and valuable",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 6
      },
      "claim_strength": {
        "qualifier": "was",
        "confidence_marker": null,
        "scope_limitation": "field season context"
      },
      "supported_by_evidence": [
        "E027"
      ],
      "supports_claims": [
        "C057"
      ],
      "supported_by_claims": [
        "C061"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C061",
      "claim_text": "In-field support time was minimal while achieving high productivity",
      "claim_type": "evaluation",
      "claim_role": "supporting",
      "verbatim_quote": "Across two seasons, in-field support for volunteers was only 7 h, representing about 1,550 features per in-field staff-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 6
      },
      "claim_strength": {
        "qualifier": "was only",
        "confidence_marker": null,
        "scope_limitation": "two field seasons"
      },
      "supported_by_evidence": [
        "E027"
      ],
      "supports_claims": [
        "C051",
        "C060"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C062",
      "claim_text": "Marginal cost for each additional feature digitised is low",
      "claim_type": "economic_efficiency",
      "claim_role": "supporting",
      "verbatim_quote": "the marginal cost for each additional feature digitised is low",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "claim_strength": {
        "qualifier": "is low",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E030"
      ],
      "supports_claims": [
        "C057"
      ],
      "supported_by_claims": [
        "C063"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C063",
      "claim_text": "Scalability of crowdsourcing approach makes it more attractive for expanding projects",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "The scalability of our crowdsourcing approach makes it more attractive if a project may expand over time to include more volunteers, more redeployments, or more maps",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 7
      },
      "claim_strength": {
        "qualifier": "makes it more attractive",
        "confidence_marker": null,
        "scope_limitation": "expansion scenarios"
      },
      "supported_by_evidence": [
        "E032",
        "E033"
      ],
      "supports_claims": [
        "C057",
        "C062"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C064",
      "claim_text": "Qualitative factors argue for implementing crowdsourcing approach using mobile application",
      "claim_type": "methodological_recommendation",
      "claim_role": "intermediate",
      "verbatim_quote": "qualitative factors also argue for implementing a crowdsourcing approach using a mobile application",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 8
      },
      "claim_strength": {
        "qualifier": "argue for",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E018",
        "E020",
        "E022",
        "E023",
        "E027"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C065"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C065",
      "claim_text": "Desktop GIS caused continual staff stress and distraction from need for troubleshooting, while mobile approach nearly eliminated interventions and better utilised time, attention, and motivation of both staff and participants",
      "claim_type": "comparative_effectiveness",
      "claim_role": "intermediate",
      "claim_status": "explicit",
      "verbatim_quote": "the need for staff to be continually available to troubleshoot problems with desktop GIS, lest digitisation stall, provided a continual source of stress and distraction. The switch to a lightweight GIS running on mobile devices nearly eliminated the need for staff interventions and improved volunteer satisfaction. [...] this approach better utilised the time, attention, and motivation of both staff and participants",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 8
      },
      "primary_function": "compare",
      "claim_nature": "interpretation",
      "supported_by_evidence": [],
      "supports_claims": [
        "C064"
      ],
      "quantitative_details": {
        "has_quantitative_component": false,
        "values": [],
        "comparative_framing": "desktop GIS vs mobile approach qualitative factors"
      },
      "alternatives_acknowledged": [],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_C065",
          "P1_C066"
        ],
        "consolidation_type": "narrative_consolidation",
        "information_preserved": "complete",
        "granularity_available": "Desktop GIS problems (stress, distraction, troubleshooting) and mobile benefits (reduced interventions, improved satisfaction, better utilization) both preserved",
        "rationale": "Problem-solution narrative assessed together as qualitative comparison supporting C064 recommendation"
      },
      "supported_by_claims": []
    },
    {
      "claim_id": "C067",
      "claim_text": "ML approaches worthwhile for large-scale projects with consistent symbology",
      "claim_type": "methodological_recommendation",
      "claim_role": "intermediate",
      "verbatim_quote": "suggests that ML approaches are worthwhile for large-scale projects that benefit from the consistent symbology and style (as found in British and Ottoman imperial maps)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "suggests",
        "confidence_marker": null,
        "scope_limitation": "consistent symbology and style"
      },
      "supported_by_evidence": [
        "E034",
        "E035",
        "E036"
      ],
      "supports_claims": [
        "C054"
      ],
      "supported_by_claims": [
        "C068"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C068",
      "claim_text": "ML papers rarely quantify time-on-task making it difficult to assess when investment yields savings",
      "claim_type": "methodological_gap",
      "claim_role": "supporting",
      "verbatim_quote": "Unfortunately, ML papers rarely quantify time-on-task, making it difficult to assess how large a dataset needs to be before the investment in ML approaches yields time savings",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "rarely quantify",
        "confidence_marker": "Unfortunately",
        "scope_limitation": "ML literature"
      },
      "supported_by_evidence": [
        "E034",
        "E037"
      ],
      "supports_claims": [
        "C067"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C069",
      "claim_text": "Project achieved 44.9 features per person-hour including all time investments",
      "claim_type": "quantitative_outcome",
      "claim_role": "supporting",
      "verbatim_quote": "These 241 h produced a dataset of 10,827 features, a rate of 44.9 features/person-hour",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "produced",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E038",
        "E039"
      ],
      "supports_claims": [
        "C070"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C070",
      "claim_text": "Crowdsourcing approach most suitable for datasets numbering 10,000-60,000 records",
      "claim_type": "methodological_recommendation",
      "claim_role": "core",
      "verbatim_quote": "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 – 60,000 records, assuming similar feature characteristics and data collection requirements",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "is most suitable",
        "confidence_marker": "perhaps",
        "scope_limitation": "similar feature characteristics and data collection requirements"
      },
      "supported_by_evidence": [
        "E040"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C069",
        "C071",
        "C072"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C071",
      "claim_text": "Below 10,000 records, desktop GIS approaches should be considered",
      "claim_type": "methodological_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "Below 10,000 records, approaches using desktop GIS should be considered",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "should be considered",
        "confidence_marker": null,
        "scope_limitation": "dataset size <10,000"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C070"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C072",
      "claim_text": "Above 60,000 records, ML approaches should be contemplated with requisite expertise",
      "claim_type": "methodological_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "Above 60,000 records, ML approaches should be contemplated, but only if a project has access to the requisite expertise",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "should be contemplated",
        "confidence_marker": null,
        "scope_limitation": "requires expertise access"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C070"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C073",
      "claim_text": "Projects where staff time is at premium may find value for datasets below 1,000 records",
      "claim_type": "methodological_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "Projects where staff time is at a premium, or that operate alongside fieldwork where staff have many competing demands, may find it valuable for smaller datasets (even those below 1,000 records)",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "may find it valuable",
        "confidence_marker": null,
        "scope_limitation": "staff time premium or fieldwork contexts"
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C074",
      "claim_text": "ML and crowdsourcing approaches are complementary, not exclusive",
      "claim_type": "methodological_synthesis",
      "claim_role": "intermediate",
      "verbatim_quote": "The approaches are not exclusive, therefore, but complementary",
      "location": {
        "section": "Discussion",
        "subsection": "4.2 Combining crowdsourcing and ML approaches",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "are",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C075",
        "C076"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C075",
      "claim_text": "Dataset large enough for ML likely needs training dataset large enough to warrant crowdsourcing",
      "claim_type": "practical_implication",
      "claim_role": "supporting",
      "verbatim_quote": "A dataset big enough to justify ML will likely need a training dataset big enough to warrant crowdsourcing, especially if the features or background are variable",
      "location": {
        "section": "Discussion",
        "subsection": "4.2 Combining crowdsourcing and ML approaches",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "will likely need",
        "confidence_marker": null,
        "scope_limitation": "variable features or background"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C074"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C076",
      "claim_text": "Crowdsourcing platform can be used for ML error-checking datasets",
      "claim_type": "practical_implication",
      "claim_role": "supporting",
      "verbatim_quote": "Once the crowdsourcing platform has been built, moreover, it can be used to produce additional datasets for error-checking to confirm the accuracy of the ML results",
      "location": {
        "section": "Discussion",
        "subsection": "4.2 Combining crowdsourcing and ML approaches",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "can be used",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C074"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C077",
      "claim_text": "Typical archaeology/history projects may not be able to incorporate ML successfully",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "a typical project in history or archaeology - often small, under-resourced, and pursuing several research activities - may not be able to dedicate the personnel, infrastructure, or attention needed to incorporate ML successfully",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "may not be able",
        "confidence_marker": null,
        "scope_limitation": "typical small under-resourced projects"
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C078"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C078",
      "claim_text": "Such projects could deploy collaborative geospatial system for crowdsourcing",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "but could deploy a collaborative geospatial system for crowdsourcing map digitisation",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "could deploy",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C077"
      ],
      "supported_by_claims": [
        "C079"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C079",
      "claim_text": "Deploying collaborative geospatial system requires higher up-front investment but is feasible",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "Deploying such a system requires a higher up-front investment in time and expertise than use of desktop GIS approaches, but it is feasible",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "is feasible",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C078"
      ],
      "supported_by_claims": [
        "C080"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C080",
      "claim_text": "Software Carpentry level skills sufficient to customise and operate FAIMS Mobile",
      "claim_type": "feasibility_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "A project with a digital humanist or similar technologist with skills at the level of core Software Carpentry lessons can customise and operate a generalised platform such as FAIMS Mobile to implement an effective crowdsourcing system",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "can customise and operate",
        "confidence_marker": null,
        "scope_limitation": "with appropriate skills"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C079"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C081",
      "claim_text": "Technical barriers to deploying such systems will likely decline in future",
      "claim_type": "projection",
      "claim_role": "supporting",
      "verbatim_quote": "in future the technical barriers to deploying such systems will likely decline",
      "location": {
        "section": "Discussion",
        "subsection": "4.3 Overall feasibility",
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "will likely decline",
        "confidence_marker": null,
        "scope_limitation": "future projection"
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C082",
      "claim_text": "Projects need to weigh trade-offs between different digitisation approaches",
      "claim_type": "methodological_recommendation",
      "claim_role": "core",
      "verbatim_quote": "Projects need to weigh the trade-offs between different approaches",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "need to weigh",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C083",
        "C084",
        "C085"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C083",
      "claim_text": "Desktop GIS complexity limits scale of novice digitisation",
      "claim_type": "limitation",
      "claim_role": "supporting",
      "verbatim_quote": "The complexity of full-featured, desktop GIS makes it difficult for novices to use, limiting the scale of digitisation",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "makes it difficult",
        "confidence_marker": null,
        "scope_limitation": "novice users"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C082"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C084",
      "claim_text": "ML approaches require significant resources and expertise to avoid failures",
      "claim_type": "limitation",
      "claim_role": "supporting",
      "verbatim_quote": "ML approaches require significant resources to implement and expertise to avoid failures arising from training bias or other pitfalls",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "require",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C082"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C085",
      "claim_text": "Purpose-built lightweight collaborative systems fill the gap between desktop GIS and ML",
      "claim_type": "synthesis",
      "claim_role": "core",
      "verbatim_quote": "Purpose-built, lightweight, collaborative geospatial data recording systems fill the gap between",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "fill the gap",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C082"
      ],
      "supported_by_claims": [
        "C086"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C086",
      "claim_text": "FAIMS Mobile facilitated multiple capabilities with high-quality results and few compromises",
      "claim_type": "evaluation",
      "claim_role": "core",
      "verbatim_quote": "Our use of FAIMS Mobile facilitated offline, multi-user map-feature digitisation, minimised post-processing, supported multiple data types, and produced high-quality data aligning with our methods and aims - with few compromises, work-arounds, or other technological distractions",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 1
      },
      "claim_strength": {
        "qualifier": "facilitated",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C085"
      ],
      "supported_by_claims": [
        "C087",
        "C088",
        "C089",
        "C090"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C087",
      "claim_text": "Deployment facilitated rapid digitisation with modest resources",
      "claim_type": "evaluation",
      "claim_role": "supporting",
      "verbatim_quote": "The deployment of the Map Digitisation FAIMS Mobile customisation facilitated the rapid digitisation (57 staff-hours; 184 volunteer-hours; total) of 10,827 features found in Soviet topographic maps",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "facilitated",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E001",
        "E002",
        "E003"
      ],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C088",
      "claim_text": "System required modest hardware and minimal supervision while supporting offline operation",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "It required only modest hardware and minimal supervision, but supported offline operation, including in-field setup, data collection, synchronisation across multiple devices, and data backup and export",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "claim_strength": {
        "qualifier": "required only",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C089",
      "claim_text": "Data was available daily and FAIR-compliant dataset ready with minimal processing",
      "claim_type": "practical_advantage",
      "claim_role": "supporting",
      "verbatim_quote": "All collected data was available daily for review, and a comprehensive, FAIR-compliant dataset was ready for analysis with less than 2 h of processing after collection",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "was available",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E043"
      ],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C090",
      "claim_text": "Overall quality of dataset was high",
      "claim_type": "evaluation",
      "claim_role": "core",
      "verbatim_quote": "Overall quality of this dataset was high",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "was high",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E041",
        "E042"
      ],
      "supports_claims": [
        "C086"
      ],
      "supported_by_claims": [
        "C091",
        "C092"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C091",
      "claim_text": "Data omissions were correctable and subsequently minimised through validation",
      "claim_type": "quality_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "Some 2% of records had recoverable data omissions which were corrected during post-processing and subsequently minimised through the addition of validation",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "were corrected",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E041"
      ],
      "supports_claims": [
        "C090"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C092",
      "claim_text": "Errors were predictable and easily mitigatable",
      "claim_type": "quality_assessment",
      "claim_role": "supporting",
      "verbatim_quote": "errors were predictable and would be easily mitigated by redundant digitisation by volunteers or volunteer peer review",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "were predictable",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [
        "E042"
      ],
      "supports_claims": [
        "C090"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C093",
      "claim_text": "Crowdsourcing approach becomes worthwhile at specific payoff thresholds based on staff time assumptions, and is most efficient for datasets up to 60,000 features, above which machine learning should be considered",
      "claim_type": "methodological_recommendation",
      "claim_role": "core",
      "claim_status": "explicit",
      "verbatim_quote": "a crowdsourcing approach like ours will become worthwhile after a specific ' payoff threshold ', determined by assumptions about available staff time, project objectives, and dataset requirements. [...] our approach is probably most efficient for up to about 60,000 features, above which ML should be seriously considered",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "primary_function": "recommend",
      "claim_nature": "interpretation",
      "supported_by_evidence": [
        "E045",
        "E044",
        "E046"
      ],
      "supports_claims": [],
      "supported_by_claims": [
        "C073"
      ],
      "quantitative_details": {
        "has_quantitative_component": true,
        "values": [
          "60,000 features upper bound"
        ],
        "comparative_framing": "payoff threshold analysis"
      },
      "alternatives_acknowledged": [
        "machine learning for > 60,000 features"
      ],
      "qualifications": [],
      "contradicts_literature": [],
      "consolidation_metadata": {
        "consolidated_from": [
          "P1_C093",
          "P1_C094"
        ],
        "consolidation_type": "synthesis",
        "information_preserved": "complete",
        "granularity_available": "Payoff threshold concept and 60,000-feature upper bound both preserved",
        "rationale": "Both claims address optimal dataset size range for crowdsourcing approach; consolidation provides complete recommendation with lower and upper bounds"
      }
    },
    {
      "claim_id": "C095",
      "claim_text": "Approach can produce training datasets for ML and serve as quality assurance tool",
      "claim_type": "practical_implication",
      "claim_role": "intermediate",
      "verbatim_quote": "Our approach can also be used to produce the training datasets needed for ML, and as a tool for quality assurance for ML outputs",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "claim_strength": {
        "qualifier": "can be used",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C096",
      "claim_text": "Approach is readily transferable to other mobile GIS systems and map corpora",
      "claim_type": "generalizability",
      "claim_role": "intermediate",
      "verbatim_quote": "This approach is readily transferable to other mobile GIS systems and map corpora",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "is readily transferable",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [],
      "supported_by_claims": [
        "C097"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C097",
      "claim_text": "Experience provides only single data point for assessing applicability of approaches",
      "claim_type": "limitation",
      "claim_role": "supporting",
      "verbatim_quote": "but our experience provides only a single data point for assessing the applicability of various digitisation approaches to historical maps",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "provides only",
        "confidence_marker": null,
        "scope_limitation": "single data point"
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C096"
      ],
      "supported_by_claims": [
        "C098"
      ],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "claim_id": "C098",
      "claim_text": "More projects need to track and publish comprehensive digitisation metrics",
      "claim_type": "research_recommendation",
      "claim_role": "supporting",
      "verbatim_quote": "More projects - whether they use manual or automated approaches - need to track and publish the expert and volunteer time required for setup, training, support, and quality assurance related to map digitisation, as well as digitisation speed, error rates and types, the characteristics of the features being digitised, and the complexity of information extracted",
      "location": {
        "section": "Conclusion",
        "subsection": null,
        "paragraph": 4
      },
      "claim_strength": {
        "qualifier": "need to",
        "confidence_marker": null,
        "scope_limitation": null
      },
      "supported_by_evidence": [],
      "supports_claims": [
        "C097"
      ],
      "supported_by_claims": [],
      "alternative_claims": [],
      "expected_information_missing": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    }
  ],
  "implicit_arguments": [
    {
      "implicit_argument_id": "IA001",
      "implicit_argument_text": "Quality control mechanisms were in place to achieve the stated error rate",
      "argument_type": "logical_implication",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "with an error rate under 6%",
        "The resulting dataset was consistent, well-documented"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Achieving a low error rate (<6%) with novice volunteers implies systematic quality control procedures were implemented. Consistency and documentation quality further suggest structured verification processes. The authors don't explicitly describe QA mechanisms in abstract, but these outcomes would be impossible without them.",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "supports_claim": "C004",
      "implicit_metadata": {
        "inferential_basis": "procedural_requirement",
        "confidence": "high",
        "gaps_in_stated_reasoning": [
          "Quality control procedures not explicitly described in abstract",
          "Verification methods not specified"
        ],
        "alternative_interpretations": []
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA002",
      "implicit_argument_text": "The customisation of FAIMS Mobile was technically feasible without prohibitive expertise",
      "argument_type": "unstated_assumption",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "a customisation of the Field Acquired Information Management Systems (FAIMS) Mobile platform",
        "it requires less technical expertise, time, and resourcing to undertake"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Introduction",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Authors claim the approach requires 'less technical expertise' than ML while simultaneously reporting successful 'customisation' of FAIMS Mobile. This implies platform customisation was achievable without high technical barriers. The assumption is that customisation was straightforward enough to still qualify as 'low expertise' approach.",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "supports_claim": "C010",
      "implicit_metadata": {
        "inferential_basis": "feasibility_assumption",
        "confidence": "moderate",
        "gaps_in_stated_reasoning": [
          "Technical requirements for customisation not specified",
          "Expertise needed for platform adaptation not detailed"
        ],
        "alternative_interpretations": [
          "Platform may have required significant technical expertise that authors minimize",
          "Customisation may have been simpler than typical mobile platform adaptation"
        ]
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA003",
      "implicit_argument_text": "The Soviet military maps were sufficiently accurate and reliable as source documents",
      "argument_type": "disciplinary_assumption",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "digitise 10,827 mound features from Soviet military topographic maps",
        "creating large, high-quality geospatial datasets from historical maps"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Title",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Authors claim to produce 'high-quality geospatial datasets' from Soviet military maps without discussing map accuracy, georeferencing precision, or systematic distortions. This implies an unstated assumption that these maps are reliable source documents. In archaeological GIS, source map quality fundamentally constrains output quality.",
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "supports_claim": "C004",
      "implicit_metadata": {
        "inferential_basis": "source_reliability_assumption",
        "confidence": "moderate",
        "gaps_in_stated_reasoning": [
          "Map accuracy not assessed",
          "Georeferencing error not quantified",
          "Systematic distortions not discussed"
        ],
        "alternative_interpretations": [
          "Maps may have known accuracy limitations not discussed",
          "High-quality claim may refer to digitisation precision rather than absolute accuracy"
        ]
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA004",
      "implicit_argument_text": "The 10,000-60,000 feature range is based on extrapolation from their single case study",
      "argument_type": "bridging_claim",
      "argument_status": "implicit",
      "verbatim_quote": null,
      "trigger_text": [
        "FAIMS Mobile was used to digitise 10,827 mound features",
        "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 – 60,000 records"
      ],
      "trigger_locations": [
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2",
          "paragraph": 4
        }
      ],
      "inference_reasoning": "Authors generalize from their 10,827-feature case to claim efficiency in 10,000-60,000 range. The bridging logic connecting their single data point to this efficiency range is not explicitly stated. The claim requires assumptions about how resource requirements scale with dataset size, setup costs, and comparison to alternative approaches.",
      "location": {
        "section": "Discussion",
        "subsection": "4.1.2",
        "paragraph": 4
      },
      "supports_claim": "C050",
      "implicit_metadata": {
        "inferential_basis": "scaling_model",
        "confidence": "moderate",
        "gaps_in_stated_reasoning": [
          "Scaling assumptions not specified",
          "Efficiency calculations not shown",
          "Comparison methodology not detailed"
        ],
        "alternative_interpretations": [
          "Range may be based on additional unpublished analyses",
          "Conservative estimate may incorporate substantial safety margins"
        ]
      },
      "consolidation_metadata": {
        "consolidation_performed": false
      },
      "supports_claims": []
    },
    {
      "implicit_argument_id": "IA005",
      "implicit_argument_text": "Staff time efficiency is the primary metric for assessing digitisation approach value",
      "implicit_argument_type": "unstated_assumption",
      "trigger_text": [
        "This discussion, furthermore, focuses on our most limited resource: staff time. As such, the calculations below, which propose dataset size thresholds for various approaches, prioritise staff time required for digitisation",
        "If staff time is the primary limiting resource, our approach becomes worthwhile for datasets no larger than about 4,500 features"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1 Choosing an approach",
          "paragraph": 4
        },
        {
          "section": "Conclusion",
          "subsection": null,
          "paragraph": 3
        }
      ],
      "inference_reasoning": "The entire comparative analysis and threshold recommendations are predicated on staff time being the primary metric of value. This assumption shapes all payoff calculations but is stated as contextual choice rather than universal principle. The implicit argument is that this metric prioritization is appropriate for the field conditions and project constraints, which may not hold for all projects.",
      "implicit_metadata": {
        "inference_basis": "methodological_framing",
        "gaps_in_support": [
          "No consideration of other potential metrics (cost, data quality, volunteer experience)",
          "No discussion of contexts where different metrics might be prioritized"
        ],
        "confidence_in_inference": "high"
      },
      "supports_claims": [
        "C052",
        "C070",
        "C093"
      ],
      "related_implicit_arguments": [
        "IA006"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "implicit_argument_id": "IA006",
      "implicit_argument_text": "Linear scaling of time-to-features ratios is valid for comparing approaches",
      "implicit_argument_type": "unstated_assumption",
      "trigger_text": [
        "At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420 – 4,275 staff-digitised features",
        "At that rate, the 1,300 h it took to deploy the ML approach taken by Can, Gerrits, and Kabadayi would yield about 58,400 records"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 1
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2 Machine learning versus crowdsourcing",
          "paragraph": 3
        }
      ],
      "inference_reasoning": "All comparative calculations assume linear relationships between time investment and features produced, projecting from observed rates to hypothetical larger datasets. This assumes no economies or diseconomies of scale, no learning effects, and consistent feature characteristics across datasets. The validity of these projections depends on this linearity holding at different scales.",
      "implicit_metadata": {
        "inference_basis": "methodological_extrapolation",
        "gaps_in_support": [
          "No empirical validation of linear scaling at different dataset sizes",
          "No discussion of potential non-linear effects (learning curves, fatigue, complexity changes)"
        ],
        "confidence_in_inference": "medium"
      },
      "supports_claims": [
        "C052",
        "C070",
        "C093"
      ],
      "related_implicit_arguments": [
        "IA005"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "implicit_argument_id": "IA007",
      "implicit_argument_text": "Project's feature characteristics and map conditions are representative of broader historical map digitisation",
      "implicit_argument_type": "bridging_claim",
      "trigger_text": [
        "Note that in all following comparisons, we present our experience as a (perhaps idiosyncratic) example. Time per feature included locating it and completing the record, hence the rate reflects the high density and moderate obtrusiveness of archaeological features in Soviet topographic maps",
        "a crowdsourcing approach like ours is most suitable for datasets numbering perhaps 10,000 – 60,000 records, assuming similar feature characteristics and data collection requirements",
        "This approach is readily transferable to other mobile GIS systems and map corpora, but our experience provides only a single data point"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1 Choosing an approach",
          "paragraph": 4
        },
        {
          "section": "Discussion",
          "subsection": "4.1.2 Machine learning versus crowdsourcing",
          "paragraph": 4
        },
        {
          "section": "Conclusion",
          "subsection": null,
          "paragraph": 4
        }
      ],
      "inference_reasoning": "The authors acknowledge their example may be idiosyncratic and represents a single data point, yet make generalized recommendations about dataset size thresholds and approach suitability. The bridging claim is that despite acknowledging specificity, the project characteristics are representative enough to warrant these generalizations. The tension between acknowledged uniqueness and generalized recommendations reveals this implicit bridging assumption.",
      "implicit_metadata": {
        "inference_basis": "generalization_from_single_case",
        "gaps_in_support": [
          "No empirical comparison with other map types or feature characteristics",
          "Acknowledged as 'single data point' but recommendations presented with confidence"
        ],
        "confidence_in_inference": "medium"
      },
      "supports_claims": [
        "C070",
        "C093",
        "C096"
      ],
      "related_implicit_arguments": [
        "IA005",
        "IA006"
      ],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    },
    {
      "implicit_argument_id": "IA008",
      "implicit_argument_text": "Volunteer satisfaction and reduced staff stress have measurable project value beyond efficiency metrics",
      "implicit_argument_type": "unstated_assumption",
      "trigger_text": [
        "qualitative factors also argue for implementing a crowdsourcing approach using a mobile application",
        "the need for staff to be continually available to troubleshoot problems with desktop GIS, lest digitisation stall, provided a continual source of stress and distraction",
        "The switch to a lightweight GIS running on mobile devices nearly eliminated the need for staff interventions and improved volunteer satisfaction",
        "this approach better utilised the time, attention, and motivation of both staff and participants"
      ],
      "trigger_locations": [
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 8
        },
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 8
        },
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 9
        },
        {
          "section": "Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 9
        }
      ],
      "inference_reasoning": "The discussion of qualitative factors (stress, satisfaction, motivation) is presented as 'also arguing for' the mobile approach, implying these factors have decision-making weight. However, the entire quantitative analysis focuses solely on time efficiency. The implicit argument is that these qualitative benefits have value comparable to or exceeding the quantitative efficiency gains, justifying their inclusion in the recommendation despite not being incorporated into the threshold calculations.",
      "implicit_metadata": {
        "inference_basis": "value_judgment",
        "gaps_in_support": [
          "No quantification of qualitative benefits",
          "No explicit weighting of qualitative vs quantitative factors in recommendations",
          "No discussion of contexts where qualitative factors might be prioritized differently"
        ],
        "confidence_in_inference": "medium-high"
      },
      "supports_claims": [
        "C064",
        "C065",
        "C086"
      ],
      "related_implicit_arguments": [],
      "consolidation_metadata": {
        "consolidation_performed": false
      }
    }
  ],
  "project_metadata": {
    "timeline": {
      "project_start": "2008",
      "project_end": "ongoing through 2022",
      "digitisation_period": "2017-2018",
      "prior_cataloguing": "2008-2016",
      "field_seasons": [
        "2017",
        "2018"
      ],
      "source": {
        "section": "Introduction",
        "subsection": "1.1. The Tundzha Regional Archaeology Project",
        "paragraph": "2-3"
      }
    },
    "location": {
      "country": "Bulgaria",
      "regions": [
        "Yambol region",
        "Kazanlak Valley"
      ],
      "broader_area": "Tundzha River watershed, southeast Bulgaria",
      "map_coverage": "over 20,000 sq km",
      "source": {
        "section": "Introduction / Abstract",
        "subsection": "1.1",
        "paragraph": "1-3"
      }
    },
    "resources": {
      "personnel": {
        "staff_hours": 57,
        "volunteer_hours": 184,
        "total_person_hours": 241,
        "volunteers": "undergraduate students in field school",
        "student_programmer_hours": 36,
        "student_programmer_cost": "ca. AUD $2,000"
      },
      "equipment": {
        "description": "mobile devices",
        "cost_characteristics": "low-cost"
      },
      "software": {
        "platform": "FAIMS Mobile (customised)",
        "license": "open-source"
      },
      "funding": "minimally resourced",
      "source": {
        "section": "Abstract / Discussion",
        "subsection": "4.1.1",
        "paragraph": "1 and 4"
      }
    },
    "track_record": {
      "prior_work": [
        {
          "period": "2008-2016",
          "output": "773 mounds catalogued in Kazanlak Valley",
          "methods": "pedestrian surface survey, manual digitisation of satellite imagery and maps"
        },
        {
          "period": "2008-2016",
          "output": "431 mounds catalogued in Yambol region",
          "methods": "pedestrian surface survey, manual digitisation"
        },
        {
          "period": "2010",
          "output": "Unsuccessful desktop GIS volunteer digitisation trial",
          "outcome": "High volunteer attrition, excessive staff time demands"
        }
      ],
      "project_aims": [
        "reconstructing ancient environment",
        "mapping evolution of habitation",
        "explaining human-environment interactions",
        "producing inventory for research and conservation",
        "analysing threats"
      ],
      "source": {
        "section": "Introduction",
        "subsection": "1.1",
        "paragraph": "2"
      }
    }
  },
  "research_designs": [
    {
      "design_id": "RD001",
      "design_type": "research_question",
      "design_status": "explicit",
      "verbatim_quote": "Here we present a customisation of the Field Acquired Information Management Systems (FAIMS) Mobile platform tailored to offer a streamlined, collaborative system for crowdsourcing map digitisation by volunteers with no prior GIS experience.",
      "research_questions": [
        {
          "question": "Can a customized FAIMS Mobile platform enable crowdsourcing of map digitization by volunteers with no prior GIS experience?",
          "question_type": "exploratory",
          "formulation_timing": "pre-data",
          "timing_basis": "Framed in abstract before presenting results"
        }
      ],
      "reasoning_approach": {
        "approach": "deductive",
        "reasoning_confidence": "high",
        "indicators": [
          "System designed to test hypothesis about volunteer capability",
          "Pre-specified approach tested in field"
        ]
      },
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "extraction_notes": "Core research question about feasibility of crowdsourced digitization by novices"
    },
    {
      "design_id": "RD010",
      "design_type": "research_question",
      "design_status": "explicit",
      "verbatim_quote": "We next plan to take our dataset and use it to train and error-check a ML model, to more systematically compare the results of crowdsourcing versus machine learning.",
      "research_questions": [
        {
          "question": "How do crowdsourcing and machine learning approaches compare for map digitization?",
          "question_type": "comparative",
          "formulation_timing": "post-data",
          "timing_basis": "Stated as future work after crowdsourcing study completed"
        }
      ],
      "reasoning_approach": {
        "approach": "deductive",
        "reasoning_confidence": "high",
        "indicators": [
          "Systematic comparison design",
          "Hypothesis testing framework"
        ]
      },
      "location": {
        "section": "5. Conclusion",
        "subsection": null,
        "paragraph": 3
      },
      "extraction_notes": "Future research direction to systematically compare approaches"
    },
    {
      "design_id": "RD002",
      "design_type": "study_design",
      "design_status": "explicit",
      "verbatim_quote": "A conservative estimate based on our work suggests our crowdsourcing approach is most efficient for digitisation projects of 10,000 – 60,000 features, but may offer advantages for datasets as small as a few hundred records.",
      "theoretical_framework": "Comparative efficiency analysis across digitization approaches (manual GIS, crowdsourcing, machine learning)",
      "scope_limitations": [
        "Focused on mid-sized datasets (100s to 10,000s of features)",
        "Archaeological feature digitization from historical maps"
      ],
      "reasoning_approach": {
        "approach": "abductive",
        "reasoning_confidence": "medium",
        "indicators": [
          "Identifying best approach for specific contexts",
          "Inference to best explanation for different project scales"
        ]
      },
      "location": {
        "section": "Abstract",
        "subsection": null,
        "paragraph": 1
      },
      "extraction_notes": "Study designed to determine when crowdsourcing approach becomes worthwhile compared to alternatives"
    },
    {
      "design_id": "RD003",
      "design_type": "research_question",
      "design_status": "explicit",
      "verbatim_quote": "The aims of the project include reconstructing the ancient environment, mapping the evolution of habitation, and combining those outcomes to explain long-term, human-environment interactions. It also seeks to produce an inventory of archaeological heritage for research and conservation purposes, including analyses of threats",
      "research_questions": [
        {
          "question": "What was the ancient environment in the Tundzha River watershed?",
          "question_type": "descriptive",
          "formulation_timing": "pre-data",
          "timing_basis": "Stated as project aim before describing activities"
        },
        {
          "question": "How did habitation patterns evolve in the Tundzha region?",
          "question_type": "descriptive",
          "formulation_timing": "pre-data",
          "timing_basis": "Stated as project aim before describing activities"
        },
        {
          "question": "How can long-term human-environment interactions be explained?",
          "question_type": "explanatory",
          "formulation_timing": "pre-data",
          "timing_basis": "Stated as project aim before describing activities"
        },
        {
          "question": "What archaeological heritage exists in the study region?",
          "question_type": "descriptive",
          "formulation_timing": "pre-data",
          "timing_basis": "Stated as project objective"
        },
        {
          "question": "What threats face archaeological heritage in the region?",
          "question_type": "descriptive",
          "formulation_timing": "pre-data",
          "timing_basis": "Stated as project objective"
        }
      ],
      "reasoning_approach": {
        "approach": "mixed",
        "reasoning_confidence": "high",
        "indicators": [
          "Descriptive mapping and inventory (inductive)",
          "Environmental reconstruction (abductive)",
          "Explanation of interactions (deductive/abductive)"
        ]
      },
      "location": {
        "section": "1. Introduction",
        "subsection": "1.1 The Tundzha Regional Archaeology Project",
        "paragraph": 2
      },
      "consolidation_metadata": {
        "consolidated_from": [
          "RD003",
          "RD004"
        ],
        "consolidation_type": "scope_integration",
        "information_preserved": "complete",
        "rationale": "Both items describe complementary TRAP project aims (research questions and heritage management objectives) from same paragraph. Combined they provide complete scope of project objectives that frame the digitization work."
      },
      "extraction_notes": "Consolidated TRAP project aims: research objectives (environment, habitation, interactions) + heritage management objectives (inventory, threats)"
    },
    {
      "design_id": "RD005",
      "design_type": "theoretical_framework",
      "design_status": "explicit",
      "verbatim_quote": "While some expert interaction is unavoidable and desirable, it can be profitably supplemented by the development of 'useful' tools that combine 'utility' with 'usability'. Utility indicates the ability of the system to accomplish the task at hand, in this case producing standardised, interoperable, well-documented digital data from scanned historical maps. Usability includes the degree to which a system's User Interface (UI) allows first time users to accomplish basic tasks, helps more experienced users to complete tasks quickly, eases resumption of work after a period of disuse, is pleasant for users to employ, and mitigates the rate and severity of errors. UIs for mobile data collection systems must allow the user to (1) focus on observations while minimising interactions with the recording mechanism, (2), enter large amounts of data quickly and accurately, with appropriate automation and validation, and (3) aid recording of data context such as metadata or related data",
      "theoretical_framework": "Integrated Utility-Usability and HCI framework for VGI system design",
      "key_concepts": [
        "Utility (system capability to accomplish task)",
        "Usability (learnability, efficiency, memorability, satisfaction, accuracy)",
        "Minimize interaction burden during observations",
        "Support rapid accurate data entry with automation",
        "Facilitate metadata and context capture"
      ],
      "location": {
        "section": "1. Introduction",
        "subsection": "1.4 Sociotechnical barriers to collaborative map digitisation",
        "paragraph": 3
      },
      "consolidation_metadata": {
        "consolidated_from": [
          "RD005",
          "RD006"
        ],
        "consolidation_type": "rationale_synthesis",
        "information_preserved": "complete",
        "rationale": "Both frameworks work together as integrated theoretical foundation for system design. RD005 (Utility-Usability from Nielsen, Jones & Weber) provides general usability principles. RD006 (HCI principles from fieldwork) adapts those principles for mobile data collection. Together they form the complete theoretical basis for design decisions."
      },
      "extraction_notes": "Consolidated theoretical framework: Utility-Usability principles + HCI adaptation for mobile/deskbound digitization"
    },
    {
      "design_id": "RD007",
      "design_type": "study_design",
      "design_status": "explicit",
      "verbatim_quote": "Three activities were undertaken: (1) visiting known burial mounds, registering their location and condition; (2) identifying changes in mound condition using satellite imagery; (3) digitising mounds from over 20,000 sq km of Soviet military 1:50,000 topographic maps covering southeast Bulgaria, followed by ground-truthing",
      "multi_method": true,
      "complementary_approaches": [
        "Field registration of known mounds",
        "Satellite imagery change detection",
        "Historical map digitization with ground-truthing"
      ],
      "scope_limitations": [
        "Yambol region of Bulgaria",
        "2017-2018 field seasons",
        "Soviet military 1:50,000 topographic maps"
      ],
      "location": {
        "section": "1. Introduction",
        "subsection": "1.1 The Tundzha Regional Archaeology Project",
        "paragraph": 3
      },
      "enables_methods": [
        "M001",
        "M002",
        "M003"
      ],
      "extraction_notes": "Overall study design with three complementary activities; paper focuses on activity (3)"
    },
    {
      "design_id": "RD008",
      "design_type": "study_design",
      "design_status": "explicit",
      "verbatim_quote": "The decision to use mobile software, and FAIMS Mobile in particular, was based on several factors. First, FAIMS Mobile worked offline. Our digitisation took place alongside fieldwork, at field bases in rural Bulgaria. Reliable internet connectivity could not be guaranteed under these circumstances; a system that tolerated degraded network connectivity was required.",
      "design_rationale": "Selection of FAIMS Mobile based on offline capability requirement",
      "scope_limitations": [
        "Rural field bases in Bulgaria with unreliable connectivity"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.3 Using a mobile application for map digitisation",
        "paragraph": 4
      },
      "extraction_notes": "First of several rationales for platform selection"
    },
    {
      "design_id": "RD009",
      "design_type": "study_design",
      "design_status": "explicit",
      "verbatim_quote": "In 2017, faced with a short field season and little time for student training, we focused on implementing tools that would empower volunteers to digitise maps independently. Our approach sought to help novice users begin digitising quickly and then work productively for the duration of each field season with minimal support or frustration. As such, it stripped GIS functionality to its essentials, focusing on three tasks: layer selection, shape digitisation, and annotation, with validation and automation to improve data quality.",
      "design_rationale": "Minimize training burden and maximize independent productivity by novices",
      "key_concepts": [
        "Essential GIS functionality only",
        "Three core tasks: layer selection, shape digitization, annotation",
        "Validation and automation for quality"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.2 Crowdsourcing digitisation with field-school participants",
        "paragraph": 4
      },
      "extraction_notes": "Core design philosophy driving all implementation decisions"
    }
  ],
  "methods": [
    {
      "method_id": "M001",
      "method_text": "Visiting known burial mounds and registering location and condition",
      "method_status": "implicit",
      "trigger_text": [
        "visiting known burial mounds, registering their location and condition"
      ],
      "trigger_locations": [
        {
          "section": "1. Introduction",
          "subsection": "1.1 The Tundzha Regional Archaeology Project",
          "paragraph": 3
        }
      ],
      "inference_reasoning": "Activity (1) mentions visiting and registering mounds but provides no procedural detail about registration protocols, data collection instruments, or field procedures. Methods section would contain these details.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "No description of registration protocols, data fields collected, or field procedures",
        "assessability_impact": "Cannot assess rigor or completeness of registration method",
        "reconstruction_confidence": "low"
      },
      "data_collection_approach": "in-situ field registration",
      "implements_designs": [
        "RD007"
      ],
      "location": {
        "section": "1. Introduction",
        "subsection": "1.1 The Tundzha Regional Archaeology Project",
        "paragraph": 3
      },
      "expected_information_missing": [
        "Registration protocol details",
        "Data fields collected",
        "Equipment used",
        "Personnel involved"
      ],
      "extraction_notes": "Method mentioned as project activity but not described procedurally"
    },
    {
      "method_id": "M010",
      "method_text": "Cataloguing inputs (time) versus outputs (features digitized) to evaluate approach",
      "method_status": "explicit",
      "verbatim_quote": "The success of this approach became apparent early in the 2017 field season. At that point, we decided to catalogue inputs (time invested by staff and volunteers) versus outputs (features digitised) as part of a research program to evaluate digital approaches to fieldwork (e.g., Sobotkova et al., 2016). To measure inputs, we collated the amount of time spent by various participants in the process, including the student programmer who instantiated the customisation, the student volunteers who undertook the digitisation, and project staff who configured the system, supported volunteers, exported data, and checked for errors.",
      "data_collection_approach": "time-tracking and productivity measurement",
      "metrics_tracked": [
        "Programmer time",
        "Volunteer time-on-task",
        "Staff setup/support/QC time",
        "Number of features digitized"
      ],
      "implements_designs": [
        "RD002"
      ],
      "realized_through_protocols": [
        "P008",
        "P009"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.5 Evaluating the digitisation approach",
        "paragraph": 1
      }
    },
    {
      "method_id": "M002",
      "method_text": "Identifying changes in mound condition using satellite imagery",
      "method_status": "implicit",
      "trigger_text": [
        "identifying changes in mound condition using satellite imagery"
      ],
      "trigger_locations": [
        {
          "section": "1. Introduction",
          "subsection": "1.1 The Tundzha Regional Archaeology Project",
          "paragraph": 3
        }
      ],
      "inference_reasoning": "Activity (2) mentions using satellite imagery for change detection but provides no methodological detail about imagery sources, analysis procedures, or change detection algorithms.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "No description of imagery sources, temporal coverage, analysis methods, or change detection procedures",
        "assessability_impact": "Cannot assess appropriateness of imagery or rigor of change detection",
        "reconstruction_confidence": "low"
      },
      "data_collection_approach": "satellite remote sensing",
      "implements_designs": [
        "RD007"
      ],
      "location": {
        "section": "1. Introduction",
        "subsection": "1.1 The Tundzha Regional Archaeology Project",
        "paragraph": 3
      },
      "expected_information_missing": [
        "Satellite imagery source and specifications",
        "Temporal coverage",
        "Change detection method",
        "Validation procedures"
      ],
      "extraction_notes": "Method mentioned as project activity but not described procedurally"
    },
    {
      "method_id": "M003",
      "method_text": "Crowdsourced digitization of mounds from Soviet military topographic maps using customized FAIMS Mobile platform, followed by ground-truthing",
      "method_status": "implicit",
      "trigger_text": [
        "digitising mounds from over 20,000 sq km of Soviet military 1:50,000 topographic maps covering southeast Bulgaria, followed by ground-truthing (which continued through 2022)",
        "Here we present a customisation of the Field Acquired Information Management Systems (FAIMS) Mobile platform tailored to offer a streamlined, collaborative system for crowdsourcing map digitisation by volunteers with no prior GIS experience."
      ],
      "trigger_locations": [
        {
          "section": "1. Introduction",
          "subsection": "1.1 The Tundzha Regional Archaeology Project",
          "paragraph": 3
        },
        {
          "section": "Abstract",
          "subsection": null,
          "paragraph": 1
        }
      ],
      "inference_reasoning": "Introduction mentions digitization + ground-truthing as Activity (3) and Abstract announces crowdsourcing approach using FAIMS Mobile, but neither provides procedural implementation detail. Both passages reference that Methods section will contain full methodological description. These describe the same core method from different sections.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "High-level approach stated but implementation details (platform customization, volunteer recruitment, task allocation, quality control) deferred to Methods section",
        "assessability_impact": "Cannot assess digitization or ground-truthing procedures from Introduction/Abstract alone",
        "reconstruction_confidence": "high"
      },
      "data_collection_approach": "crowdsourced map digitization with mobile platform and field validation",
      "sampling_strategy": {
        "sampling_type": "convenience",
        "sampling_notes": "Undergraduate students associated with field project"
      },
      "implements_designs": [
        "RD001",
        "RD007"
      ],
      "location": {
        "section": "1. Introduction",
        "subsection": "1.1 The Tundzha Regional Archaeology Project",
        "paragraph": 3
      },
      "expected_information_missing": [
        "Platform customization details",
        "Volunteer recruitment and training",
        "Task allocation procedures",
        "Quality control measures",
        "Ground-truthing protocols"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "M003",
          "M004"
        ],
        "consolidation_type": "redundancy_elimination",
        "information_preserved": "complete",
        "rationale": "Both items describe the same core digitization method from different sections (Introduction vs Abstract). M003 emphasizes ground-truthing component, M004 emphasizes crowdsourcing approach. Consolidated item captures complete method description with all trigger text preserved."
      },
      "extraction_notes": "Consolidated digitization method: combines Activity (3) description from Introduction with crowdsourcing approach from Abstract. Central method of paper, fully described in Methods section."
    },
    {
      "method_id": "M005",
      "method_text": "Extraction of archaeological mound features from Soviet military 1:50,000 topographic maps (1980s) as georeferenced GeoTIFFs through streamlined three-task workflow: layer selection, shape digitization, and annotation",
      "method_status": "explicit",
      "verbatim_quote": "The goal of the work was to extract archaeological features from 1:50,000 scale Soviet military topographic maps dating to the 1980s. Available as georeferenced GeoTIFFs (from http://web.uni-plovdiv.bg/vedrin/index_en.html in 2008), each of these maps covers ca 400 sq km. Our approach sought to help novice users begin digitising quickly and then work productively for the duration of each field season with minimal support or frustration. As such, it stripped GIS functionality to its essentials, focusing on three tasks: layer selection, shape digitisation, and annotation, with validation and automation to improve data quality.",
      "data_collection_approach": "digitization from historical maps",
      "data_sources": [
        {
          "source_type": "historical_maps",
          "source_description": "Soviet military topographic maps 1:50,000 scale, 1980s",
          "acquisition_method": "Georeferenced GeoTIFFs from http://web.uni-plovdiv.bg/vedrin/index_en.html (2008)",
          "spatial_coverage": "~400 sq km per map sheet"
        }
      ],
      "workflow_components": [
        "Layer selection",
        "Shape digitization",
        "Annotation (attribute transcription)"
      ],
      "quality_control": {
        "approaches": [
          "On-device validation",
          "Automated data quality checks"
        ]
      },
      "implements_designs": [
        "RD001",
        "RD007",
        "RD009"
      ],
      "realized_through_protocols": [
        "P001",
        "P007",
        "P008",
        "P009"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.1 Archaeological features in Soviet topographic maps",
        "paragraph": 1
      },
      "consolidation_metadata": {
        "consolidated_from": [
          "M005",
          "M006"
        ],
        "consolidation_type": "workflow_integration",
        "information_preserved": "complete",
        "rationale": "M005 describes source material and extraction goal, M006 describes the three-task workflow for accomplishing that extraction. Together they provide complete picture of digitization method: what is being extracted (mound features from Soviet maps) and how (three essential tasks). Assessed together as unified digitization approach."
      },
      "extraction_notes": "Consolidated digitization approach: combines source material specification with streamlined three-task workflow"
    },
    {
      "method_id": "M007",
      "method_text": "Recruitment of field-school participants as digitization volunteers",
      "method_status": "explicit",
      "verbatim_quote": "The task of digitising potentially thousands of mounds provided an opportunity to involve students in authentic research. Our students came from a range of academic backgrounds in Arts and Humanities. Most had no training in archaeology, cartography, or digital methods (unlike Pődör, 2015 or Can et al., 2021). The students' motivation in joining our project included curiosity about field archaeology, the desire to travel outside Australia, and the satisfaction of assisting with heritage preservation.",
      "sampling_strategy": {
        "sampling_type": "convenience",
        "population": "Field school participants",
        "characteristics": "Arts and Humanities backgrounds, no training in archaeology/cartography/digital methods"
      },
      "implements_designs": [
        "RD001",
        "RD007",
        "RD009"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.2 Crowdsourcing digitisation with field-school participants",
        "paragraph": 1
      }
    },
    {
      "method_id": "M008",
      "method_text": "Customization of FAIMS Mobile platform for map digitization",
      "method_status": "explicit",
      "verbatim_quote": "Having decided to adopt a crowdsourced approach to produce VGI, we chose to customise FAIMS Mobile for map digitisation. The history of the FAIMS Project and the features of FAIMS Mobile have been described elsewhere (Sobotkova et al., 2021; Ballsun-Stanton et al., 2018; Ross et al., 2013). Briefly, FAIMS Mobile is a server-client platform that generates customised Android applications for data collection during offline field research. Customisation is accomplished via definition files that can be shared, modified, and redeployed.",
      "data_collection_approach": "customized mobile application",
      "tools_technologies": [
        {
          "tool_name": "FAIMS Mobile",
          "tool_type": "mobile data collection platform",
          "version": "Not specified",
          "configuration": "Customized via definition files for map digitization"
        }
      ],
      "implements_designs": [
        "RD008",
        "RD009"
      ],
      "realized_through_protocols": [
        "P002"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.3 Using a mobile application for map digitisation",
        "paragraph": 2
      }
    },
    {
      "method_id": "M009",
      "method_text": "Workflow design dividing technical tasks (staff) from digitization tasks (volunteers)",
      "method_status": "explicit",
      "verbatim_quote": "The stages of FAIMS Mobile implementation (Fig. 3) included: (1) project staff modelled the data and workflow to ensure that the final dataset met research needs, (2) a junior software developer worked with project staff to customise the system, (3) project staff defined a spatial reference system (SRS) and imported preprocessed historical maps, (4) volunteers drew a shape (usually a point) wherever they saw a target symbol and (5) volunteers transcribed attributes from the map, (6) project staff exported data using the FAIMS Mobile server, (7) project staff undertook a targeted accuracy-checking exercise. This approach moved activities requiring technical expertise to phases where specialists could contribute, while simplifying the tasks assigned to student volunteers as much as possible.",
      "data_collection_approach": "divided workflow with role-based task assignment",
      "multi_phase": true,
      "phase_descriptions": [
        "Staff: data/workflow modeling",
        "Developer: system customization",
        "Staff: SRS definition, map preprocessing",
        "Volunteers: shape digitization",
        "Volunteers: attribute transcription",
        "Staff: data export",
        "Staff: accuracy checking"
      ],
      "implements_designs": [
        "RD009"
      ],
      "realized_through_protocols": [
        "P004",
        "P006",
        "P007"
      ],
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 1
      }
    },
    {
      "method_id": "M011",
      "method_text": "Quality assurance using on-device validation and project leader review",
      "method_status": "explicit",
      "verbatim_quote": "Reports from on-device validation, as well as quality assurance by project leaders, suggest that digitisation accuracy was good.",
      "quality_control": {
        "approaches": [
          "On-device validation",
          "Project leader quality assurance review"
        ]
      },
      "implements_designs": [
        "RD009"
      ],
      "realized_through_protocols": [
        "P008",
        "P020",
        "P022",
        "P023"
      ],
      "location": {
        "section": "3. Results",
        "subsection": "3.5 Data quality",
        "paragraph": 1
      }
    },
    {
      "method_id": "M012",
      "method_text": "Calculation of payoff thresholds comparing crowdsourcing vs desktop GIS approaches",
      "method_status": "implicit",
      "trigger_text": [
        "After brief workspace setup, project staff with desktop GIS experience could digitise at a sustained rate of 60–75 features per staff-hour. At this rate, the 57 h of staff time devoted to set-up, support, and quality assurance for our crowdsourcing system could have resulted in some 3,420–4,275 staff-digitised features (see Table 4). Such a payoff threshold suggests that digitisation by project staff will be suitable only for smaller datasets.",
        "Had specialist project staff instead trained and supervised volunteers to use desktop GIS for digitisation, based on our 2010 digitisation rate of 130–180 features per staff-hour, 57 h might have produced 7,410–10,260 features."
      ],
      "trigger_locations": [
        {
          "section": "4. Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 1
        },
        {
          "section": "4. Discussion",
          "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
          "paragraph": 2
        }
      ],
      "inference_reasoning": "Discussion describes comparative calculations (features per hour × total hours) to determine payoff thresholds, but does not provide full methodological protocol for how rates were measured or thresholds calculated. Results are presented without detailed calculation procedures.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Calculation procedures not fully specified: how 60-75 features/hour measured, how threshold points determined, uncertainty bounds not provided",
        "assessability_impact": "Can assess outputs (threshold values) but cannot verify calculation methodology",
        "reconstruction_confidence": "medium"
      },
      "data_collection_approach": "comparative analysis",
      "implements_designs": [
        "RD002"
      ],
      "location": {
        "section": "4. Discussion",
        "subsection": "4.1.1 Desktop GIS approaches versus crowdsourcing",
        "paragraph": 1
      },
      "expected_information_missing": [
        "Measurement protocol for digitization rates",
        "Threshold calculation formula",
        "Uncertainty estimation method",
        "Sensitivity analysis procedures"
      ]
    },
    {
      "method_id": "M013",
      "method_text": "Benchmarking comparison with ML approach from Urban Occupations Project",
      "method_status": "implicit",
      "trigger_text": [
        "The ERC-funded Urban Occupations Project (Can, Gerrits, and Kabadayi 2021), however, provides one benchmark for judging when pursuing a ML approach might be worthwhile. This project reported 1,250 h of manual digitisation to create enough training data to classify roads visible in historical maps of the Ottoman Empire. Using this input, and after additional preprocessing and filtering, an ML expert spent seven days testing and fine tuning the model.",
        "We spent 44 staff hours customising and deploying a streamlined geospatial system in FAIMS Mobile, 184 participant-hours digitising features, seven staff-hours directly supporting that digitisation, and six staff hours checking for errors. These 241 h produced a dataset of 10,827 features, a rate of 44.9 features/person-hour. At that rate, the 1,300 h it took to deploy the ML approach taken by Can, Gerrits, and Kabadayi would yield about 58,400 records"
      ],
      "trigger_locations": [
        {
          "section": "4. Discussion",
          "subsection": "4.1.2 Machine learning versus crowdsourcing",
          "paragraph": 2
        },
        {
          "section": "4. Discussion",
          "subsection": "4.1.2 Machine learning versus crowdsourcing",
          "paragraph": 3
        }
      ],
      "inference_reasoning": "Discussion uses published data from Urban Occupations Project to calculate comparative thresholds, but does not detail the benchmarking methodology, assumption handling, or how comparability was assessed between different feature types (roads vs mounds).",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Benchmarking methodology not described: how projects deemed comparable, adjustment procedures for different feature types, quality assurance time handling",
        "assessability_impact": "Cannot assess appropriateness of comparison or validity of extrapolation",
        "reconstruction_confidence": "low"
      },
      "data_collection_approach": "comparative benchmarking",
      "data_sources": [
        {
          "source_type": "published_literature",
          "source_description": "Urban Occupations Project (Can, Gerrits, and Kabadayi 2021)",
          "acquisition_method": "Literature review"
        }
      ],
      "implements_designs": [
        "RD002"
      ],
      "location": {
        "section": "4. Discussion",
        "subsection": "4.1.2 Machine learning versus crowdsourcing",
        "paragraph": 2
      },
      "expected_information_missing": [
        "Comparability assessment procedures",
        "Feature type adjustment factors",
        "Uncertainty propagation methods",
        "Sensitivity to different assumptions"
      ]
    }
  ],
  "protocols": [
    {
      "protocol_id": "P001",
      "protocol_text": "Point feature record with 10 attributes per mound symbol",
      "protocol_status": "explicit",
      "verbatim_quote": "The records we sought to create were relatively simple: a point for the feature, a record number, plus ten attributes.",
      "data_recording_protocol": {
        "recording_medium": "digital mobile application",
        "data_structure": "Point geometry with 10 attribute fields",
        "record_id_scheme": "Record number (details not specified)"
      },
      "implements_method": "M005",
      "location": {
        "section": "2. Approach",
        "subsection": "2.1 Archaeological features in Soviet topographic maps",
        "paragraph": 2
      },
      "expected_information_missing": [
        "Specific attribute field names and types",
        "Record number generation scheme"
      ]
    },
    {
      "protocol_id": "P020",
      "protocol_text": "Re-examination of four randomly selected maps for error checking",
      "protocol_status": "explicit",
      "verbatim_quote": "Finally, re-examination of four randomly selected maps after fieldwork required 6 h of staff time, including desktop GIS setup, confirmation of feature digitisation, and tabulating errors and error rates.",
      "sampling_strategy": {
        "sampling_type": "random",
        "sample_size": 4,
        "sampling_unit": "maps"
      },
      "procedure_steps": [
        "Random selection of 4 maps",
        "Desktop GIS setup",
        "Confirmation of feature digitization",
        "Error tabulation"
      ],
      "personnel": "project staff",
      "time_investment": {
        "staff_hours": 6
      },
      "implements_method": "M010",
      "location": {
        "section": "3. Results",
        "subsection": "3.1 Project staff time for setup, support, and accuracy-checking",
        "paragraph": 3
      },
      "extraction_notes": "Links to P013 from Section 2"
    },
    {
      "protocol_id": "P002",
      "protocol_text": "FAIMS Mobile platform selection based on functional requirements and capabilities: offline data collection, multi-device synchronization, validation, simple UI with streamlined workflow, layer management, geometry creation/editing, structured data capture, arbitrary raster import (GeoTIFFs), automated metadata creation, and data validation",
      "protocol_status": "explicit",
      "verbatim_quote": "It can collect, manage, and bind spatial, structured, multimedia, and text data as part of a single record, obviating the need to use multiple applications. Data collection works offline, and can employ as many devices as necessary. It is later synchronised opportunistically, when a network is available. Data can be validated on devices at the time of capture, or on the server after synchronisation. The server can also be used to edit data, view data history, selectively revert data to earlier states, and export data in a variety of formats. Second, this system met the functional requirements we identified for geospatial software. It supported the production of a customised map digitisation system with a simple UI and streamlined workflow, while still providing essential features including layer management, geometry creation and editing, capture and association of structured data, import and use of arbitrary rasters (scanned maps as geotiffs), automated metadata creation, and data validation.",
      "tools_technologies": [
        {
          "tool_name": "FAIMS Mobile",
          "tool_type": "server-client platform",
          "capabilities": [
            "Offline data collection",
            "Multi-device deployment",
            "Opportunistic synchronization",
            "Device-level and server-level validation",
            "Data history tracking",
            "Selective reversion",
            "Multi-format export"
          ],
          "functional_requirements_met": [
            "Simple UI with streamlined workflow",
            "Layer management",
            "Geometry creation and editing",
            "Structured data capture",
            "Arbitrary raster import (GeoTIFFs)",
            "Automated metadata creation",
            "Data validation"
          ]
        }
      ],
      "implements_method": "M008",
      "location": {
        "section": "2. Approach",
        "subsection": "2.3 Using a mobile application for map digitisation",
        "paragraph": 2
      },
      "consolidation_metadata": {
        "consolidated_from": [
          "P002",
          "P003"
        ],
        "consolidation_type": "tool_specification",
        "information_preserved": "complete",
        "rationale": "P002 lists FAIMS Mobile general capabilities, P003 lists specific functional requirements for this project. Together they explain why FAIMS was selected (meets requirements) and what capabilities it offers. Assessed together as complete platform specification and selection justification."
      }
    },
    {
      "protocol_id": "P004",
      "protocol_text": "Collaborative system development: project staff modeled data and workflow to ensure research needs alignment, then junior software developer worked with staff to customize the system",
      "protocol_status": "explicit",
      "verbatim_quote": "project staff modelled the data and workflow to ensure that the final dataset met research needs; a junior software developer worked with project staff to customise the system",
      "procedure_steps": [
        "Project staff model data structure",
        "Project staff model workflow",
        "Ensure alignment with research needs",
        "Junior developer collaborates with staff to customize system"
      ],
      "personnel": "project staff + junior software developer",
      "implements_method": "M009",
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 1
      },
      "expected_information_missing": [
        "Modeling methodology",
        "Requirements gathering process",
        "Customization methodology",
        "Developer skill requirements",
        "Customization timeline"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P004",
          "P005"
        ],
        "consolidation_type": "workflow_integration",
        "information_preserved": "complete",
        "rationale": "Sequential development steps: P004 (staff modeling) → P005 (developer customization). Assessed together as integrated system development workflow. Both from same paragraph describing collaborative development process."
      }
    },
    {
      "protocol_id": "P006",
      "protocol_text": "Spatial reference system definition and preprocessed map import by staff",
      "protocol_status": "explicit",
      "verbatim_quote": "project staff defined a spatial reference system (SRS) and imported preprocessed historical maps",
      "procedure_steps": [
        "Define SRS",
        "Preprocess historical maps",
        "Import maps into system"
      ],
      "personnel": "project staff",
      "implements_method": "M009",
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 1
      },
      "expected_information_missing": [
        "SRS specification (coordinate system, projection)",
        "Map preprocessing procedures",
        "Import protocols"
      ]
    },
    {
      "protocol_id": "P007",
      "protocol_text": "Volunteer drawing shapes and transcribing attributes from maps",
      "protocol_status": "explicit",
      "verbatim_quote": "volunteers drew a shape (usually a point) wherever they saw a target symbol and (5) volunteers transcribed attributes from the map",
      "procedure_steps": [
        "Identify target symbol on map",
        "Draw shape (usually point) at symbol location",
        "Transcribe attributes from map"
      ],
      "personnel": "volunteers",
      "implements_method": "M009",
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 1
      },
      "expected_information_missing": [
        "Symbol identification training",
        "Attribute transcription rules",
        "Quality control during transcription"
      ]
    },
    {
      "protocol_id": "P008",
      "protocol_text": "Automated tasks and capabilities provided by FAIMS Mobile",
      "protocol_status": "explicit",
      "verbatim_quote": "To support this workflow and make work easier for both project staff and participants, FAIMS Mobile automated a number of tasks and provided necessary capabilities. It applied the spatial reference system, rendered maps in the workspace, provided layer management (including a data entry layer), enforced shape topology, displayed pre-defined controlled vocabularies for attribute terms, recorded creation time and author for each record, maintained a history of all changes to data, applied validation to ensure record completeness, merged data from multiple devices, and exported data in common formats.",
      "automated_features": [
        "Spatial reference system application",
        "Map rendering",
        "Layer management",
        "Shape topology enforcement",
        "Controlled vocabulary display",
        "Creation timestamp recording",
        "Author recording",
        "Data history maintenance",
        "Validation for completeness",
        "Multi-device data merging",
        "Multi-format export"
      ],
      "implements_method": "M009",
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 2
      }
    },
    {
      "protocol_id": "P009",
      "protocol_text": "Streamlined volunteer interface design: toggle between map view (geospatial interactions) and form view (attribute editing), with layer controls, pan/zoom, search/retrieval, and record editing. Staff handled infrastructure setup, map preprocessing, data aggregation, export, and backup, insulating volunteers from technical GIS tasks. Unnecessary GIS features hidden or eliminated.",
      "protocol_status": "explicit",
      "verbatim_quote": "The digitisation interface itself was as streamlined as possible (see Figs. 4 and 5). Volunteers could toggle between a map view for geospatial data interactions and a form view for attribute creation and editing. In the map, they could adjust layer focus and visibility, pan, and zoom. Existing records could be searched, retrieved, inspected, and edited. Since project staff set up the infrastructure and pre-processed and loaded the required maps, volunteers were insulated from the friction of setup, layer management, data aggregation, export, and backup. GIS features not needed for digitisation were hidden or eliminated. Digitisation and metadata creation required no GIS or computing skills. Students capable of selecting files from a list, panning and zooming a map, dropping a point, and filling out a form were able to create data.",
      "interface_components": [
        "Map view (geospatial interactions)",
        "Form view (attribute creation/editing)",
        "Toggle between views",
        "Layer focus and visibility controls",
        "Pan and zoom controls",
        "Record search and retrieval",
        "Record inspection and editing"
      ],
      "task_simplification": [
        "Staff handled: infrastructure setup, map preprocessing/loading, data aggregation, export, backup",
        "Hidden/eliminated: unnecessary GIS features",
        "Required volunteer skills: file selection, map navigation, point placement, form completion"
      ],
      "implements_method": "M009",
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 2
      },
      "consolidation_metadata": {
        "consolidated_from": [
          "P009",
          "P010"
        ],
        "consolidation_type": "tool_specification",
        "information_preserved": "complete",
        "rationale": "P009 describes volunteer-facing interface components, P010 describes task simplification strategy. Together they explain complete interface design philosophy: what volunteers see (streamlined UI) and how complexity is hidden (staff handle technical tasks). Assessed together as integrated volunteer interface specification."
      }
    },
    {
      "protocol_id": "P011",
      "protocol_text": "Data export producing FAIR-compliant datasets with minimal cleaning",
      "protocol_status": "explicit",
      "verbatim_quote": "Exported data was consistent and complete, ready for analysis with minimal cleaning. This data adhered to key elements of the FAIR data principles, especially the production of 'rich' and 'plural' metadata at the time of data creation (principles F2, R1.1–1.3; GO-FAIR).",
      "data_quality_outputs": [
        "Consistent data structure",
        "Complete records",
        "Minimal cleaning required",
        "Rich metadata (F2)",
        "Plural metadata (R1.1-1.3)",
        "FAIR-compliant"
      ],
      "implements_method": "M009",
      "location": {
        "section": "2. Approach",
        "subsection": "2.4 Design and implementation of the recording system",
        "paragraph": 4
      }
    },
    {
      "protocol_id": "P012",
      "protocol_text": "Time measurement using timesheets, record timestamps, and staff journals",
      "protocol_status": "explicit",
      "verbatim_quote": "Project records provided much of this data (timesheets from the programmer; record creation timestamps for students using the system), while project staff logged time-on-task for activities in journals.",
      "measurement_procedures": [
        {
          "measured_variable": "Programmer time",
          "measurement_method": "timesheets"
        },
        {
          "measured_variable": "Student time-on-task",
          "measurement_method": "record creation timestamps from devices"
        },
        {
          "measured_variable": "Staff time (setup, support, QC)",
          "measurement_method": "journal logging"
        }
      ],
      "implements_method": "M010",
      "location": {
        "section": "2. Approach",
        "subsection": "2.5 Evaluating the digitisation approach",
        "paragraph": 2
      }
    },
    {
      "protocol_id": "P013",
      "protocol_text": "Random selection of digitization work for error characterization",
      "protocol_status": "explicit",
      "verbatim_quote": "Finally, project staff reviewed randomly selected digitisation work completed by volunteers to characterise errors.",
      "sampling_strategy": {
        "sampling_type": "random",
        "sampling_unit": "digitized maps or sections"
      },
      "procedure_steps": [
        "Random selection of digitization work",
        "Staff review",
        "Error characterization"
      ],
      "personnel": "project staff",
      "implements_method": "M010",
      "location": {
        "section": "2. Approach",
        "subsection": "2.5 Evaluating the digitisation approach",
        "paragraph": 3
      },
      "expected_information_missing": [
        "Sample size",
        "Sampling methodology details",
        "Error classification scheme"
      ]
    },
    {
      "protocol_id": "P014",
      "protocol_text": "Map Digitisation customization development requiring 35h programmer + 4h staff (2017)",
      "protocol_status": "explicit",
      "verbatim_quote": "For the first season of use (2017), creating the Map Digitisation customisation of FAIMS Mobile required 35 h from an undergraduate student programmer plus 4 h from staff (Nassif-Haynes et al., 2021).",
      "procedure_steps": [
        "Student programmer development (35 hours)",
        "Staff collaboration/oversight (4 hours)"
      ],
      "personnel": "undergraduate student programmer + project staff",
      "time_investment": {
        "programmer_hours": 35,
        "staff_hours": 4,
        "total_hours": 39
      },
      "implements_method": "M008",
      "location": {
        "section": "3. Results",
        "subsection": "3.1 Project staff time for setup, support, and accuracy-checking",
        "paragraph": 1
      }
    },
    {
      "protocol_id": "P015",
      "protocol_text": "Field deployment protocol: server setup and client device configuration (3h), map preparation including tiling and adding pyramids (1.5h), file compression, upload to server, and download to devices with monitoring (2.5h)",
      "protocol_status": "explicit",
      "verbatim_quote": "Setup of the server and configuration of the client devices in the field required 3 h from staff. Map preparation (tiling, adding pyramids) required about 1.5 h. Monitoring file compression, upload to the server, and download to devices took an additional 2.5 h.",
      "procedure_steps": [
        "Server setup",
        "Client device configuration",
        "Map tiling",
        "Adding pyramids to maps",
        "File compression",
        "Upload to server",
        "Download to devices",
        "Monitoring throughout process"
      ],
      "personnel": "project staff",
      "time_investment": {
        "server_device_setup_hours": 3,
        "map_preparation_hours": 1.5,
        "file_transfer_monitoring_hours": 2.5,
        "total_deployment_hours": 7
      },
      "implements_method": "M009",
      "location": {
        "section": "3. Results",
        "subsection": "3.1 Project staff time for setup, support, and accuracy-checking",
        "paragraph": 1
      },
      "expected_information_missing": [
        "Server specifications",
        "Device configuration details",
        "Number of devices configured",
        "Tiling specifications (size, format)",
        "Pyramid levels",
        "Software tools used",
        "Compression algorithm/settings",
        "Transfer protocols",
        "Network conditions"
      ],
      "consolidation_metadata": {
        "consolidated_from": [
          "P015",
          "P016",
          "P017"
        ],
        "consolidation_type": "workflow_integration",
        "information_preserved": "complete",
        "rationale": "Sequential deployment steps forming integrated field deployment workflow: server/device setup → map preparation → file transfer. All from same paragraph describing staff time for deployment activities. Assessed together as complete deployment protocol with total time investment (7h)."
      }
    },
    {
      "protocol_id": "P018",
      "protocol_text": "Minimal student training and supervision (0.5h staff time per season)",
      "protocol_status": "explicit",
      "verbatim_quote": "Training and supervision of students took no more than half an hour of staff time across the entire season.",
      "procedure_steps": [
        "Student training",
        "Ongoing supervision"
      ],
      "personnel": "project staff",
      "time_investment": {
        "staff_hours_2017": 0.5,
        "staff_hours_2018": 0.5,
        "total_hours": 1.0
      },
      "implements_method": "M009",
      "location": {
        "section": "3. Results",
        "subsection": "3.1 Project staff time for setup, support, and accuracy-checking",
        "paragraph": 1
      },
      "extraction_notes": "Notably minimal training time compared to desktop GIS approaches"
    },
    {
      "protocol_id": "P019",
      "protocol_text": "Additional validation to ensure latitude/longitude population from GPS (2018)",
      "protocol_status": "explicit",
      "verbatim_quote": "For the second season, adding additional validation to ensure population of latitude and longitude from GPS (see 'Recoverable data omissions and incomplete records' below) took 1 h of development from the programmer.",
      "procedure_steps": [
        "Identify validation need (from 2017 errors)",
        "Developer implements validation (1 hour)",
        "Deploy in 2018 season"
      ],
      "personnel": "programmer",
      "time_investment": {
        "programmer_hours": 1
      },
      "implements_method": "M008",
      "location": {
        "section": "3. Results",
        "subsection": "3.1 Project staff time for setup, support, and accuracy-checking",
        "paragraph": 2
      },
      "extraction_notes": "Iterative improvement based on 2017 error analysis"
    },
    {
      "protocol_id": "P021",
      "protocol_text": "Mitigation of performance degradation through data export and app reinstantiation",
      "protocol_status": "explicit",
      "verbatim_quote": "Deteriorating performance was mitigated by exporting all data and instantiating a new and empty version of the application. Since data structures were identical, aggregation of multiple exports was trivial.",
      "procedure_steps": [
        "Identify performance degradation (>2500 records)",
        "Export all data",
        "Instantiate new empty application",
        "Aggregate multiple exports"
      ],
      "parameters": {
        "performance_threshold": "~2500 records"
      },
      "implements_method": "M009",
      "location": {
        "section": "3. Results",
        "subsection": "3.4 Application performance",
        "paragraph": 2
      },
      "extraction_notes": "Workaround for database performance limitations"
    },
    {
      "protocol_id": "P022",
      "protocol_text": "Correction of spatial omissions by re-extracting lat/long from geodatabase",
      "protocol_status": "explicit",
      "verbatim_quote": "Since the geodatabase preserved geometries, spatial omissions were corrected by re-extracting latitude and longitude; only two data points could not be recovered.",
      "procedure_steps": [
        "Identify records with empty lat/long fields",
        "Re-extract coordinates from SpatiaLite geodatabase",
        "Update records"
      ],
      "data_recovery": {
        "recoverable": "205 of 207 spatial omissions",
        "unrecoverable": 2
      },
      "implements_method": "M011",
      "location": {
        "section": "3. Results",
        "subsection": "3.5.1 Recoverable data omissions and incomplete records",
        "paragraph": 1
      }
    },
    {
      "protocol_id": "P023",
      "protocol_text": "Error categorization scheme for quality assessment",
      "protocol_status": "implicit",
      "trigger_text": [
        "Forty-two of these errors were false negatives (symbols missed by students). Six were double-marked (Student C digitised a section of a map twice). Students made only one classification error (a similar symbol mistaken for a benchmark), and no outright false positives."
      ],
      "trigger_locations": [
        {
          "section": "3. Results",
          "subsection": "3.5.2 Digitisation errors",
          "paragraph": 2
        }
      ],
      "inference_reasoning": "Results present detailed error categorization (false negatives, double-marking, classification errors, false positives), but Methods section doesn't describe the error classification scheme. Staff must have applied systematic categories during quality review.",
      "implicit_metadata": {
        "basis": "inferred_from_results",
        "transparency_gap": "Error classification scheme not described in Methods",
        "assessability_impact": "Can verify error counts but cannot assess classification consistency",
        "reconstruction_confidence": "high"
      },
      "error_categories": [
        "False negatives (missed symbols)",
        "Double-marked (duplicate entries)",
        "Classification errors (wrong symbol type)",
        "False positives (non-existent features)"
      ],
      "implements_method": "M011",
      "location": {
        "section": "3. Results",
        "subsection": "3.5.2 Digitisation errors",
        "paragraph": 2
      },
      "expected_information_missing": [
        "Error classification decision rules",
        "Inter-rater reliability procedures",
        "Borderline case resolution"
      ]
    },
    {
      "protocol_id": "P024",
      "protocol_text": "Post-collection data processing requiring less than 2 hours",
      "protocol_status": "explicit",
      "verbatim_quote": "All collected data was available daily for review, and a comprehensive, FAIR-compliant dataset was ready for analysis with less than 2 h of processing after collection.",
      "time_requirements": {
        "processing_time": "< 2 hours",
        "notes": "Dataset ready for analysis after minimal processing"
      },
      "procedure_description": "Post-collection processing to prepare FAIR-compliant dataset for analysis",
      "implements_method": "M009",
      "location": {
        "section": "5. Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "expected_information_missing": [
        "Specific processing steps",
        "Software/tools used",
        "Quality checks performed"
      ]
    },
    {
      "protocol_id": "P025",
      "protocol_text": "Proposed error mitigation through redundant digitization or peer review",
      "protocol_status": "implicit",
      "trigger_text": [
        "Simple expedients, such as assigning multiple students to digitise the same map tiles independently or assigning one student to review work by another, would likely eliminate most errors.",
        "errors were predictable and would be easily mitigated by redundant digitisation by volunteers or volunteer peer review"
      ],
      "trigger_locations": [
        {
          "section": "3. Results",
          "subsection": "3.5.2 Digitisation errors",
          "paragraph": 4
        },
        {
          "section": "5. Conclusion",
          "subsection": null,
          "paragraph": 2
        }
      ],
      "inference_reasoning": "Paper mentions error mitigation strategies (redundant digitization, peer review) as possibilities but does not describe implementation protocols. These are proposed approaches, not implemented procedures.",
      "implicit_metadata": {
        "basis": "mentioned_undocumented",
        "transparency_gap": "Proposed strategies lack implementation detail: assignment protocols, review procedures, conflict resolution methods not specified",
        "assessability_impact": "Cannot assess feasibility or effectiveness without implementation protocols",
        "reconstruction_confidence": "low"
      },
      "procedure_description": "Proposed quality control through redundant work or peer review",
      "location": {
        "section": "5. Conclusion",
        "subsection": null,
        "paragraph": 2
      },
      "expected_information_missing": [
        "Assignment protocol for redundant digitization",
        "Peer review procedures",
        "Conflict resolution methods",
        "Threshold for consensus"
      ],
      "extraction_notes": "Proposed strategy, not implemented protocol"
    }
  ],
  "extraction_notes": {
    "pass": 2,
    "stage": "RDMAP Pass 2 - Rationalization COMPLETE",
    "section_extracted": "Complete paper: Abstract + Sections 1-5",
    "items_before_rationalization": 48,
    "items_after_rationalization": 39,
    "reduction_count": 9,
    "reduction_percentage": 18.8,
    "consolidations_performed": 8,
    "tier_corrections": 0,
    "boundary_corrections": 0,
    "consolidation_details": {
      "research_designs": {
        "before": 10,
        "after": 8,
        "consolidations": [
          "RD003 + RD004 → TRAP project aims (scope_integration)",
          "RD005 + RD006 → Theoretical frameworks (rationale_synthesis)"
        ]
      },
      "methods": {
        "before": 13,
        "after": 11,
        "consolidations": [
          "M003 + M004 → Map digitization method (redundancy_elimination)",
          "M005 + M006 → Digitization approach (workflow_integration)"
        ]
      },
      "protocols": {
        "before": 25,
        "after": 20,
        "consolidations": [
          "P002 + P003 → FAIMS platform specification (tool_specification)",
          "P004 + P005 → System development protocol (workflow_integration)",
          "P009 + P010 → Interface design specification (tool_specification)",
          "P015 + P016 + P017 → Deployment protocol (workflow_integration)"
        ]
      }
    },
    "notes": "Pass 2 rationalization complete. Achieved 16.7% reduction through 8 consolidations (2 designs, 2 methods, 4 protocols). All consolidations maintain complete information with proper sourcing (verbatim_quote for explicit, trigger_text for implicit). No tier corrections or boundary corrections needed - Pass 1 classifications were accurate. Ready for Pass 3 validation."
  }
}