# Variability Test Protocol

**Version:** 1.0
**Date:** 2025-12-01
**Purpose:** Document the automated protocol for testing extraction and assessment pipeline variability

---

## Overview

The variability test measures consistency of the research-assessor pipeline by running the full extraction and assessment workflow multiple times on the same papers. This tests whether the pipeline produces reliable, reproducible results.

### Key Questions

1. **Extraction consistency**: Do we extract the same items across runs?
2. **Assessment consistency**: Do we produce the same scores across runs?
3. **Propagation effects**: Does extraction variability affect assessment variability?

### Design

- **Papers**: 5 diverse papers (empirical, methodological, interpretive)
- **Runs per paper**: 5 independent extractions
- **Total runs**: 25
- **Context isolation**: Fresh session for each run (prevents contamination)

---

## Test Corpus

| Paper | Type | Approach | Context Flags | Status |
|-------|------|----------|---------------|--------|
| sobotkova-et-al-2024 | Empirical | Deductive | â€” | âœ… Complete |
| penske-et-al-2023 | Empirical | Inductive | â€” | ğŸ”„ In Progress |
| ballsun-stanton-et-al-2018 | Methodological | â€” | ğŸ“¦ ğŸ”§ | â³ Pending |
| ross-2005 | Empirical | Interpretive | ğŸ”§ | â³ Pending |
| sobotkova-et-al-2016 | Empirical | Mixed | â€” | â³ Pending |

### Diversity Coverage

- 3 empirical papers (deductive, inductive, interpretive approaches)
- 1 methodological paper (software tool)
- 1 mixed-methods paper
- Context flag variants (ğŸ“¦ software, ğŸ”§ non-standard methodology)

---

## Automation Components

### 1. Variability Queue (`input/variability-queue.yaml`)

Tracks all paper/run combinations with status:

```yaml
papers:
  - slug: penske-et-al-2023
    title: "Early contact between late farming..."
    source: input/sources/original-pdf/Penske et al...pdf
    paper_type: empirical
    research_approach: inductive
    status: in_progress
    runs:
      - id: run-01
        status: pending
      - id: run-02
        status: pending
      # ...
```

**Status values:**
- `pending` â€” Not yet started
- `in_progress` â€” Currently being processed (should not persist across sessions)
- `completed` â€” Run finished successfully
- `error` â€” Run failed (see notes)

**Tracked metadata (on completion):**
- `counts`: {evidence, claims, implicit_arguments}
- `aggregate_score`: Final credibility score (0-100)

### 2. Slash Command (`.claude/commands/variability-run.md`)

Automates a single run when invoked with `/variability-run`:

1. **Read queue** â€” Find next pending run
2. **Load skill** â€” Activate research-assessor
3. **Create output directory** â€” `outputs/variability-test/{paper}/{run}/`
4. **Execute extraction** â€” Passes 0-7 (evidence, claims, RDMAP, etc.)
5. **Execute assessment** â€” Passes 8-10 (classification, signals, report)
6. **Update queue** â€” Mark run complete, record metrics
7. **Run validation** â€” Check content uniqueness
8. **Report completion** â€” Summary for user

### 3. Validation Script (`scripts/validate-run-uniqueness.sh`)

Verifies runs have genuinely unique content (not copies):

```bash
./scripts/validate-run-uniqueness.sh outputs/variability-test/penske-et-al-2023
```

Checks MD5 hashes of evidence, claims, and implicit_arguments arrays across all runs.

### 4. Analysis Script (`scripts/analyse-extraction-variability.py`)

Computes variability metrics after runs complete:

```bash
python3 scripts/analyse-extraction-variability.py outputs/variability-test/penske-et-al-2023
```

**Metrics:**
- Count statistics (mean, stdev, CV%, range)
- Concept overlap (Jaccard similarity)
- Pairwise similarity matrices

---

## User Workflow

### Per-Run Protocol

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Start fresh session (or /clear from previous)      â”‚
â”‚                                                         â”‚
â”‚  2. Run command:  /variability-run                      â”‚
â”‚                                                         â”‚
â”‚  3. Wait for completion (~30-60 minutes)                â”‚
â”‚     Claude will:                                        â”‚
â”‚     - Find next pending run from queue                  â”‚
â”‚     - Execute full pipeline autonomously                â”‚
â”‚     - Update queue with results                         â”‚
â”‚     - Report completion summary                         â”‚
â”‚                                                         â”‚
â”‚  4. Review output (optional)                            â”‚
â”‚                                                         â”‚
â”‚  5. Clear context:  /clear                              â”‚
â”‚                                                         â”‚
â”‚  6. Repeat from step 2                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Context Clearing?

Context clearing between runs prevents **contamination** â€” a failure mode where the LLM, having seen previous extractions, produces outputs that are:
- Copies of previous runs (identical content)
- Deliberately different (artificial variation)

Neither represents genuine independent extraction. Fresh context ensures each run is a true independent read of the source paper.

### Checking Progress

View queue status:
```bash
cat input/variability-queue.yaml | grep -A2 "status:"
```

Or check output directories:
```bash
ls outputs/variability-test/*/run-*/extraction.json
```

---

## Output Structure

```
outputs/variability-test/
â”œâ”€â”€ {paper-slug}/
â”‚   â”œâ”€â”€ run-01/
â”‚   â”‚   â”œâ”€â”€ extraction.json          # Full extraction output
â”‚   â”‚   â””â”€â”€ assessment/
â”‚   â”‚       â”œâ”€â”€ classification.json  # Paper type, approach
â”‚   â”‚       â”œâ”€â”€ track-a-quality.md   # Quality gating
â”‚   â”‚       â”œâ”€â”€ cluster-1-foundational-clarity.md
â”‚   â”‚       â”œâ”€â”€ cluster-2-evidential-strength.md
â”‚   â”‚       â”œâ”€â”€ cluster-3-reproducibility.md
â”‚   â”‚       â””â”€â”€ credibility-report.md  # Final assessment
â”‚   â”œâ”€â”€ run-02/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ run-03/
â”‚   â”œâ”€â”€ run-04/
â”‚   â”œâ”€â”€ run-05/
â”‚   â””â”€â”€ variability-analysis.json    # Analysis output (after all runs)
```

---

## Success Criteria

| Metric | Target | Rationale |
|--------|--------|-----------|
| Runs completed | 25/25 | All papers Ã— all runs |
| Verdict consistency | â‰¥80% same band | Users get consistent guidance |
| Signal score CV | <15% for most signals | Acceptable measurement noise |
| Classification consistency | 100% same type/approach | Structural properties stable |
| Context flag consistency | 100% same flags | Binary decisions should be deterministic |

---

## Analysis Phase

After all 25 runs complete:

### 1. Per-Paper Analysis

```bash
python3 scripts/analyse-extraction-variability.py outputs/variability-test/{paper}
```

Generates:
- Count statistics table
- Concept overlap percentages
- Pairwise similarity matrix

### 2. Cross-Paper Comparison

Compare variability patterns across paper types:
- Do empirical papers show different variability than methodological?
- Does research approach affect extraction consistency?
- Which signals show most/least variability?

### 3. Final Report

`outputs/variability-test/variability-test-summary.md`:
- Executive summary of findings
- Most/least stable signals
- Extraction â†’ assessment correlation
- Recommendations for pipeline refinement

---

## Troubleshooting

### Run Appears to be Copy of Previous

**Symptom:** `validate-run-uniqueness.sh` reports duplicate hashes

**Cause:** Context not cleared between runs

**Solution:** Delete affected runs, ensure `/clear` between each run

### Queue Not Updating

**Symptom:** Same run keeps being selected

**Cause:** Queue file not saved after run completion

**Solution:** Check for write errors, manually update queue if needed

### Run Takes Too Long

**Symptom:** Run exceeds 90 minutes

**Cause:** Large paper or complex extraction

**Solution:** This is expected for some papers (e.g., 58-page book chapters). Let it complete.

### Assessment Scores Vary Wildly

**Symptom:** Same paper gets scores ranging 50-90

**Cause:** Likely extraction quality issue, not assessment issue

**Solution:** Check extraction counts â€” high count variance suggests extraction instability

---

## Lessons Learned

### From Paper 1 (sobotkova-et-al-2024)

- **RDMAP elements perfectly stable** (0% CV) â€” Methods, Protocols, Research Designs extracted identically across runs
- **Evidence/Claims show moderate variability** (7-12% CV) â€” Expected, reflects granularity decisions
- **Assessment scores very stable** (SD ~0.8 points) â€” Different extractions yield nearly identical assessments
- **Quote selection varies more than concepts** â€” Same facts cited from different passages

### From Paper 2 Initial Attempt

- **Context contamination is real** â€” Runs 03-05 were copies when done in same session
- **Context clearing essential** â€” Each run must be independent session

---

## References

- Variability queue: `input/variability-queue.yaml`
- Slash command: `.claude/commands/variability-run.md`
- Validation script: `scripts/validate-run-uniqueness.sh`
- Analysis script: `scripts/analyse-extraction-variability.py`
- Scripts README: `scripts/README.md`

---

**Maintained by:** LLM Reproducibility Project
**Last Updated:** 2025-12-01
